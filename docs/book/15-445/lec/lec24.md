

00:15 - 00:16
let's get started
我们开始吧
00:18 - 00:21
so DJ drop tables is not here today 
So，DJ Drop Table今天并不在
00:21 - 00:26
he should be here next week ,he's a bit of cracking up here 
他下周应该在，他遇上了点事
00:26 - 00:32
so his girlfriend's birthday this week ,and he blew off money last week in Vegas 
So，这周是他女朋友生日，但他上周在拉斯维加斯把钱嚯嚯没了
00:32 - 00:38
so he lied and told her that he's going to visit his family for Thanksgiving 
So，他对他女朋友撒谎说感恩节的时候要去见家人
00:38 - 00:39
so that's why he's gone this week 
So，这就是他这周不在的原因
0.39-0.41
but that's just really get out of ruse of buying him a present 
但这是他为了摆脱买礼物所耍的诡计
他为了不送女朋友礼物，真的是小精心


00:43 - 00:44
so he'll be back next week 
So，他下周会回来
00:44 - 00:48
alright, so again just a reminder for what's on the docket for you guys 
So，这里我要再次提醒下你们
00:48 - 00:52
homework 5 is due next week ,project 4 we do after that 
Homework 5下周截止，Project 4则是在下下周截止
00:52 - 00:56
the extra credit for the feedback submission was last night
昨晚我们搞定了你们Extra Credit的反馈
00:57 - 01:01
some people emailed about about an error cuz some funky Unicode issue 
有些人发邮件跟我说这里面有些因为Unicode而导致的错误
01:02 - 01:04
but everybody else should have submitted 
但所有人应该都已经提交了你们的东西
01:04 - 01:06
the final exam will be on December 9th
期末考试应该是在12月9号
1.06-1.08
 actually they announced the room
实际上，他们已经宣布了考试地点
1.08-1.09
but I haven't looked to see where it is
但我还没看它是在哪
1.09-1.11
 that has anybody looked
有人知道是在哪里么
1.11-1.12
it doesn't matter
算了，这不重要
01:12 - 01:13
we'll figure it out 
我们之后再看
01:14 - 01:19
and then for next week again, the Oracle talk will be on Monday 
在下周一会有一场Oracle的演讲

01:19 - 01:23
the system Potpourri and the final review will be on on the Wednesday
System Potpourri以及期末复习我们会放在下周三
01:24 - 01:26
so we'll do the final review in the beginning 
So，我们会在周三那节课先先进行期末复习
01:26 - 01:30
and then we'll do will cover three or four different database systems that you guys voted on 
然后，我们会去介绍你们选出来的3到4个数据库系统
01:30 - 01:36
and I'll just sort of give you you know a ten minute over your what's interesting about it, why it's good ,why it's bad ,and so forth
我会花10分钟讲下你们感兴趣的那个数据库系统，我会说下它的优缺点等一系列东西
01:37 - 01:37
so if you haven't voted yet
So，如果你还没有投票
1.37-1.39
 please please go in here and vote 
那么就赶紧点这个网站进行投票
01:40 - 01:43
the numbers look similar to previous years 
投票列表中数据库系统的数量和往年差不多
01:45 - 01:48
ah so again you can go look at the videos and last year to see what we're gonna discuss
So，你们可以去看下去年的视频，看看我们去年讨论了哪些东西
1.48-1.51
but go vote first then then go look it up 
但你们还是先投票再看比较好


01:51 - 01:55
and as I said on Monday Tuesday next week our friends our Oracle are coming
我说过下周一周二的时候，我们Oracle的朋友会来我们这里给我们讲课
1.55-1.57
,and there's a gonna be three different talks 
这里有3场不同的讲座
01:58 - 02:00
so on Monday class in here,
So，周一那场讲座就是在这里举行
2.00-2.06
 they'll be will have Shashank Chavan talk about you know there's something his group is building 
 Shasank Chavan会讨论下他们那个组所构建的东西
02:07 - 02:08
again this is not a recruiting talk 
这并不是招聘讲座
02:08 - 02:14
this is like a you know a scientific or or assistant discussion of what, they're building 
这是一场科学性质或者研讨性质的讲座，内容是关于他们所构建的东西
02:14 - 02:18
and basically he'll use all the same key words and buzzwords, that I've used throughout the entire semester 
简单来讲，他会使用我这学期所使用的关键字和术语
02:18 - 02:20
so you realize and I'm not not is making sure up 

02:21 - 02:23
so he'll come and talk about their system
So，他会来讨论下他们所做的系统
2.23-2.28
, there's a more recruiting oriented systems talk will be Monday at 4:30 in in gates 
周一下午四点半的时候，在GHC 6115那里会有一场招聘性质相关的数据库系统讲座
02:28 - 02:30
and that one will be pizza 
这场讲座有披萨提供
02:30 - 02:36
and then a pure research talk would be on Tuesday December 3rd the following day at 12 p.m. and the CIC floor 
在下周二的时候，我们会有一场纯研究性质的讲座
02:36 - 02:43
and so the Hideaki Kimura is as I mentioned on Piazza, I went to grad school with him 
So，我之前在Piazzaa上提到Hideaki Kimura，我和他一起上的研究生
02:45 - 02:50
he's probably one of the most hardcore systems programmers ever met my life 
他可能是我人生中所见到的一个很硬核的系统编程人员
02:50 - 02:52
he is also the most stubborn man that I've ever met in my life 
他也是我人生中见过的最为固执的人
2.52-2.58
when we were building the first the first summer we were building H store 
当年，我们在第一个暑假的时候，一起构建了HStore
02:58 - 03:01
he wanted we had to build our expression trees like for the where clauses
我们需要去构建用于处理where子句之类的表达式树
03:02 - 03:04
I had my way of doing it he had his way 
我有我的方法，他有他的方法
03:04 - 03:06
we literally had a four-hour debate in my office
我们在我的办公室争论了4小时
3.06-3.09
 just yelling at each other about, what to do and he just broke me down said to do it 
我们互相争论该怎么做，他让我有点失望，他让我按照他的做
03:09 - 03:10
he was wrong
他的想法是错的
3.10-3.13
, and we removed what he did later on， my codes still there 
我们之后移除了他做的那部分，我的代码被保留了下来
03:13 - 03:13
so I was right
So，我的想法是正确的
3.13-3.15
, but in general he's awesome 
但总的来讲，他很棒
03:15 - 03:17
so he's gonna come talk about some 

03:18 - 03:22 ！！！！！！
he's just come talk to the non-volatile memory stuff that they've been working on for the system
他会来讲下他们所负责系统中的非易失性内存(non-volatile memory)
03:22 - 03:23
because he's part of shashank group
因为他是Shasank组中的成员
3.23-3.27
so again if you're interested in doing internships or full-time positions with them
So，如果你想去他们那里实习或者全职工作
03:27 - 03:30
he will come and tell you the kind of things you could be working on, okay 
他会来告诉你们，你们可以负责哪些东西
03:31 - 03:34
and I'll some reminders about all these things on Piazza 
我会将这些东西放在Piazza上
03:34 - 03:35
any questions about any of these
对此你们有任何问题吗？
03:38 - 03:38
okay

3.38-3.44
and then I also make arrangers if you want to eat these guys one-on-one to talk about internships ,and full-time positions as well ,well 
如果你们想要和他一对一讨论实习和全职工作方面的问题，我也会安排时间
03:44 - 03:45
I'll send that email 
我会发邮件的


03:45 - 03:45
Okay

3.45-3.50
so last class was the second lecture we had on distributed databases 
So，上节课是我们关于分布式数据库的第二节课
03:50 - 03:55
the first class are just defining what a distributed database system looks like from architecture standpoint 
第一节课的时候，我们从架构层面定义了一个分布式数据库系统该是怎么样的
03:55 - 04:01
you know where's data relative to where the computers actually gonna run on it 
即相对于机器来说，数据应该放在哪里
04:01 - 04:06
and then last class was all about talking about taking these distributed databases when you want to do transactions 
上节课则是关于分布式数据库执行事务这块的内容
04:06 - 04:14
and making sure that we provide all the ACID guarantees, that we'd want on a single node system, but now doing this in a distribute environment 
为了确保我们将在单节点数据库系统上所提供的ACID保证应用到分布式环境下
04:14 - 04:18
and we spend most of our time talking about the atomic commit protocol and replication
我们上节课花了大部分时间在讨论原子提交协议和replication上面
04:18 - 04:21
because again that's the hard part of apps you have in distributed database
因为这是分布式数据库中很难的一部分
4.21-4.21
when things are split up 
当这些东西都被拆分开来的时候
04:22 - 04:25
and now you have transactions doing writes on a bunch of different nodes at the same time 
你的事务就会在同一时间对不同节点上的数据执行写操作
04:25 - 04:26
how do you keep it all in sync
你如何让它们保持同步
4.26-4.28
how do you avoid losing data
以及，该怎么做才能避免丢失数据
4.28-4.31
,how do you have reads not see stale data ,if you care about those things 
如果你在意这些东西的话，你该如何保证不读取到过时的数据
04:32 - 04:34
so for today class
So，在今天的课上
4.34-4.40
 now we're going to sort up just leave alone or you know leave all the transaction stuff we talked about before
我们不会再去讨论我们之前讨论的事务相关内容
04:40 - 04:42
and now so I was talking about how to do analytics 
今天我们要讨论的是如何进行数据分析
4.42-4.45
,where we're not doing a lot of writes,we're not doing transactions
我们不会去执行大量的写操作，也不会去执行事务
04:45 - 04:48
we're mostly going to be doing reads 
我们大部分要做的还是执行读操作
我们要做的绝大多数操作都是读操作
04:48 - 04:55
but the amount of data we're going to be reading is much larger than what the OLTP transactions were doing from last class 
但我们要读取的数据量要远比我们上节课执行事务时所读取的数据量大的多的多


04:55 - 05:01
so I want to show what a sort of a typical set up would be in an analytical database
So，我想向你们展示下分析型数据库中的一种标准设置
05:01 - 05:04
and this doesn't necessarily have to be a distributed database 
分析型数据库不一定就得是一个分布式数据库
05:04 - 05:06
but this is a common arrangement 
但这是一种常见的安排
05:06 - 05:08
so on the front end you have your OLTP databases
So，在前端我们有一些OLTP数据库
5.08-5.13
 ,that this is where you're ingesting new information from the outside world 
这是你从外界收集信息的地方
05:13 - 05:15
and this could be distributed could be single node
它可以是分布式的，也可以是单节点数据库系统
5.15-5.15
 it doesn't matter 
这都没关系
05:16 - 05:21
and then you want to get all the data from these front-end data silos into your back-end analytical database
接着，你想从这些前端data silos（数据孤岛）处获取数据，并放入后端分析型数据库中
5.21-5.23
sometimes called a data warehouse 
有时这也叫做数据仓库


05:24 - 05:28
so there's this process called ETL extract transform the load 
So，这个过程叫做ETL（提取、转换、加载）
05:28 - 05:33
so there are tools you can buy that do this we just write Python scripts or do whatever you do manually 
So，你可以去购买这样的处理工具来处理这些数据，我们也可以写一些Python脚本来进行处理，或者你可以手动处理


05:33 - 05:34
but the idea is that
但这里的思路是
5.34-5.38
 to take all the data from these different front-end OLTP databases 
我们先从前端所有不同的OLTP数据库中获取全部数据
05:38 - 05:43
and put it into a universal schema inside your data warehouse 
接着，将这些数据放入你数据仓库的universal schema中进行处理
05:43 - 05:44
so for example
So，例如
5.44-5.46
let's say you have your front-end database
假设，你有一些前端OLTP数据库
5.46-5.50
, one application has that you know they all have different customers names 
这些数据库中保存了不同的客户名
05:50 - 05:52
but this one has F name for the first name
但这个数据库中保存的是First name以F开头的用户信息
5.52-5.54
this one has first underscore name 
这个数据库中保存的是以下划线_为开头的用户信息
05:54 - 05:57
so you can't just chuck that all into a single database
So，你不能直接将所有数据都放入一个数据库中
5.57-5.59
because the database doesn't know that F name equals first name 
因为数据库并不知道它的First name是否以F开头
05:59 - 06:03
so this is where you do that cleanup process here in the ETL world 
So，这就是你在ETL中数据清洗所要做的事情
06:04 - 06:06
so this is a very common setup right 
So，这是一种非常常见的设置
06:06 - 06:08
if you're gonna build a startup
如果你要构建这样一种系统
6.08-6.09
you typically start with this
你通常就应该从这方面入手
6.09-6.11
because you need to get data first 
因为你需要先获取数据
06:11 - 06:13
and then once you have a lot of data,
接着，一旦你拿到了大量数据
6.13-6.15
 then you want to put it into your back-end data warehouse 
 然后，你会想将这些数据放入你的后端数据仓库中
06:16 - 06:16
and the idea is that
这里的思路是
6.16-6.18
 we don't wanna do analytics on the front-end
我们不想在前端数据库中进行分析处理
6.18-6.20
because that's gonna slow down our transactions 
因为这会降低我们事务执行的速度
06:20 - 06:22
so you can put this into our back-end data warehouse 
So，你可以将这些业务放到我们的后端数据仓库中
06:23 - 06:24
so this is what we're focusing on today
So，这就是我们今天所关注的内容
6.24-6.25
 how do we actually do this 
我们实际该怎样做到这点


06:27 - 06:30
so I use the term OLAP
So，这里我使用了OLAP这一术语
6.30-6.32
 online analytical processing
即在线分析处理
06:33 - 06:36
sometimes you'll see these types of systems referred to as a data warehouse
有时你们会将这类型系统叫做数据仓库
06:36- 06:40
 or more traditionally  sometimes they call it decision support systems DSS 
或者，有时更传统的说法叫做决策支持系统（decision support systems DSS）
06:40 - 06:41
and again the idea is that
它的思想是
6.41-6.47 ！！！！
 these are applications we're going to write now in our back-end data warehouse
这些是我们现在要在后端数据仓库中编写的应用程序
06:47- 06:50
That it's gonna analyze the data we have
它会对我们所拥有的数据进行分析
6.50-6.54
 ,that we've kept from the OLTP side extrapolate new information 
这些数据是我们从OLTP数据库那里拿到的，并以此推断出新的信息
06:54 - 07:00
and then guide our decision-making processes for the business for the organization or for the OLTP allocations 
然后用于对我们的组织业务或OLTP的分配的决策指导


07:01 - 07:05
right so very common set up the one is they've always like to use is like zinga 
我们最喜欢用来讲解的一个例子就是Zinga（一家社交游戏服务提供商）
07:05 - 07:08
zinga has all their stupid farmville games up here 
假设，这里是Zinga他们推出的游戏Farmville
07:08 - 07:09
right these are OLTP databases 
这是他们的OLTP数据库
7.09-7.11
for every click in the game
游戏中的每次点击

7.11-7.13
 ,that's another transaction or another update to the database 
其实就是一个事务，会对对数据库进行更新
07:14 - 07:19
but then there's a shove all those clicks into the backend data warehouse 
但接着，他们将所有的点击事件所产生的数据发送到后台数据仓库中
07:19 - 07:20
do some analysis on this 
并对这些数据进行分析
7.20-7.27
at whether your decision support systems or machine learning to try to figure out some of, how to make you buy more stuff on the front end ,right 
你的决策支持系统或者机器学习会去弄清楚如何让用户去购买更多的道具
07:28 - 07:33
like the the one example, I always heard was it's the candy crush game 
我经常听到的一个例子就是Candy Crush（糖果传奇）这个游戏
07:33 - 07:35
if you played the candy crush game 
如果你之前玩过糖果传奇
7.35-7.38
and so you get all these updates on the OLTP side 
so，你会从OLTP一侧数据库中获取到所有这些更新
07:38 - 07:41
and then it's a you get a hard puzzle and you can't beat it, right 
当你遇上难以通过的关卡时
07:42 - 07:43
and so you put the game down 
你就会结束游戏
07:43 - 07:46
so they're gonna collect all this click stream to see how you played the game 
So，他们就会去收集这些点击数据，以此来弄清楚你是怎么玩这游戏的
07:46 - 07:47
and then they'll learn that oh
然后，它们就可以学到这么一招
7.47-7.52
if you come back after not having played the game for like a day because you got frustrated 
在你因为沮丧而很久不玩这游戏，然后在回归这游戏时

07:52 - 07:55
they make sure you they give you an easy puzzle that you can solve right away
他们为了确保你能够轻松通关，会给你降低难度
7.55-7.57
, so you get hooked again, right
So，你就会再度沉迷
07:57 - 07.58
and then keep playing it 
并继续玩这游戏
7.58-7.59
,because they know if they give you a hard puzzle
因为他们知道，如果给你一个困难关卡
7.59-8.01
, you'll get frustrated and never come back ever again
你就会变得沮丧，并且不再玩这游戏了
08:02 - 08:04
right so that extrapolating that information that
So，他们就会提取出这样的信息
8.04-8.09
oh this is how I give you know the person, you know a game that they'll be able to beat 
他们会给出你能够过关的关卡
08:09 - 08:11
you figure that out on this side
你会在OLAP数据库中弄清楚这些
8.11-8.13
and then you push the update to the OLTP side 
然后，你将这些更新信息推送到OLTP数据库这边


08:15 - 08:17
so the in general 
So，总的来讲
8.17-8.25
at a high level, there are two different ways you can model a database application on a back-end data warehouse or analytical database 
从高级层面来讲，你可以在后端数据仓库或者分析型数据库上从两个不同的方面进行程序建模

08:25 - 08:27
so you could take the standard schema
So，你可以使用常规方案
8.27-8.31
, that you know your application would have like typically it's usually a tree schema 
你知道的，你的应用程序通常会使用一种tree schema
08:32 - 08:32
because you have this hierarchy
因为你这种层级结构确实很容易设计数据结构和算法，
8.32-8.33
,I have customers
我有用户信息
8.33-8.34
, customers have orders
用户拥有对应的订单信息
8.34-8.35
 ,orders have items 
订单信息中又包含了商品信息
08:37 - 08:39
but those schemas can be quite messy
但这些schema可能很混乱
8.39-8.42
and they're not going to be very efficient for analytical queries 
对于分析型查询来说，它们的效率并不高
08:43 - 08:44
so instead
So，相反
8.44-8.48！！
 you would model your database using either what's called a star schema or a snowflake schema 
你会使用Star schema或者SnowFlake Schema来对你的数据库进行建模
08:48 - 08:53
and sometimes you see analytical databases, that they'll say hey we only support star schemas 
有时，你们会看到一些只支持Star schema的分析型数据库
08:53 - 08:54
you can't do a snowflake schema
你无法在这些数据库中使用Snowflake schema
8.54-8.58
 ,you'll see this is basically a subset of this 
简单来讲，Star Schema其实是Snowflake Schema中的一个子集
08:58 - 08.59
but you see why
但在后面你会看到我为什么这么讲
8.59-9.02
 let's discuss why this matching might be better for it some analytics 
让我们来讨论下为什么这种方案对于某些分析来说要来得更好



09:04 - 09:08
so a very common arrangement would be something like this
So，如图所示，这是一种非常常见的安排
09:08 - 09:10
so this is a star schema 
So，这就是一个关于Star Schema的例子
09:10 - 09:12
and you have two types of tables in a star schema
在Star Schema中，你有两种类型的表
9.12-9.14
you have facts tables and dimension tables 
即Fact table和Dimension Table（看图中的后缀名）
09:14 - 09:17
so the middle of the star is the fact table 
So，中间这张表就是Fact Table
09:17 - 09:20
think of this is like whatever event you're trying to mode
不管你要对哪种事件进行建模
9.20-9.23
this is where you store all the occurrences of them 
这里就是你保存所有出现事件的地方
09:23 - 09:24
so if you're like Walmart 
So，以沃尔玛为例
9.24-9.32
and your data warehouse keeps track of every single item that  anyone has ever bought, at any Walmart stored at any given time 
你的数据仓库会跟踪在任意给定时间里任何人在任何沃尔玛门店处所购买的每个商品信息

09:32 - 09:36
all of those items getting scanned at the cash check or the checkout counter
收银台会扫描所有这些商品信息
9.36-9.38
, that's another event that we've put in our fact table 
这就是我们要放入我们Fact Table中的一个事件
09:39 - 09:43
so this thing is gonna be massive, like hundreds of billions of records 
So，这些东西的数据量很大，可能有上百亿条记录
09:43 - 09:43
same thing Amazon
Amazon也是如此
9.43-9.47
 every single item that anyone's ever bought on Amazon goes and you're in your fact table 
任何人在Amazon上所购买的每个商品都会进入你的Fact table



09:47 - 09:53
but we're not actually going to store any information about what these items are that someone bought 
但我们实际不会去保存任何关于这批商品的购买者是谁的信息
09:53 - 09:57
 have foreign key references to our outer dimension tables
我们会使用外键引用来指向这些dimension table



09:58 - 10:01
where they're going to maintain that additional information 
它们（dimension tables）会去维护这些额外信息
10:01 - 10:02
all right

10.02-10.03
cuz can this is things we massive
因为这些数据量太大了
10.03-10.06
we want this to be through as trim as possible
我们想让我们的Fact Table尽可能地精简
10.06-10.08
because we have billions of rows 
因为我们拥有数十亿行数据
10:08 - 10:11
so we put all the actual metadata  in  the dimension tables 
So，我们会将这些元数据放在dimension table中
10:11 - 10:13
but  in a star schema
但在Star Schema中
10.13-10.20
, you can only have one one level dimension tables out from the center of the star 
在星星的中心外侧，你只能拥有一层dimension table(知秋注：将整个体系图看作是一个四角星结构，只针对这个例子，因为可能是更多角的星)
10:21 - 10:25
Right, so there's no additional tables over here that these guys can join with 
So，这里没有其他额外可以加入的表
10:26 - 10:26
Right, in this case here,
在这个例子中



10.26-10.28
I have a category named a category description 
我有CATEGORY_NAME和CATEGORY_DESC这两个字段
10:28 - 10:35
so I could extract that out and normalize that store that has another dimension table with another foreign key going from this to this 
So，我可以将它们提取出来，对它们进行规范化设定，然后保存到另一张dimension table中，并在通过外键来将这两张表关联起来（知秋注：所谓的规范化设定，就好比我们看到一个Integer，就能知道它代表什么意思）
10:35 - 10:39
but under a star schema, you know you're not allowed to do that 
但在Star Schema中，我们不允许你这么做
10:39 - 10:41
and we take a guess why 
你们可以猜下这是为什么
10:45 - 10:45 
yes 
请问
10:48 - 10:48
exactly
没错
10.48-10.53
so you said the time it takes you to diverse or join those different tables is going to be expensive 
正如你说的，对这些表进行join操作所要花的时间成本实在是太高了
10:53 - 10:56
because again we're not doing like find where Andy's items
因为我们并不会去做诸如：请找到Andy购买的商品清单之类的事情
10:56 - 11:02
we're saying find all the items that the you know that the state of Pennsylvania has bought within this date range
假设我们要做的是找出这段时间内，宾夕法尼亚州中购买的所有商品
11.02-11.05
can be hundreds of millions of rows
这可能就会有上百万行记录
11:05 - 11:09
so we want to avoid as much as doing as many joins as possible 
So，我们想尽可能避免做太多的join操作


11:10 - 11:14
so a snowflake schema is where you're allowed to do have multiple dimension outside of it 
So，在Snowflake schema中，它允许你拥有多层dimension table



11:15 - 11:16
right so again going back up here ,right 
So，来看这里
11:16 - 11:19
I have now can break out my category information
这里我将我的分类信息拆分了出来
11.19-11.21
 I have a foreign key in the product dimension table 
我在我的PRODUCT_DIM表中建立了外键



11:21 - 11:23
and then I have what is called now a lookup table
接着，我将这个拆分出来的表叫做CAT_LOOPUP表

11.23-11.26
which is the things beyond the dimension table
这里面保存了PRODUCT_DIM表以外的一些信息
11.26-11.30
where I have that normalized information as the output 
我将这些范式化信息作为输出结果
 用于作为规范化的信息（normalized information）来作为输出结果
11:30 - 11:31
and as I said
正如我说的
11.31-11.40
 a some database system , some OLAP systems will say explicitly, you can't have look-up tables ,you can't have multiple levels beyond the first dimension table 
某些OLAP系统明确表示，你不能有这种查询表，你也不能拥有多层dimension table



11:42 - 11:49
so the main sort of two issues in this world as she said one of them is performance 
So，正如她所说，其中一个主要原因就是性能问题
11:49 - 11:53
the other one is actually going to be the integrity of the data that were storing
实际上，另一个原因则是这些被保存数据的完整性（知秋注：非规范化的数据模型可能会导致完整性和
一致性问题）



11:55 - 11.57
again so going back here,
So，回到这里
11.57-12.01 ！！！！！！！
if I collapse down my lookup table into a single dimension table 
如果我将我的查找表合并到一个dimension table中
12:01 - 12:04
well I'm gonna be repeating the category name over and over again 
我就会遇上不断重复的分类名称(category_name)
12:04 - 12:06
so now if the category name changes
So，如果分类名称(category_name)发生了变化
12.06-12.12
 ,I need to make sure in my application code, that I go update all the records that have that same category name 
在我的应用程序代码中，我需要确保我更新了所有包含有该分类名称的records （记录条目）

12:12 - 12:14
so that everything is in sync 
这样的话，所有的东西都是同步的


12:14 - 12:17
if I'm normalized out like this in the snowflake schema
如果我将数据归类成Snowflake schema中的样子
12.17-*12.17
 I don't have that problem
我就不会遇上这种问题
12.17-12.19
 ,because I want have one entry for the category 
因为我会有一个分类条目(知秋注：如果某一分类发生了变化，比如某一个分类id对应的category_name发生了改变，我只需要更新该分类id对应的分类条目的category_name即可，其他无须改变)


12:21 - 12:24
right so if you do a star schema 
So，如果你使用的是Star Schema
12.24-12.32
then this extra work that you have to do in your application to make sure that your denormalized tables are are consistent 
你必须在你的应用程序代码中做些额外工作，以确保这些非规范化（denormalized ）的表中的数据都是一致的

12:32 - 12:35
you now actually potentially storing more redundant information that's unnecessary 
实际上，你们可能会去保存更多不必要的冗余信息
12.35-12.39
,and so the size of your database could be larger 
So，你数据库的体积可能就会变得更大
12:39 - 12:40
it's not that big of a deal
这并不是什么大问题
12.40-12.46
, because again the fact table is the main juggernaut in this model 
因为Fact table才是这个模型中的重点
12:46 - 12:47
and we will have ways to compress that down
我们可以通过多种方式对其进行压缩
12.4712.52
 ,so the the storage overhead of denormalized tables is not that big of a deal 
So，这些denormalized 表的存储开销其实不是什么大问题
12:52 - 12:54
it's more the integrity stuff that's more important 
更重要的地方在于数据库的完整性
12:55 - 12.56
and then as she said 
正如她所说
12.56-13.05
,the complexity of the queries with a star schema are gonna be significantly less than the complexity of the queries in a snowflake schema 
Star Schema中查询的复杂度明显小于Snowflake Schema中查询的复杂度
13:05 - 13:09
because there's only so many joins I could possibly do 
因为我join操作的工作量就那么多
13:09 - 13:11
I only have to go sort of one level deep 
并且我dimension table的层级只有一层
13:11 - 13:15
as we talked about a query optimization 
正如我们讨论查询优化时所说
13:15 - 13:19
the having more tables join against, just makes everything super harder, when we have to figure out the join ordering 
当我们需要弄清楚join操作的顺序时，如果表数量越多，那么进行join操作时，事情就会变得超级难
13:20 - 13:28
and so by restricting ourselves to a star schema, but may end up finding a the optimal plan whereas the snowflake schema we may not be able to 
So，我们为了限制自己并去使用Star Schema，最终我们会找到一种我们在Snowflake schema中可能无法做到的更优方案
13:30 - 13:32
so again when you go out in a real world
So，当你踏入社会，
13.32-13.33
 ,if you come across data warehouses
如果你遇到数据仓库
13.33-13.36
 you're likely to see either these two approaches 
你可能就会看到这两种方案
13:36 - 13:38
Because they're better for analytics 
因为它们更适合用于分析处理
13:39 - 13:45
and this distinction between the dimension table and the fact table will come up when we start talking about doing joins
当我们开始讨论执行join操作时，我们会去讲下dimension table和fact table之间的区别
13:46 - 13:52
because we need to decide how we're going to move data between nodes， if we can't do any local joins on our machines 
因为如果我们无法在本地机器上执行join操作，我们就需要决定该如何在节点之间移动数据

13:53 - 13:53
okay 
============================


13:55 - 13:58
all right so let's talk about the problem we trying to solve today 
So，我们来讨论下我们今天要尝试解决的问题
13:58 - 14:02
and I've already sort of briefly just mentioned just now 
其实我刚刚已经提到过了


14:02 - 14:06
so our query shows up in a master node that wants to join on R and S 
So，在master节点处有我们的一个查询，它想对R表和S表进行join操作
14:06 - 14:09
and let's say that the two tables R and S
假设我们有两张表，即R表和S表
14.09-14.14
 ,are just split across the different partitions on the different nodes uniformly 
这两张表被均匀地拆分到不同节点上的不同分区中
14:15 - 14:20
so what's the stupidest way for me to execute this query in this set up 
So，对我来说，在这种情况下，执行这个查询的最笨方式是什么？

14:26 - 14:26
yes 
请讲


14:28 - 14:29
exactly right
说的没错
14.29-14.32
,the dumbest thing I could do and it would work it would still be correct, is that
为了能得出正确结果，我所能使用的最笨办法是
14.32-14.36
 I know I need touch data set partitions two three and four 
我知道，我需要去访问分区2、3和4中的数据
14:36 - 14:40
and so I just copy them in their entirety up into the the node where partition one is, 
So，我直接将它们的数据整个复制到分区1中
14.40-14.42
now all my data is local
现在我的数据就都在本地了
14.42-14.44
I do my join and spit back the result 
我执行我的join操作，然后返回结果
14:45 - 14:46
why is that stupid
为什么这样做很笨呢？
14:48 - 14:48 
yes 
请讲
14:50 - 14:52
he says me you may not need all the data.
他表示我们可能不需要全部数据
14.52-14.53
, potentially yes
部分正确
14.53-14.55
 but team Team stupider 
但还有更蠢的原因
14:56 - 14:56
yes 
请讲
14:59 - 15:00
absolutely
说的没错
15.00-15.02
 yeah you're not doing any computation on the other nodes 
这样做的话，你就不会在其他节点上执行计算了
15:02 - 15:05 
like this defeats the whole purpose of having your distributed database 
这就违背了你使用分布式数据库的目的
15:06 - 15:07
all right, think about what I did 
思考下我这里所做的事情
15.07-15.08
I bought a bunch of machines
我买了一堆机器
15.08-15.10
 I partition my table across these machines 
我将我的表拆分到这些机器上
15:11 - 15:12
but then my query shows up
但接着，数据库要执行我的查询
15.12-15.14
 and I just copy things back over to the singing machine anyway
然后，我将这些数据又复制到了一台机器上
15:14 - 15:18
so I bet it would have been better off just buying this one machine and doing the join there 
So，我为什么不干脆只买一台机器，并在这台机器上执行join操作呢？
15:20 - 15:22
right so this is the problem we're trying to solve today 
So，这就是我们今天试图解决的问题
15:22 - 15:23
we're trying to say 
我们试着说
15.23-15.24
,all right 

15.24-15.26
if a query shows up and it wants to do a join
如果我们拿到一个查询，它想去执行一次join操作
15.26-15.30
 , we need access data that's put now across multiple resources 
我们需要访问的数据分布在多个机器资源上
15:30 - 15:32
how do we actually do this efficiently 
我们实际该如何有效地执行这个操作呢？
15:32 - 15:39
what do we need to be mindful of when we decide whether we move data, or copy data or push the query, or pull the results 
当我们选择移动/复制数据，或者推送这些查询、拉取结果时，我们需要注意哪些东西呢？
15:40 - 15:43
all right, these are all bit we have to deal with today 
这就是今天我们所要面对的东西
15:43 - 15:47
this also assumes in this example that my database can't  fit in on a single node 
在这个例子中，我们假设我的数据库无法放在一个节点上
15:48 - 15:50
right again think of like the Walmart database
思考下沃尔玛的数据库
15.50-15.52
it's whatever hundreds of petabytes,
它的大小差不多是数百PB
15.52-15.54
 it's not gonna fit on a single machine 
它无法存放在单个机器上
15:54 - 15:58
so we're gonna happen run distributed environment order to get any work done
So，我们想让它放在分布式环境下运行，以此来完成任何工作
15:59 - 16:00
OLTP it's not an issue
OLTP则没有这种问题
16.00-16.03
OLTP I'm only touching andy`s data 
在OLTP中，我只涉及Andy的数据
16:03 - 16:07
andy's data is maybe couple hundreds of kilobytes or megabytes 
Andy的数据可能就只有一两百KB或者MB
16:07 - 16:12
right it's not I can easily fit that on a single box and do all my transactions on that one box
我可以很容易地将它放在一台机器上，并在该机器上执行我的所有事务
16:13 - 16:17
in analytics, I'm trying to touch the entire table or large portions of the table
但在进行分析的时候，我会试着访问整表数据或者该表中的大部分数据
16.17-16.19
, I'm not going to be able to do everything on a single node 
我无法在一个节点上干所有事情


16:20 - 16:26
so today our focus is going to be on first discussing the execution models  we have in a distributed database system for analytics
So，今天我们的重点首先是讨论下分布式数据库系统中用于处理分析的执行模型

16:26 - 16:29
we've already briefly touched on this a little bit in the first lecture
我们已经在第一节课的时候简单接触了下这个
16.29-16.32
, but now we'll talk about it more more concretely and see why it matters 
但我们现在会更具体的讨论下它，并看看它为什么很重要
16:32 - 16:35
then let's briefly talk about the issues doing query planning,
接着，我们会简单讨论下我们在执行查询计划时所遇到的问题
16.35-16.37
 then we'll talk about how we do distributed joins 
然后，我们会讨论如何执行分布式join操作
16:37 - 16:42
the spoiler would be that all the algorithms we talked about early in the semester still remain, we still do them
我们这学期前面所讨论过的那些算法，我们这里依然会去使用它们
16:43 - 16:46
there's no magic distributed join that doesn't exist on a single node 
这并没有什么神奇的，分布式join操作不会只在一个单节点上进行
16:46 - 16:48
it's just the question is 
这里的问题是
16.48-16.52
again where do we move data or where do we move the computation 
我们要将数据移动到哪，我们要将计算移动到哪
16:52 - 16:59
and then I'll finish up with sort of a quick smattering of , what this sort of state of the art of cloud databases look like, in the world today
然后，我们讨论下当下的云数据库是什么样的
17:01 - 17:05
and just you know just it again ,you'll see how just because it's in the cloud 
因为这些数据库是放在云端的
17.05-17.08
doesn't mean we still don't care about all the things, we talked about the entire semester
这并不意味着我们就不会关心我们整个学期所谈论的东西
17.08-17.09
, okay 



17:11 - 17:12
all right
======================
17.12-17.13
so as I said
So，正如我说的
17.13-17.19
 I briefly touch about this first issue for the execution model， when we talked about just to the introduction to distributed databases 
当我们在介绍分布式数据库的时候，我简要介绍下执行模型中的第一个问题

17:19 - 17:26
but the two approaches we have to execute a query are either a push VS. a pull 
我们执行查询的方式有两种，push和pull
17:27 - 17:28
so with a push
So，我们来讲下push
17.28-17.29
 the idea is that, 
它的思路是
17.29-17.39
we want to send the the query or the portion of the query like the plan fragment, to the location of where the data is located
我们想将查询或者该查询的碎片发送到数据所在的地方（知秋注：查询碎片：分割任务后的子查询元素）
17:39 - 17:42
then run that portion of the query at that local data 
接着，在该位置上执行这部分查询
17:42 - 17:46
and then now just send back the result to whoever asked for it 
然后，将执行结果返回给要求执行该查询的人
17.46-17.50
like, the home node or the base node that's coordinating the query 
比如，用于协调该查询的home节点或者base节点




17:52 - 17:53
the idea here is that 
第二种方案的思路是
17.53-17.59
we want to just as we we do projection push down or predicate push down on a single node system 
就如同我们在单节点数据库系统中所做的projection下推和判断条件下推一样
17:59 - 18:07
we want to filter out and remove as much useless data as early as possible Before we send anything over the network 
在我们将任何数据通过网络发送之前，我们想尽早地过滤并移除掉无用的数据

18:08 - 18:14
so if we can send a portion of the query to where the data is located ,crunch on it there, do some early filtering 
如果我们将该查询的一部分发送到数据所在的位置，并对数据进行早期过滤
18:14 - 18:17
and then when we transmit the data back to another node 
接着，当我们将数据传输回另一个节点时
18:17 - 18:20
we're not just blindly copying all the data that the node has
我们不会去盲目地复制该节点中的所有数据
18:20 - 18:25
we're just limiting it to the subset of that we actually need for this particular query 
我们只会去复制该查询所需要的那部分数据
18:26 - 18:28
now we'll see in a second 
我们稍后会看到这个
18:29 - 18:32
the lines get blurred and I shared disk system whether you're doing one of versus another
在shared disk系统中，这些界限其实很模糊
18:32 - 18:35
because in a shared disk system, in general you can't do any filtering 
因为在shared-disk系统中，通常你没法做任何过滤
18:36 - 18:38
because it's just a just you know read and write a single page
因为当你在对一个page进行读写时
18.38-18.39
 ,you can't do anything special 
你没法做任何特别的事情
18:40 - 18:43
but for shared disk systems again the lines are blurred 
但对于shared disk系统来说，这些界限都是模糊的
18:44 - 18:48
the other approach is to pull the data at the query which is what I shared disk system normally would do
另一种方案就是拉取该查询需要的数据，这也是shared-disk系统通常会做的事情
18:50 - 18:53
where we grab whatever the data we need for execute this query,
我们会去获取执行该查询所需要的数据
18.53-18.58
 we recognize based on the query plan, you know these are the pages I want to access 
基于查询计划，我们会意识到这些page是我想访问的page
18:58 - 19:03
we pull the data out make a copy of it transmitted over the network bring it to the node where our query is located 
我们会去拉取这些数据，并将这些数据的副本通过网络传输到我们查询所在的节点
19:04 - 19:05
Then we can then process it and crunch on it
接着，我们就可以对这些数据进行处理
19:06 - 19:07
and of course again
当然
19.07-19.09
 the issue here is that
这里的问题在于
19.09-19.10
 in an analytical system 
在一个分析型系统中
19:10 - 19:18
 the amount of data the size of the data are relative to the size of the query is gonna be quite different 
数据量的大小相对于查询的大小是相当不同的
19:18 - 19:22
like a query say it's just a SQL query couple kilobytes 
假设这个查询是SQL查询，它只有2kb那么大
19:23 - 19:27
the most I've ever heard from like Google or Facebook is that 
我从Google或Facebook那里听到最多的东西是
19.27-19.32
,sometimes they have queries that are like the SQL text itself is like 10 megabytes 
他们有的查询，比如SQL语句的文本差不多就有10MB那么大
19:32 - 19:34
Right, that's that's a large as query, 
这是一个很大的查询
19.34-19.38
but still that compared to like reading a terabyte of data it's nothing 
但比起读取1TB数据来说，这不算什么
19:38 - 19:41
so the thing we have to be mindful whether you want to one versus the other is
So，在使用这两种方案时我们要注意的东西是
19.41-19.44
 you know, where's the data located ,how can I access it 
这些数据是否在本地，我该如何访问这些数据
19:44 - 19:50
and is my query going to be larger or smaller in size to transmit over the network ,then the data I'm trying to access 
通过网络传输我的查询时，我查询的体积是会变大还是变小，接着就是我要试图要访问的数据的体量是多少
19:51 - 19:55
and in analytics, it's always the case that the data you're trying to crunch on is larger .
在分析的过程中，通常你要处理的数据量会更大


19:56 - 19:59
so let's see this in the context of a shared-nothing system 
So，我们来看下这在shared-nothing系统中是怎么样的




19:59 - 20:03
so shared nothing systems are typically push data to push the query to the node 
So，shared-nothing系统通常会将查询推送到节点处


20:04 - 20:06
so my query shows up to this node here
So，我的查询会推送到这个节点处
20.06-20.10
, it's in charge of coordinating with the other node to process the join 
该节点会与其他节点一起协作来处理这个join操作
20:11 - 20:15
so we're gonna do our query planner I want to say on this node 
So，我们会去执行我们的查询计划，假设在上面这个节点处
20:15 - 20:21
if we had recognized oh we need to access the ID field for R and S tables 
如果我们意识到我们需要去访问R表和S表中的id字段
20:22 - 20:26
and I know that this node down here has a partition that I want to access 
并且我知道，下方这个节点中放着我想去访问的一个分区


20:26 - 20:30
so I'll just send information I'll send the query plan fragment down to this guy
So，我会将对应查询计划的片段发送给下方这个节点
20.30-20.35
, and say hey I know you have this data between 101 and 200, crunch on this join 
并说：hey，我知道你保存着id为101到200这个区间的数据，请对它们进行join操作


20:35 - 20:37
and then send me back the result
然后，将结果返回给我
20:37 - 20:44
and the node up above is responsible for taking the result of that this guy sent, plus it's local result 
上方这个节点会负责去获取下方这个节点所生成的结果，并将该结果与它的本地结果进行合并
20:44 - 20:47
combining them together to then produce a sing result back to the application 
并将合并完的结果返回给应用程序
20:48 - 20:51
so again we had this transparency issue or transparency guarantee
So，我们就拥有了这种透明性问题或者透明性保证
20.51-20.58
, where the application doesn't know ,doesn't care where the query actually executed long as it gets back a single result 
即应用程序不知道也不在意这个查询实际是在哪执行的，只要它拿到返回的结果就行了
21:01 - 21:03
so in a shared-nothing system
So，在一个shared-nothing系统中
21.03-21.07
 it's quite obvious, that you'd want to do push the query of the data 
很明显，你想将对应查询片段推送到数据所在的位置
 

21:07 - 21:10
right because this makes no sense for us to actually have to copy this data up and processes it there 
因为这样做的话，对我们来说，我们就不需要将数据复制到一个节点，再对数据进行处理了
21:10 - 21:13
it's just better just to send the query do the crunching locally for this example
对于这个例子来说，将查询发送到数据所在节点处，并在该节点处对数据进行处理，这种做法其实更好
21:15 - 21:18
, we'll see some scenarios where maybe you do want to do some copy 
我们会看些场景，在这些场景例子中，你可能想对数据进行一些复制
21:19 - 21:19
yes
请讲
21:23 - 21:24
your question is 
你的问题是
21.24-21.27
what if S is distributed as well then what 
如果S这张表也是分布在各处，然后你想说什么？
21:27 - 21:28
well in this case here,
Well，在这个例子中
21.28-21.29
 it is distributed
它是分布式存储的
21.29-21.32
 I'm just saying like it's partitioned just but it's partition
我想说的是它是保存在不同分区中的
21:32 - 21:34
well comment to your question later on like 
Well，我之后再去点评你的问题
21:37 - 21:38
correct
说的没错
21.38-21.39
 what we'll come to that later 
我们稍后会讲这个
21.39-21.40
in this simple example 
在这个简单例子中
21.40-21.43
assume that, R and s are both partition on ID
假设R表和S表都是根据id进行分区的
21:44 - 21:48
the ranges of the values at this partition are exactly the same 
它们在这个分区的id值范围都是完全相同的
21:48 - 21:50
so, therefore, I know when I'm doing a join at this node
因此，我知道当我在这个节点进行join操作时
21.50-21.54
, I don't need to look at any other partition in my cluster
我不需要去查看我集群中的其他分区
21:54 - 21:58
everything I need to have compute node for a single tuple in R, it`s located here
我计算所需要用到的R表中的数据都在这个节点处
22:01 - 22:03
but yeah exactly right I'm joining the partition key
说的没错，我是使用Partitioning Key进行join操作的
22.03-22.04
 ,well see those scenarios in a second
我们稍后会看些例子
22.04-22.05
 ,yes question
请讲

22:14 - 22:15
the question is
她的问题是
22.15-22.17
, in my example here,
在我的例子中
22.17-22.21
 my tables are partitioned on my join key
我的表是根据我进行join操作的那个key来分区的
22.21-22.22
, which is the best-case scenario
这其实也是最好的情况
22.22-22.23
, how often does that happen 
这种情况出现的频率是怎么样的呢？
22:29 - 22:32
usually for the fact table pretty often 
通常对于fact table来说，这是很常见的
22:33 - 22:34
right like 

22:37 - 22:39
I would say it's pretty common 
其实我应该说这很常见
22:40 - 22:41
let me think about it like
让我思考下
22.41-22.42
 in a real system like
在一个真实系统中
22.42-22.46
 so I want to partition, I partitioned on user IDs 
假设我想进行分区，我会使用用户id进行分区
22:48 - 22:51
and you know the fact table could say
fact table表示
22.51-22.54
 here's this person who bought this item 
这个人买了这个商品
22:54 - 22:54
and so 

22:56 - 22.58
it's a bad example 
这是个糟糕的例子
22.58-23.04
,so say I want to do a join between like, here's the user IDs and here's all their items that they bought 
假设，这里是用户id以及用户所购买的所有商品
23:04 - 23:08
and then here's the session of how they visited the webpage 
接着，这里是他们访问网页的session
23:08 - 23:10
just to figure out what items I looked at before I bought something 
在我购买任何东西之前
23:10 - 23:14
anyone trying to figure out to learn like, oh if they go look at these much items 
有人会试着去弄清楚，Oh，如果有人买了这么多东西
23:14 - 23:16
then they're more likely to buy something 
那么，他们就会更乐意买这些东西
23:16 - 23:17
and so in that case
So，在这个例子中
23.17-23.19
 the user ID would be the partitioning key
用户id就是partitioning key
23.19-23.20
 and that would work out nicely 
并且它的效果非常不错
23:20 - 23:22
so it's not always the case 
虽然我们不一定总是遇上这种情况
23.21-23.23
,but I would say it's common enough 
但我表示这足够常见
23:23 - 23:26
but we'll see in a second how we handle case where it's not this like this 
但我们稍后会看我们如何处理一个不是这种情况的例子
23:26 - 23:28
the main thing I emphasized here is 
这里我主要强调的东西是
23.28-23.33
we can push the query to the data , and that's be better for us
我们可以将查询推送到数据所在位置，这样做对我们来说会更好
23:33 - 23:36
because this this data is gonna be larger than then this guy here 
因为这些数据可能要比这个节点上数据的量还要大
23:36 - 23:40
plus we can also get the the additional benefit that we parallelize the computation
并且我们也能获得额外的好处，即并行化计算
23:40 - 23:42
because now the top node doesn't have to do all join
因为上面这个节点不用去执行所有的join操作了
23.42-23.45
, this guy can do the join you know portion the join
这个节点会负责部分join操作
23:45 - 23:46
 ,and this guy can do the other portion the join 
下面这个节点则会负责另一部分join操作
23:46 - 23:48
and we just combine it together
我们将这两个结果合并在一起
23.48-23.49
 ,and the combined part is cheaper 
合并部分的成本更低
23:49 - 23:51
it's cheap relative to the join cost 
相对于join操作的执行成本来说，合并的成本相对较低



23:54 - 23:57
so the other approach is the pull the data to the query 
So，另一种方案是将数据拉到查询这边
23:57 - 24:00
again this is where I'm saying in a shared disk system the lines are blurred 
正如我说的，在shared disk系统中，这种界限很模糊


24:01 - 24:04
so we send our query to this node here 
So，我们将查询发送到这个节点


24:04 -24:07
this node would recognize that we've logically partitioned the data
这个节点意识到，我们对数据进行了逻辑分区
24.07-24.10
 , such that this node down here is responsible for this range 
比如：下面这个节点负责101-200范围的数据
24:10 - 24:12
that node on top of that range 
上面这个节点负责1-100范围的数据


24:12 - 24:16
so then they then go to the the share disk 
So，接着它们跑到共享磁盘这里


24:16 - 24:18
go access those pages,
访问这些page
24.18-24.21
 pull back the results or so pull back the pages they need
并拉取他们需要的数据
24.21-24.23
 ,then they compute their local join
然后，它们在节点本地执行join操作


24:24 - 24:27
and then this guy shows up the result to the other one 
然后，这个节点会将执行结果向另一个节点展示


24:27 - 24:34
so again this step here would be pulled of data to the query 
So，这一步其实是将数据拉取到查询所在的节点
24:34 - 24:36
because I'm just blindly asking for pages where it's located here
因为这里我只是盲目地去询问page所在的位置
24.36-24.38
, and I have to copy it over here
然后，我需要将数据复制到这里


24:38 - 24:42
but then certainly this part here was push the query to the data 
但在这部分，我们将查询推送到数据这里
24:42 - 24:46
because this guy was able to you know compute the join locally ,and send the result up
因为下面这个节点能够在本地计算join操作，然后将结果发送给上面这个节点
24:47 - 24:50
so would you say that this is a pull versus a push 
So，你们可能会问这是pull还是push呢？
24:51 - 24:52
again it's both 
两者都有
24:55 - 25:00
well see it it will, I don't think I've a slide about this but 
我不觉得我做了与这有关的幻灯片
25:02 - 25:07
the cloud vendors are recognizing about having a dumb disks, if you want to call it that 
云厂商意识到他们要转储磁盘，如果你想这么称呼的话
25:08 -25:10
Andy shared-disk databases is a bad idea  .
Andy表示shared-disk数据库是一个糟糕想法


25:11 - 25:15
because again I'm just always copying this page, without checking to see whether actually need data on the page 
在不检查我所需的数据是否在这个page上的情况下，我始终要去复制这个page
25:16 - 25:17
I just know that I think I need to look at it,
我觉得我需要去查看下这个page
25.17-25.19-
 so I just say go give me this page 
So，我就会说，请给我这个page
25:19 - 25:20
on like Amazon s3
以Amazon S3为例
25.20-25.21
 they now have a filter command
它们有一个过滤相关的命令
25.21-25.23
 where you actually can do predicate push down 
实际上，你就可以将判断条件下推
25:24 - 25:25
and even when you say go get this page
当你表示要获取这个page时
25:25 - 25:26 
you can also say like,
你可以说
25.26-25.30
 oh but it also check this filter for me to see whether everything actually matches inside the page 
你要帮我去过滤这些数据，帮我检查下该page中这些数据是否符合我的条件
25:30 - 25:32
and if yes then send it to me if no then don't
如果符合我的条件，那么就将数据发送给我，不然就不用将数据发送给我
25:33 - 25:36
Right, so again that's pushing the query to the data 
So，这就是将查询发送到数据所在位置的工作方式
25:37 - 25:39
so again the lines are blurred 
So，这些界限很模糊



25:41 - 25:41
okay 

25:43 - 25:46 !!!!!!!!
so one thing we talk about though is that
So，有个东西我们要讨论下，
25.46-25.52
, I said that last class we made a big deal about 
在上节课的时候，我有说过，
25:52 - 25:55
if we have a transaction commit and it touches multiple nodes 
如果我们要提交一个事务，该事务涉及多个节点
25:55 - 26:03
I want to make sure that everyone agrees ,that all the nodes would have to agree that this transaction is allowed to commit， before we tell the outside world then it committed 
在我们告诉外界该事务被提交之前，我想确保所有节点都同意我们提交这个事务

26:03 - 26:06
all right because we're modifying the database
因为我们修改了数据库
26.06-26.07
we don't want to lose any changes 
我们不想丢失任何修改
26:07 - 26:08
but in an OLAP database
但在OLAP数据库中
26.08-26.11
we're just doing read-only queries 
我们只是执行只读查询
26:13 - 26:18
so we're not really worried about updating DBMS multiple locations and keeping those in sync 
So，我们并不担心要对多个地方的DBMS数据进行更新，并让数据保存同步
26:18 - 26:20
but now we have to deal with the case
但现在我们需要处理这种情况
26.20-26.22
 where a node could crash,while we're processing the query
当我们处理一个查询时，有一个节点崩溃了
26.22-26.25
 ,and we had to figure out how to handle that 
我们需要弄清楚该如何处理这个问题
26:25 - 26:28
so the important thing to understand is that 
So，这里要理解的重要事情是
26.28-26.33
when we go request data from another machine or the shared disk 
当我们从另一台机器或者共享磁盘处请求数据时
26:33 - 26:36
and  we get the copy of that data when we receive that 
我们会去拿到该数据的副本
26:36 - 26:41
that's gonna get stored in the buffer pool just like any other data we have that we read from disk 
就像我们从磁盘中读取到的其他数据一样，这些数据会被放入buffer pool中

26:41 - 26:45
but it's stored in like a temporary buffer space
但它是被保存在一个临时buffer空间中
26.45-26.48
 ,meaning it could get page to the disk
这意味着它可以将page写入磁盘
26.48-26.49
 , because we run out of space 
因为我们会耗尽磁盘空间
26:50 - 26:52
but if we crash or restart the system to come back 
但如果系统崩溃重启后
26.54-26.55
,all that temporary spack disk is blown away
所有临时空间中的数据都会消失不见
26:55 - 27:03
because the the query or the transaction that was protected the query that was , that was running that needed that data is now gone, because I crashed 
因为系统崩溃，所有执行该查询所需的数据都消失不见了，
27:03 - 27:05
and so I don't need to persist anything
So，我不需要对任何数据进行持久化
27:06 - 27:12
so these OLAP queries for really large databases can take a long time 
So，对于大型数据库来说，这些OLAP查询要花的时间很长
27:13 - 27:16
it's not unheard of to have queries that could take hours 
这种要花数小时执行的查询并非闻所未闻
27:17 - 27:19
I've also heard of queries that take days
我甚至还听说过那种要执行数天的查询
27:21 - 27:24
for column stores that's gotten much better 
对于列式存储来说，它们的执行效果会更好
27:24 - 27:24
but back in the old days 
但在以前
27.24-27.32
it was certainly very common for like,  a quarter to take days, like you want you run your reports once a month, and takes like a week to run it 
这相当常见，就好比当你每个月要生成一份报告时，你就要花1周的时间去执行这个查询
27:32 - 27:34
so if we had this long running query
So，如果我们正在执行这个要慢查询
27.34-27.37
 and our node crashes what should we do 
如果我们的节点发生了崩溃，我们应该做什么呢？
27:37 - 27:38
it's not a correctness issue,
这并不是正确性问题
27.38-27.40
 because we weren't updating anything 
因为我们不会去更新任何数据
27:40 - 27:45
but ideally be nice  may be that we may not have to restart the whole thing from scratch if it's gonna take days 
但理想情况下，如果这个查询要花数天时间执行，如果中途系统出现问题，但我们无需从头开始执行这个查询的话，这就会很nice



27:47 - 27:56
so the design decision that most shared-nothing distributed databases make
So，大部分shared-nothing分布式数据库系统所做的设计决策是
27:56 - 28:00
is that they're not gonna actually support query fault tolerance 
它们实际不会对查询进行容错方面的支持
28:00 - 28:01
meaning 
这意味着
28.01-28.02
if your long-running query crashes
如果你这个执行时间很长的查询发生了崩溃
28.02-28.05
, if a node crashes during your query is running
当你查询执行的时候，如果一个节点发生了崩溃
28:05 - 28:08
unless there's a replica that has the data you need
除非这个replica上有你需要的数据
28:08 - 28:12
that could you know fill in some missing piece depending where you are in the query plan 
取决于你查询计划的进度，你可以去填补一些缺失的数据
28:12 - 28:14
they're just gonna abort the query
它们会中止这个查询
28.14-28.15
 and throw back an error 
并抛出一个错误
28.15-28.17
and tell you to restart it 
然后告诉你重新执行这个查询
28:18 - 28:21
and we take a guess why they make this decision
我们可以猜下他们这样做的原因是什么

28:25 - 28:27
it's expensive right
这样做的成本很高
28.27-28.28
, I'm running a long-running query
假设我正在执行一个耗时很长的查询
28.28-28.31
, it starts generating a bunch of enemy results 
它会开始生成一堆数据
28:31 - 28:34
and now I got to make sure I flush them all the write them to the disk 
现在我要去确保将这些数据写入到磁盘
28.34-28.37
and make sure there's durable across replicas 
并确保这些数据都持久化到replica上
28:37- 28:37
in case that there's a crash 
以防系统崩溃的情况发生
28.37-28.39
that's gonna make your query to run a lot slower
这会让你查询执行的速度变慢
28.39-28.41
because the disk is super slow 
因为磁盘的速度会超级慢
28:41 - 28:47
so in this sort of traditional data warehouse world 
So，在传统的数据仓库中
28:47 - 28:47
they would say 
他们表示
28.47-28.52
oh you just pay me ten million dollars to for this very expensive database system Software
你们要向我支付1000万美金来购买这个非常昂贵的数据库系统软件
28:53 - 28:56
I assume you're not running on machines you found at Goodwill
假设，你并不是在Goodwill上所购买的机器上运行这些软件
28.56-28.58
I'm assuming you're buying on high-end hardware 
假设你们买的硬件非常高端
28:58 - 29:01
so the likely that high-end hardware is gonna have a catastrophic failure 
So，高端硬件可能会遇上灾难性的故障
29:01 - 29:05
where you know it's gonna crash in the middle of query, is gonna be low 
执行查询时发生崩溃的可能性会很低
29:05 - 29:12
so therefore I'd rather not pay the penalty of taking snapshots or writing me results about the disk as I run 
So，当我执行查询的时候，我不会付出一定代价去制作快照或者将结果写出到磁盘
29:13 - 29:14
so in general
So，通常来讲
29.14-29.20
 most OLAP systems are going to if a query if a node crashes during query execution 
对于大部分OLAP系统来说，如果查询执行的时候有一个节点发生了崩溃
29:20 - 29:26
and let's say it's not just reading  from a disk ,where you have copies of like it's it's some the middle part of the query we have enemy results 
我们无法从该节点磁盘上读取数据，你也就拿不到本应从它身上得到的有关查询的中间部分结果
29:26 - 29:28
that you say the query fails 
在这种情况下，你会说该查询失败了
29:29 - 29:32
is when Hadoop came out in the mid-2000s 
Hadoop是在2000年代中期的时候出现的
29:32 - 29:34
they were actually taking snapshots 
实际上，它们会去制作快照
29:34 - 29:38
they were actually taking you know writing at every step of a query plan writing that out the disk 
在执行每一步查询计划的时候，它们会将数据写出到磁盘
29:38 - 29:40
but that made it super slow 
但这使它的速度变得非常慢
29:40 - 29:45
right cuz it gives back in the old days when even still now 
无论是过去还是现在
29:45 - 29:48
but like when Google was building that produce 
当Google将Hadoop投入生产使用时
29:48 - 29:50
they talked about running on you know cheap hardware
他们使用廉价的硬件去运行Hadoop
29:50 - 29:53
where if you're running 1,000 node cluster,
如果你的集群中有1000个节点
29.53-29.55
 and your query is run across 1,000 nodes 
你的查询会在这1000个节点上执行
29:55 - 29:57
the likelihood that any nodes gonna crash during that time is pretty high 
在执行查询的时候，任何节点发生崩溃的可能性都很高
29:58 - 30:00
so and they're well they would rather take the snapshots to avoid this 
So，他们更想通过制作快照来避免这种情况发生
30:01 - 30:01
yes 
请讲
30:09 - 30:14
we're not so much transactions here ,which read-only queries analytical queries 
我们这里只有只读分析型查询，没有那么多的事务
30:15 - 30:23
right so if it's a if say in from last class , which I went transactional distributed database transactions 
在上一堂课，我聊了事务型分布式数据库
30:23 - 30:29
if a node fails that while we're exiting transaction, we just abort that transaction, because who cares 
如果一个节点挂了，我们会关闭(exit)事务，我们要做的仅仅就是中止这个事务，就这么简单
30:29 - 30:33
so that transaction what ran for 50 milliseconds, who cares 
so 那个事务只跑了50ms，谁会关心中止它的代价
30:34 - 30:37
if now I have analytical where that's gonna take days to run 
如果我有一个要跑好几天的分析型查询
30:37 - 30:41
if it's take if takes five days to run, I cry the fourth day the night's wasted four days of work 
如果跑完它要花5天时间，如果在第四天晚上的时候挂了，我得哭死
30:42 - 30:46
some people would get actually pissed about that, because now you gotta you know start it up and run all over again 
有些人就得砸桌子了，因为又得重新开始全部运行一遍！
30:47 - 30:56
and so you could take a snapshot ,some systems allow you to take snapshots ,as you run the query at the oleg as the output of one query plan as gets fed into the next operator 
so ，你可以搞一个快照(snapshot),有些系统允许你搞快照，即你可以把一个查询计划用于传入下一个operator的输出做成一个快照
30:56 - 30:59
I'll just write all that out to disk as well, so that if I crash at that point I can bring it back
我将这些输出写出到磁盘，so，如果我在这个点发生了崩溃，我可以恢复过来
31:00 - 31:02
there's ways to turn that on but by default most systems do not
这就是解决这个问题的办法，但默认情况下，大多数系统是不会这么做的
31:03 - 31:05
because they don't want to pay that performance penalty overhead 
因为他们不希望在这上面消耗太多的性能


31:07 - 31:10
so again on a shared your system it's easy to visualize
so，再次回到这里，在一个shared-disk 系统中，我们很容易想象到




31:10 - 31:12
and we're doing that same join ,we send the plan timing down here 
我们做一个相同的join操作，我们将部分查询计划下发到下面这个节点
31:12 - 31:16
and then as it computes the join result 
然后他计算出join的结果
31:16 - 31:18
it's gonna write that out to the shared disk 
它会将结果写出到这个共享磁盘
31:18 - 31:23
there's some notification to a coordinator to say hey if you're looking for this join result
会与一个通知发给协作者，说，hey，如果你要查看这个join的结果
31:23 - 31:25
here's where to go get it on the shared disk
你可以从共享磁盘这里获取到


31:26 - 31:30
and that way this guy crashes and goes away, this guy knows he can just pull it from from there 
同样，下面这个节点挂掉了，上面这个家伙可以从共享磁盘那里拉取想要的数据
31:30 - 31:32
and not worry about you know how to read good anything 
无须担心你能不能读到你想要的东西
31:35 - 31:39
so so again this is an important design decision that the distributed DBMS are gonna make
so，这是分布式DBMS的重要设计决策
31:39 - 31:47
their there they're not gonna provide you what sometimes called query resiliency or fault tolerance where the query executes
它们不会为你在查询执行时提供那些，有时我们称之为查询弹性或容错的功能
31:47 - 31:49
they maybe restart the query for you 
他们可能就是重新为你执行这个查询，仅此而已
31:50 - 31:55
but they're not gonna try to pick up where a node was running at the Miller at the crash 
但他们不会在一个节点崩溃后，再去尝试选择一个正在运行的节点去执行这段查询
31:57 - 31:57
okay 



32:00 - 32:04
all right so the other thing you got to worry about now is also have new query planning 
all right，so，另一件你需要去关心的就是新的查询计划
32:05 - 32:11
so we have all the same issues that we had before, join orderings, how to do predicates pushdowns or the projections 
so 我们之前也同样看过这些话题，最佳join的顺序，该如何去做条件下压（predicates pushdowns）以及提前映射（projection ）操作
32:11 - 32:16
all those same decisions we had to do on a single node system ,we have to do in the distributed system 
这些我们在单节点系统上做的工作，我们现在要放在分布式系统中了
32:16 - 32:25
but now we have a certain extra level of planning, where we need to reason about ,where our data is located, how its partitioned or replicated 
但现在，我们要有一些额外的计划，因为我们需要去考虑我们的数据是不是在本地，还是它被分区了，或者它有被复制容错（replicated ）
32:25 - 32:31
and now account for the network communication cost for our algorithms
我们还要将网络通信的成本考虑在我们算法中
32:34 - 32:37
right and this makes more sense in a second , when we talk about the different scenarios 
当我们谈论不同的场景时
32:37 - 32:41
but like the join ordering still matters 
比如，join的顺序依然很重要
32:41 - 32:45
but now it's also like ,alright well I need it should I join these two tables first 
但现在，就像我们首先需要去将这两个表进行join操作
32:45 - 32:49
because they're on the same node a partition in the same way and therefore that can be really fast 
因为这两张表在同一个节点上，同一个分区中，因此它会非常的快
32:50 - 32:53
even though if I was on a single node, I may not want to join those two table first 
甚至于，如果我就在一个单节点上，我可能不想首先请join这两张表
32:54 - 32:58
so that just could now included in your cost model to have you help me make a decision about 
so，我可以仅根据我的成本模型（cost model）来帮我做出最好的决策
32:58 - 33:03 
again what's the write this query planned a most optimal query plan for the for the system 
即在这个系统下，为这个查询写出一个最优的查询计划
33:03 - 33:08
again just as I said before doing query optimization on a single node is hard 
就像我之前说的，在一个单节点上做查询优化很难很难了
33:08 - 33:09
this is just even harder 
但这个更难
33:12 - 33:12
okay 

33:13 - 33:22 ！！！！！！
and like we  ,sometimes you can make a decision on a single node and centralize the location ,sometimes you can make it into distribute across all the nodes 
就好像，有时候，当要查询的数据都集中在本地的话，我们需要在单节点上做决策，有时候你的数据横跨多个节点，此时你会有一个中央节点做协调，你要在这种场景下做决策（知秋注：有点类似于fork join）
33:22 - 33:30
but now again you had this,you have to make sure all your statistics, and all your nodes are updated ,as much as possible,to help you make decisions about what the best plan could be 
所以，现在，你必须确保所有统计信息和所有节点都尽可能得到更新，以帮助你来决定最佳方案是什么
33:30 - 33:33
it's just everything just much much harder when it's distributed
当它分布式以后，一切就变的真的很难很难了
33:34 - 33:38
but now same way we have now a query plan 
但，现在，同样的，我们要有一个查询计划
33:38 - 33:44
what do we actually want to send to our different nodes, that are going to participate in executing a join query 
将查询计划发送到我们各个不同的节点之上，用来在节点处执行join查询



33:45 - 33:47
so there's two approaches
so，这里有两种方式
33:47 - 33:55
one is we could actually send the physical plan or plan fragment to the node for it to execute 
一种是我们可以发送物理计划片段到节点上执行
33:56 - 34:01
by just taking the query plan that we've generated on you know on the base node of the home node 
这个查询计划是在home node上生成的
34:01 - 34:05
and then carving it up to say well I know this portion of the query plan needs to execute on this node 
然后，它会说，well，查询计划的这一部分需要在此节点上执行
34:05 - 34:07
this other portion the query planner needs actually on that node 
查询计划的另一部分需要在那个节点上执行
34:08 - 34:10
you just share those physical operators to the other nodes 
你要做的就是将这些物理操作发送到其他节点上
34:10 - 34:16
they just take them and immediately execute them without reasoning about whether that's the best thing to do for the local data 
它们在接收到这些计划后，会立即执行，这些节点不会在意你这个对于我本地的数据来讲是不是最好的方案
34:17 - 34:18
and then sending back the results 
然后，在执行完后返回结果
34:19 - 34:25
so as far as they know most distributed databases that are doing analytics, do this first approach 
到现在为止，大部分分析型的分布式数据库，使用的都是第一种方案
34:26 - 34:32
well the generate the run the query plan through an optimizer once, generate a global plan for the entire cluster for the query 
通过优化器一次性生成用于全局的、整个集群运行的查询计划
34:33 - 34:36
then carve it up into two plan fragments and divide it up
然后将它分割为一些计划片段并发送出去
34:37 - 34:42
another approach is to actually take the SQL query that came in 
另一种方案实际上是在拿到SQL查询后
34:42 - 34:49
and then rewrite it to have it be on a per partition basis have a C query each individual partition 
对它进行重写，即为每一个分区都重写一个适合它执行的语句
34:49 - 34:56
and then send that out ,then when the node that where the partition is located it gets that SQL query 
然后将重写后的sql语句发出，对应分区的节点就得到了可以在它本地执行的sql查询语句
34:56 - 35:01
then it runs it through its own query optimizer to generate the physical plan that it wants to execute
然后通过它自己的查询优化器去为它生成物理计划并执行它
35:02 - 35:10
the idea here is that, if we can assume that the , if we did a global plan on a single node 
此处的想法是，如果我们在一个单节点上来做一个全局计划
35:11 - 35:19
the statistics and information about ,what's best at each partition is not going to be up to date or you know or fresh 
那就需要有关的统计信息，每个分区的一些最新的相关信息
35:19 - 35:25
and therefore rather than me trying to reason at my home node, what the best thing to do at this other node 
因此比起我们尝试在 home node上去推断诸如在另一个节点处该怎么做才是最好的
35:25 - 35:30
I'll just say to this other node, hey I think you need to execute this query or I need to react to this query for me 
我所说的另一个节点，是指你需要去执行查询的地方
35:30 - 35:36
and then that node can then do all the local optimization and all local planning when that psycho query arrives
然后，在那个节点上做所有的本地优化，得到最适合自己的本地物理查询计划


35:38 - 35:40
so again look at example here 
so，来看这里的例子
35:40 - 35:47
so this is my query like this ,and so I could in this case it there's you know there's no join order 
这是我的查询语句，so，在这个例子中没有什么join的先后顺序可言
35:48 - 35:51
ah well it's either RS or SR 
可以是RS，也可以是SR
35:51 - 35:55
but I just say alright well I know me actually the query ,and need such data at these three partitions
实际上，对于这个查询来说，它所涉及的数据分布在这三个分区上


35:55 - 35:59
so I'll rewrite MySQL statement to now include a where clause 
so，我会重写MySqL的where子句
35:59 - 36:06
that says here's the portion of the data you need to look at, which matches what the partition key is ,or the partition ranges 
看这里的这部分数据，它的partition key的值的范围
36:07 - 36:12
then the node gets this each node gets these individual query plans, running through its own optimizer 
然后每个节点会拿到属于各自切分后的查询语句，通过各自的优化器来获取到最适合自己的查询计划
36:12 - 36:20
and then they can decide, Oh based on the data I have here is it better to join R and S,or S and R, shut you a hash join, shut you a sort-merge join
每个节点都可以基于自身数据状况，去选择R与S join还是S与R join更好，根据自身选择hash join或者sort-merge join
36:21 - 36:25
I can make a local decision here, because I have the best view of what's what date I'm actually storing
我可以在本地进行决策，因为我可以根据我自己实际存储的数据来选择最好的方案
36:25 - 36:27
whereas the home node again could be out of date 
然而要知道的是，home node那里的信息可能是过时的（知秋注：这些分区数据的范围可能会变大或者有更多新的分区）


36:31 - 36:37
so the only database something that actually that I'm aware that actually does something like this is is MemSQL 
我知道的实际只有MemSQL 这个系统是这么搞的
36:37 - 36:41
everybody else sends the physical plan 
其他用的都是物理计划
36:43 - 36:46
I don't I mean to me this seems convincing that you could do this 
我这里的意思并不是说，我在通过这个数据来说服你应该这么做
36:46 - 36:52
I think you still have to make a higher-level decision of the possible join order at you know 
我想的是你依然要有一个更高的视角去做决策，比如join 顺序，你知道的，
36:52 - 36:56
if you have multiple tables more than two ,like R S and T 
如果你有多个表，不只是这两个，就好像你有R S T三张表
36:56 - 37:02
should I join R and S, or T and S first, like I think there is some information there , that you may have the reason about at the upper level 
我是先R Join S呢还是先T Join S，我想这里要有一些信息，用来在更高一层去确定这些信息
37:02 - 37:09
but when it comes time for the local node to make this decision ,you know everything it doesn't worry about communicating with anybody else
当它到达本地node去做这个决策时，你完全无须担心它是否还需要和其他人进行通信
37:11 - 37:16
I'm not saying one is better than other, I think this has interesting implications that have not been explored in the research
 我并没有说哪个比哪个更好，我想这个里面隐含了一些我们很感兴趣的东西，可以让我们去研究探索



37:19 - 37:19
okay 

37:21 - 37:23
so now we want talk about joins 
so，now，我们来探讨下join
37:23 - 37:27
again joins are those expensive most important thing you have to do in a single node database
join在单节点数据库中是很消耗性能但又很重要的存在
37:27 - 37:30
and just you know also in a distributed database 
然后，你知道的，它在分布式数据库中同样如此
37:31 - 37:37
for analytical workload the large portion of the time that the system is going to spend executing a query 
对于分析型工作负载，系统将花费大部分时间来执行查询
37:37 - 37:40
where I be reading data from disk or doing our join 
我需要从磁盘上读取数据并来进行我们的join操作
37:42 - 37:44
as we said reading from data from disk  
对于我说的要从磁盘上读取数据
37:44 - 37:47
there's you know there's there are some methods you can do to speed that up 
有几个方式可以加快它的速度
37:47 - 37:52
but in the end of the day you're usually bound on how fast you can get things of the physical device
但最后，你通常会受制于以多快的速度从物理设备上获取信息
37:52 - 37:55
but for joins we can be smart about things 
但对于join来说，我们可以更机智一些
37:56 - 37:59
so as I showed in the first example 
so，对于我展示的第一个例子来说，
37:59 - 38:07
the easiest way to do a join is just get all the data we need to do our join ,put them on a single node, and run our join album 
最简单的方式就是去拿到执行我们这个join相关的所有数据，把它们放在一个单节点上，执行即可
38:08 - 38:14
but as I said, you lose all the extra parallelism actual resources you have by dividing the data across multiple nodes 
但就像我说的，你会丢掉所有的并行处理能力，要知道，实际上你的数据资源已经被分割到很多节点中了
38:14 - 38:20
you lose all those benefits ,and you also you know you may not be able to actually put everything in memory to run things fast
你将所有这些优点都丢掉了，而且，你要知道，你不可能将所有的数据都放在这个单节点内存中去执行
38:22 - 38:29
so there's before different approaches actually, how we actually or afford it scenarios you need to handle, when we want to do it should be to join 
so 有没有和这种不一样的方案，来让我们可以应对这种场景，即当我们想要去做这种join操作时，我们该如何去处理



38:30 - 38:36
and again a distributed join is gonna be the exact same way ,we would do a join on a single node system 
对于分布式join来说，大体和我们在单节点系统上做的join一样，
38:36 - 38:43
but the idea is that we need to figure out how to get the data we want to join together on  a node 
但我们这里的idea是，我们需要弄清楚我们想要进行join操作所需要的数据信息，该如何从一个（或多个）节点上获取
38:43 - 38:46
it could be one node to be multiple nodes 
可能是一个节点，也可能是多个节点
38:46 - 38:51
such that we can do a join locally without having to coordinate with any other node ,while we're doing the join 
我们可以在本地做join，在进行本地join的这个过程中不需要去和其他节点一起沟通协作
38:53 - 38:58
so when we do that local join, it's all the same algorithms, the server's join, nested loop join the hash join, that we talked about before
so 当我们在做本地join时，所用算法和我们前面学的单节点系统用到的是一样的，比如nested loop join、  hash join,等
38:58 - 39:00
all the same optimizations apply 
同样，优化也是一样的，没什么区别
39:00 - 39:07
it's the step you have to deal with before you do that join of how to get the data to a node that's gonna need it 
这是你必须执行的步骤，即在你进行join操作之前，你要考虑该如何到一个节点获取到join操作所需要的数据信息
39:08 - 39:13
so again we're gonna talk about four different scenarios if you've already sort of come up in our earlier examples
基于我们前面已经接触的例子，我们从四个不同的场景来探讨，
39:13 - 39:15
and we'll see how we actually wanna handle them 
我们来看看我们实际是怎样处理它们的
39:16 - 39:19
and again the main takeaway is that there's no magic here that we can actually do 
对于我们实际要做的这些，真没什么黑魔法在里面
39:19 - 39:24
there's no magic industry to join algorithm, that's gonna form so much better than a single node one 
并没有给join算法添加什么黑魔法，为了让它比在单个节点下做的更好
39:25 - 39:28
it's all about how to get the data to where needs to be
都是关于如何将数据传输到需要的位置



39:30 - 39:38
All right, the best-case scenario would be one where the table is one of the tables for joining us partitioned on the join him
All right, 最好的情况是进行join操作所依据的条件字段，同时也是其中一张表分区依据
39:39 - 39:44
and then the other table is replicated in its entirety at every single node 
然后，另一张表会被整体复制到每一个单独的节点中


39:45 - 39:52
so in this case here, the R table is partitioned on the ID it's ID field, which is in R join clause 
so，在这个例子中，R表通过id字段来进行分区，我们也是根据id这个字段来进行join的
39:52 - 39:54
and then the S table is replicated in every node 
接着，S表会被复制到每个节点处
39:55 - 39:57
again this could be the fact table
这个可能会是Fact table
39.57-39.58
 this could be the dimension table 
这个可能是Dimension table
39:58 - 40:04
that this is small enough where we could split it up or small enough where we could replicate it on every single machine 
我们已经将这张表拆分的足够小，并将它复制到每台机器上了




40:05 - 40:06
so in this case here 
So，在这个例子中


40:06 - 40:16
again all we need to do is have every single node ,do a local join to produce the result for the data ,that it has you know store in local storage 
我们所需要做的事情就是让每个节点对它们本地所保存的数据进行join操作，并生成结果


40:16 - 40:24
and then we just need to transmit the the output of one of joining one node to some centralized location 
我们需要做的就是将一个节点所执行join操作的结果传输到某个中央位置
40:24 - 40:28
so that we can combine the results that produce a single answer to our application
So，这样我们就可以将这些执行结果合并为一个结果，并返回给我们的应用程序



40:29 - 40:30.
all right 

40.30-40.31
again this is the best-case scenario is 
这是最佳情况
40.31-40.36
this transfer to get the join result from this node to that node, is unavoidable 
因为像这种将一个节点的join结果传输到另一个节点，这是不可避免的
40:36 - 40:37
we have to do it 
我们必须这么做
40:37 - 40:38
but when we actually get the join 
但当我们执行该join操作时
40:38 - 40:40
we didn't need a coordinator or communicate with any other node
我们不需要与其他节点进行通信
40.40-40.42
, because everything we needed was local to us 
因为对我们来说，我们所需要的所有东西都在该节点本地
40:43 - 40:46
so this gives us the benefit of a distributed database
So，这就是分布式数据库的好处
40:46 - 40:51
because now we can run this join in parallel on every single node without any coordination 
因为我们在不需要进行任何协调的情况下，我们就可以并行执行join操作


40:52 - 40:56
and then everyone just sends the result back to the head node
然后，所有节点只需将结果返回给头节点即可


40:59 - 41:01
next best-case scenario is
下一个最佳情景案例是
41.01-41.08
 where both the tables are partitioned on the join key 
这两张表都使用join key进行分区
41:08 - 41:10
again in this case here
在这个例子中
41.10-41.15
 the again S.ID on the last slide it was partitioned on today was replicated 
在上一张幻灯片中，S表是被复制到每个节点
41:15 - 41:17
this one is partitioned on the ID field 
在这个例子中，它是根据id字段进行分区的
41:17 - 41:26
the range of the partition for S is the same as the range for Partition on R 
S表的分区范围与R表的分区范围是相同的


41:27 - 41:28
so again just like before 
So，就和之前一样
41.28-41.30
we compute our local join
我们在节点本地执行join操作
41.30-41.33
could be a hash join going to be nested loop could be sort merge, it doesn't matter 
你可以使用hash join，也可以使用nested loop join，或者sort-merge join，这都可以


41:33 - 41:37
and then this guy's transmits the result to this other node where we combine it together 
然后左边这个节点将执行结果传输到另一个节点处，然后我们在这里将结果进行合并
41:38 - 41:39
yes 
请讲
41:54 - 41:57
so his question is about data skew 
So，他的问题是关于那些有偏向性的数据


41:58 - 42:00
so in this example here 
So，在这个例子中
42.00-42.05
,I'm showing you that the ranges are the same 1~100 and 101~200 
我向你们展示的范围都是相同的，即1-100和101-200
42:05 - 42:11
but it doesn't necessarily tell you how many tuples exist ,in this range or this partition 
但它不一定要告诉你这个范围或者这个分区中存在着多少tuple
42:11 - 42:14
so like say this ID is the primary key for R
So，假设id是R表的主键
42.14-42.17
, so there's exactly one hundred tuples for R 
So，那么这个分区中，R表确实有100个tuple
42:17 - 42:21
but say this is IDs not the primary key for S 
但假设id不是S表的主键
42:21 - 42:24
I could have a billion entries within this range 
那么在这个范围内我可能会有十亿个tuple
42:24 - 42:26
and then a billion entries for that range 
在另一个范围内我也有十亿个tuple
42:26 - 42:27
how do I handle that 
我该如何处理这种情况呢？
42.27-42.28
,well in that case
Well，在这种情况中
42.28-42.32
you would not want to have the same range as the ID field 
你可能不会想使用和id字段相同的范围
42:32 - 42:35
so in that case you will have to shuffle some data around which we'll talk about in a second 
So，在这个例子中，你就需要对某些数据进行shuffle，这个我们之后会讲
42:36 - 42:37
there's the best-case scenario
这就是最好的情况
42.37-42.41
these are uniformly distributed exact same range
在这种情况下，数据都是均匀分布，并且拥有相同的数据范围
42.41-42.42
 ,I don't need to coordinate
我也不需要进行协调
42:44 - 42:50
I'm sure it doesn't I agree ,but it won't have a real-world ,I'm saying best-case of worst-case scenario 
我这里说的是最好情况以及最差情况
42:50 - 42:52
and we'll see how to handle as things get worse 
我们会去看下如何处理糟糕情况



42:55 - 42:56
all right 
42.56-42.59
so so related to him 
So，这里与他刚才说的东西有关
42:59 - 43:00
so so the next issues gonna be
So，下一个问题是
43.00-43.07
 let's say that when R tables is not partitioned ,on the the same attribute that we want to join on 
假设当R表并没有使用我们进行join操作所基于的那个条件属性进行分区（知秋注：两张表都是以其他字段进行的分区）
43:07 - 43:10
so in this case here S is partition on the value field
So，在这个例子中，S是使用val字段进行分区
43:11 - 43:12
so in this case
So，在这个例子中
43.12-43.15
,we can't compute our local join,
我们没法在节点本地执行join操作
43.15-43.18
 because for every single value of R.ID here 
对于R.id对应的每个tuple来说
43:18 - 43:24
I don't know whether it's it's, you know a matching tuple will exist on my local partition, 
我不清楚在我的本地分区中是否存在与之匹配的tuple
43.24-43.26
then maybe will over here with the same ID 
可能具有相同id的tuple是在另一个分区
43:28 - 43:31
so this case this is called a broadcast join 
So，在这种情况下，这种叫做broadcast join




43:31 - 43:32
and the basic idea is that
它的基本思路是
43.32-43.43
you copy the portion of the table that's missing from each partition to every other partition 
你将那些每个分区中所缺失的那部分表数据复制到所有其他分区中
43:43 - 43:47
so that now this node has a complete copy of the table 
这样一来，这个节点就拥有了该表的完整副本


43:48 - 43:51
and then now you just compute your local join 
现在，你就可以在节点本地执行join操作了


43:51 - 43:53
and then send the result over to the other guy 
然后，将结果发送给另一个节点
43:54 - 43.56
again this assumes that S is small enough
这里假设S表足够的小
43.56-43.58
, that it could you reside in memory
它可以放入内存中
43.58-44.01
not overwhelm this machine or this node here 
并且也不会弄垮这台机器或者这个节点

44:02 - 44:03
you try to copy everything here 
你会试着将所有数据都复制到这个节点
44:06 - 44:08
Right the broadcast means that
broadcast的意思是
44.08-44.12
 you're basically generating every node would end up with a replicated copy of the table 
每个节点都会有一份该表的副本
44:13 - 44:18
sometimes just see you ever heard to as a broadcaste hash join , broadcast sort-merge  join 
有时候你们会听到broadcast hash join和broadcast sort-merge join之类的东西
44:18 - 44:21
it just means they're doing this initial step 
这意味着当它们在执行初始步骤的时候
44.21-44.23
, where they transmit the data across to different nodes 
它们会将数据传输到不同的节点
44.23-44.24
that everyone has a complete copy
这样每个节点都会有一份完整副本
44:25 - 44:26
and then you do the join 
然后，你就可以进行join操作


44:30 - 44:32
the last scenario is the absolute worst-case scenario
最后一种情况无疑是最糟糕的情况
44.32-44.36
is where both tables are not partitioned at all on our join key 
即这两张表都没有使用我们执行join操作的key来进行分区
44:36 - 44:40
and now we need to reorganize the layout of the data 
现在，我们需要去重新组织下该数据的布局
44:41 - 44:43
so that we compute our join more efficiently 
这样我们才能更高效的执行我们的join操作
44:44 - 44:48
so this will be called a shuffle join, shuffle hash join
So，这种叫做shuffle join或者叫做shuffle hash join


44:48 - 44:53
so basically we recognize that we'll really want things to be partitioned on the ID field 
So，简单来讲，我们意识到，我们想让这些数据是根据id字段来进行分区的






44:53 - 44:58
so let me just start copying the data that I need from these two tables to  these two different nodes 
So，我会将这两张表中所需要的数据复制到这两个不同节点上
44:58 - 45:02
if this has to spill to disk run out of space, that's unavoidable,
因为内存空间耗尽的原因，这些数据会溢出到磁盘，这是不可避免的
45.0245.04
 so we just go ahead and do that 
So，我们继续执行我们的操作


45:04 - 45:07
and then once I know I have everything Partition the way I want to partitioned
一旦我知道，在所有数据都是按照我预想的方式进行分区了之后
45.07-45.11
, then I can keep my local join and generate the results 
那么，我就可以在本地执行join操作，并生成结果
45:13 - 45:13
yes
请讲
45:16 - 45:16
what 
你再说一遍


45:21 - 45:22
so his question is 
So，他的问题是
45.22-45.24
and the space is limited what can you do
如果空间是有限的，我们该怎么做
45:24 - 45:25
so the point I'm trying to make here is like 
So，此处我尝试要做到的事情是


45:29 - 45:30
Think about what you end up doing here right 
思考下你们这里最终在这里做什么
45:30 - 45:36
so I have to now make another copy of R on you know and store it on this node 
So，我现在需要将R表的副本复制并保存到左边这个节点上
45:37 - 45:39
and so if it doesn't fit in memory
So，如果它无法放入内存
45.39-45.39
 just as I said like, 
正如我之前所说
45.39-45.44
it's gets spilled as a temporary result for the query could get spilled a disk, 
该查询的临时结果就会溢出到磁盘
45:44- 45:46
that's unavoidable 
这是不可避免的


45:47 - 45:49
right but the more important thing is that,
但更为重要的地方在于
45.49-45.50
 now when I compute the join
当我计算join时
45.50-45.52
, I want that to be fast as possible
我想让它的执行速度变得尽可能快
45.52-45.53
, because that's the most expensive thing 
因为它是执行成本最高的东西


45:53 - 45.55
so by copying data around 
So，通过复制数据
45.55-45.57
when I do the join
当我执行join操作时
45.57-45.58
 everything is partitioned nicely the way I want it
所有数据都以我想要的方式分好了区
45.58-45.59
 and it's more efficient 
执行join操作时，这就会变得更高效
46:04 - 46:05
this question is 
他的问题是
46.05-46.07
do we always assume that the disk is enough 
我们是否始终假设我们的磁盘空间是足够的？
46:14 - 46:15
for our class, yes
在我们的课上，我们会假设磁盘空间是足够的
46.15-46.17
 in the real world, no 
但在现实生活，这个假设是不成立的，磁盘空间可能不够


46:17 - 46:19
what will happen is
这里要发生的事情是
46.19-46.20
 the database recognizes that
数据库会意识到
46.20-46.23
if I get to here
当我执行到这一步的时候
46.23-46.25
 ,and I can't copy any more data to this node here
我无法再往这个节点中复制更多数据了 


46:26 - 46:27
the query would fail
该查询就会执行失败
46.27-46.29
, just say I run out of swap stays for space
这里我表示磁盘的空间用完了
46.29-46.32
 , and you throw an error and the query fails 
接着，我们就会抛出一个错误，然后查询就会失败
46:34 - 46:37
and actually related to his question about the distribution of data 
实际上，他的问题和数据分布有关
46:38 - 46:42
if say it was partitioned on the ID field 
如果我们假设这里是根据id字段进行分区的
46:42 - 46:48
but the distribution was highly skewed sister that most of the data was on this node for S instead of that node 
但这个数据的分布是极度倾斜的，S表大部分的数据是在这个节点上，那个节点它的数据很少
46:48 - 46:52
I can still do this reshuffling to to realign my data
我依然可以对我的数据进行shuffle操作，以此来对我的数据进行对齐
46.52-46.54
 maybe move some data from R and S over here
我可能会将旁边R表和S表中的某些数据移动到这里
46:54 - 46:56
so that things even get balanced 
这样，就能让两边数据平衡
46:57 - 46:59
but still it's still called a shuffle process
但这过程依然是shuffle过程
47:00 - 47:00
question 
请讲
47:05 - 47:06
how does a lot more compact oh sorry
oh sorry，再讲一下
47:14 - 47:14
yes 
Yes
47:21 - 47:23
of course how do you manage what sorry
你该如何管理什么？能再讲一下吗？
47:33 - 47:34
Her question is 
她的问题是
47.34-47.36
how do I make a decision about what data to send 
我该如何决定要发送哪些数据
47:37 - 47:38
because if things are replicated,
因为如果这些东西都被复制了
47.38-47.43
 I don't want to waste you know waste network transmission of sending data that I don't need to send 
我不想去浪费网络带宽来发送那些我不需要发送的数据
47:43 - 47:45
you know everything ahead of time 
你会提前知道所有这一切
47:45 - 47:46
right SQL is declarative
SQL是声明式的
47.46-47.48
 we know what the query is 
我们知道这个查询是什么
47.48-47.49
,we know what data are trying to access 
我们知道我们在尝试访问哪些数据
47:49 - 47:51
we then look in our system catalog 
接着，我们会去查看我们的System Catalog
47.51-47.54
our system catalog is gonna tell us, how this data is actually partitioned 
我们的System Catalog会告诉我们这些数据的实际分区情况
47:54 - 47:55
we know this ahead of time
我们会提前知道这些
47:56 - 47.58
so the query planner can make a decision
So，查询计划可以做出决策，并表示
47.58-48.00
, oh well this data partition this way
Oh，这些数据是以这种方式进行分区的
48.00-48.01
 ,and it's this size,
它的大小是这样的
48.01-48.02
 and it's in on this node
它是放在这个节点上的
48:02 - 48:05
so therefore and either move or not move it or copy here , don't copy there 
So，因此我们可以选择去移动数据，或者不移动数据，以及需不需要去复制数据
48:06 - 48:07
I can do all that ahead of time
我可以提前做这些事情
48.07-48.11！！！！！！！
, it's not like as I'm going along , as I say oh well yeah maybe I should copy this 
而不是当我执行的时候，我再去说我可能需要复制这些数据
48:12 - 48:13
you figure out everything ahead of time 
你会将这些事情都提前弄清楚
48:14 - 48:16
except for the issue he brought up
除了他刚提的问题以外
48.16-48.16
 or like I ran out of disk space 
即我可能会用光磁盘空间
48.16-48.20
,you know you just fail ,nothing you can do 
你知道的，遇上这种情况，查询就会执行失败，对此你无能为力
48:24 - 48:34
and then the how efficient the database system is in making those decisions about , what the copy or not the copy, depends on how good your query optimizer is 
数据库系统做出这些决策（比如是否需要复制数据）效率取决于你查询优化器的好坏
48:34 - 48:38
which is why people pay a lot of money to have people like work on query optimizers
这就是为什么人们愿意花很多钱去请人负责他们的查询优化器的原因所在了
48:38 - 48:39
question 
请讲
48:43 - 48:43
your question is
你的问题是
48.43-48.46
 what if the data is sorted on the partition ID beforehand 
如果数据提前根据partitioning id进行排序的话，会怎么样
48:46 - 48:48
but it's partition ID or join ID 
你指的是partitioning id还是join id
48:50 - 48:54
so if it's sorted for what I'm showing here who cares 
So，正如我这里所展示的，没人会去在意这个
48:55 - 48.58
right cuz what I care about is the locality of the data
因为这里我关心的是数据的所在位置
48.58-49.01
, I need to join R.ID on S.ID 
我需要根据条件R.id=S.id进行join
49:01 - 49:02
so I want to make sure that
So，我想去确保
49.02-49.04
 when I do that join locally 
当我在本地执行join操作时
49:04 - 49:11
I have all the data that I need you know to join the outer table at the inner table ,is at my local node 
我要确保我用来对outer table和inner table进行join时所需要的所有数据都在我的本地节点
49:11 - 49:13
it can't be at some other node that I don't know about, 
这些数据不能放在那些我不知道的节点上
49.13-49.16
and that could end up with a false negative or false positive 
这可能就会导致假阴性或者假阳性的问题出现
49:17 - 49:19
so sorting doesn't matter 
So，对数据进行排序其实并不重要
49:19 - 49:23
sorting would matter when we can make a decision about do I want to do a sort merge or hash join 
只有当我们决定要不要使用sort-merge join或者hash join的时候，排序才会变得重要
49:24 - 49:28
word like a step above this, we're designing how do we move data or not 
就比如在这一步之上，我们要去设计我们该如何移动数据
49:32 - 49:32
yes 
请讲
49:39 - 49:39
okay

49.39-49.39
so question is
So，他的问题是
49.39-49.41
what am I actually transmitting here 
这里我实际传输的是什么东西
49:42 - 49:44
and be clear I'm not swapping this,
要明确的一点是，我并没有交换数据
49.44-49.45
 I shouldn't have a divider line 
这里我不应该使用分割线



49:45 - 49:48
so this is the permanent data on this node
So，这部分数据是该节点的永久数据
49.48-49.49
, it does not go anywhere 
它不会跑到其他地方去
49:50 - 49:53
I just make extra copies as temporary data to do my join
我只是将一些副本数据作为用于执行我join操作的临时数据
49.53-49.55
, and then I throw it away when my queries over 
当我的查询执行完毕时，我就会将这些临时数据给丢掉了
49:55 - 49:57
so even though I'm shuffling here on ID 
So，即使这里我根据id来对数据进行shuffle操作
49:58 - 50:03
the I'm still going at the end of a partition R name and value here , this stays 
我依然要知道它是以R{Name}和S{Val}来分区的
50:03 - 50:07
but then your other question is , what am I actually transmitting 
然后你的问题是，实际上我传输的是什么
50:07 - 50:12
my transmitting the whole tuple or am I transmitting the summit some identifiers next slide 
我传输的是整条tuple还是只是一些id
50:13 - 50:14
okay we'll get to that yes 
Ok，我们稍后会讲，请讲
50:25 - 50:26
your question is 
你的问题是
50.26-50.27
on a local node
在一个本地节点上
50.27-50.29
if the data is not sorted
如果数据是未排序的
50.29-50.31
, how do I make sure that there's no duplicates, duplicates of what 
我该如何确保这里面是没有重复的数据？哪些重复的数据？
50:33 - 50:35
for the join 
你说的是用于join操作的数据？
50:40 - 50:44
aha is a partition ID the primary key or a unique ID 
这里的partitioning id使用的是主键还是唯一id？
50:55 - 51:01
well ,yes all right get 2 questions that 
Well，你其实问了两个问题
51:02 - 51:03
so I think what you're asking is actually, 
So，我觉得你实际问的东西是
51.03-51.04
if I have a primary key,
如果我有一个主键
51.04-51.06
 then I need to guarantee it's unique 
那么我需要保证它是唯一的
51:06 - 51:09
but my my partition key is not the primary key,
但如果我的partitioning key不是主键的话
51.09-51.12
 how do I make sure that I enforce that 
我该如何确保我强制它是唯一的呢？
51:13 - 51:15
So that is a transaction 
So，这和事务有关
51:17 - 51:19
right that's not us, we're just doing analytical queries 
这与我们这里做的事情无关，我们这里做的只是分析型查询
51:20 - 51:21
we assume we don't like 

51:22 - 51:28
we assume that someone else has solved that for us when they stored the data as a part of a transaction 
我们假设其他人在将数据存储为事务的一部分时就已经为我们解决了这一问题
51:29 - 51:30
so in your example
So，在你的例子中
51.30-51.33
if I have a partition key that's not the same as the primary key
如果你有一个partitioning key，但它并不是主键
51:33 - 51:35
then I insert a new record,
接着，我插入了一条新纪录
51.35-51.36
 how do I make sure that's unique 
我该如何确保它是唯一的呢？
51:37 - 51:44
while I  need to maintain a sort of centralized index that I look up and see whether this key does party exist 
我需要去维护一份中央索引，我会在里面进行查找看看某个key是否存在
51:44 - 51:50
or at the broadcast the query to every node to say hey I'm certain this key do you already have a copy of it 
或者，当你将查询广播到每一个节点，并询问你是否有该key的副本
51:52 - 51:55
I'm saying you could, right
我说的是，你可以这么做
51:57 - 52:00
help me how come in me how could how could that be
这种索引是怎么样的
52.00-52.01
, here's a node, here's an index 
这里有一个节点，这里有一个索引
52:02 - 52:05
we're building idiots we do have everyone, right ,we can do this
我们可以构建这样的中央索引，这样我就知道了所有，我们可以这么做
52:09 - 52:17
again ,like I don't care about in this world for these queries ,I'm not enforcing integrity constraints 
在这些分析型查询中，我不会强制使用这些完整性约束
52:18 - 52:26
I'm just running this read query is analytical queries to figure out , you know how to compute the join efficiently on a large data corpus 
我会去执行这些分析查询以弄清楚该如何对大量的数据以更高效的方式计算这些join操作
52:26 - 52:28
the transactional side,
从事务方面来讲
52.28-52.30
 it cares about integrity constraints
它会去在意完整性约束方面的东西
52.30-52.32
because you're modifying the state of the database 
因为你会去修改数据库的状态
52:41 - 52:42
no No
你说的不对
52.42-52.46
so so going back to my initial example here 
So，回到我刚开始的那个例子


52:51 - 52:52
like for this ETL thing 
就拿这里的ETL来说
52:53 - 52.58
so this thing of this like your bulk loading a bunch of data 
So，这里你会加载大量的数据
52.58-53.02
like, this is sort of streaming, it's not happy it's not like all at once here's a bunch of data 
这里传入的是数据流，它不会立刻将一大堆数据加载到位
53:02 - 53:07
but you're streaming incrementally updates from the front end to your back end data warehouse 
但你会逐步将数据流从前端的OLTP数据库发送到后端数据仓库中
53:08 - 53:13
and the back end data warehouse could choose or not choose to enforce those integrity constraints as it comes in 
当这些数据传入的时候，后端数据仓库会决定要不要使用这些完整性约束
53:14 - 53:20
but it won't be on the critical path of when we execute our queries 
但当我们执行查询时，这并不会对我们产生很大的影响
53:20 - 53:21
because I'm running a selects statement
因为我执行的是一个select语句
53.21-53.23
 I'm not checking to see whether my primary key is unique 
我不会去检查我的主键是不是唯一的
53:23 - 53:29
so now how you enforce that integrity constraint as you ingest the data into your back end data warehouse 
So，当你从你的后端数据仓库中提取数据时，你该如何强制使用这种完整性约束呢？
53:29 - 53:32
well that's the same thing we talked at last class how to do transactions 
Well，这和我们上节课讨论如何执行事务时所讲的是一回事
53:32 - 53:34
because that is a type of transaction 
因为这就是事务的一种类型
53:34 - 53:36
you know insert something make sure it's unique 
比如：插入某个数据时要确保它是不是唯一的
53:36 - 53:41
I need to coordinate across multiple nodes, if I'm not everything's not on a single node  
如果执行事务所涉及的那些数据并不在同一个节点时，我们需要对多个节点进行协调
53:41 - 53:44
to make that check it does everyone agree that we can go ahead and make this change 
并检查是否所有节点都同意我们执行这个修改
53:45 - 53:56
so a lot of times in these analytical data warehouses they will have  they will sort of have a separate engine or a storage area 
So，很多时候，在这些分析型数据仓库中，它们会有一个单独的引擎或者存储区域
53:56 - 54:04
,that is designed to do efficient more are more efficient updates than what a you know a traditional column store DBMS would do 
它是为了让更新变得更有效率而设计的，这要比传统的列式存储DBMS的效率还要高

54:05 - 54:07
if you take the advanced class
如果你们要学cmu 15-721这门课的话
54.07-54.07
 we will cover that
我们会介绍这个
54.07-54.08
, we didn't cover that here 
我们就不在这里介绍这个了
54:10 - 54:12
right so again for our purpose here today 
So，出于我们今天的目的
54.12-54.14
we don't care about enforcing that integrity constraint
我们不去关注完整性约束方面的东西
54.14-54.17
 ,we assumed that it was already handled for us 
我们假设它已经为我们处理好了
54:17 - 54:21
some database systems I mean some data warehouses,they just turn all that crap off
我的意思是在某些数据仓库中，它们将这些没用的功能关闭了
54.21-54.25
they turn off foreign keys, they turn off unique keys 
它们禁用了外键和唯一键
54:25 - 54:29
your data is a little dirty ,who cares for analytics 
对于整个分析过程，谁会去在意你的数据是不是dirty的呢（知秋注：分析型数据库的数据量是超级大的，不要在意那点50ms内的小细节）
54:46 - 54:46
right so this question is
So，他的问题是
54.46-54.55
, you have your bank account or what it whatever your  game information 
假设你有银行账户或者说这里是你的游戏信息
54:56 - 54:58
you update your user account on the front end OLTP database 
你会去更新你前端的OLTP数据库中的账户信息
54:58 - 55:01
because that's what that's the users are touching this part 
用户会接触OLTP数据库中的数据
55.01-55.02
,and they don't touch the back in dataware house
它们不会接触后端数据仓库中的数据
55.02-55.03
 they make updates here 
它们会在OLTP数据库中更新数据
55:04 - 55:09
that update gets propagated 
更新会进行传播

and you want to go update the record here has that happen advanced class 
你想要在OLAP端去更新这个record，具体请关注我们的高级课15-721
55:09 - 55:09
in general
一般来讲
55.09-55.10
 what I'll say is like
这里我要说的是
55.10-55.17
 you buffer a bunch of changes in a sort of write-optimized storage layer or execution engine in these data warehouse
你会将一堆修改缓存在一个为写操作而优化的存储层或者执行引擎中
55:17 - 55:21
and then they periodically merge the change into the data warehouse 
然后，它们会定期将这些修改合并到数据仓库中
55:22 - 55:27
or into like the main column store tables for the for that for the new analytics
用于新的数据分析
55:28 - 55:29
different systems do different things
不同的系统有不同的做法
55:35 - 55:35
okay

55:40 - 55:40
so 



55:42 - 55:44
any other questions about this stuff here
对此你们还有任何问题吗
55:45 - 55:47
yes go ahead 
请讲
55:53 - 55:53
yes 

56:01 - 56:02
her question is
她的问题是
56.02-56.03
 I said here that
这里我说过


56.03-56.07
 when we go to copy this thing here for this particular query 
当我们为这个查询将这些数据复制到这里时
56:07 - 56:09
if we run out disk space
如果我们用完了磁盘空间
56:10 - 56:12
and the query crashes
然后，这个查询就崩溃了
56.12-56.17
 , couldn't the query optimizer figure that ahead of time ,and say oh I'm not gonna have a disk space 
查询优化器是否能提前弄清楚我们有足够的磁盘空间了
56:17 - 56:22
let me let me make sure that maybe not run a certain way here 
它能否提前告诉我这个节点可能无法执行这个查询
56:22 - 56:28
the query optimizer is usually do not reason about what our other queries running at the same time 
查询优化器通常不会去推断同一时间是否有其他查询正在执行
56:29 - 56:30
it assumed your query runs in isolation 
它假设你的查询是单独执行的
56:33 - 56:38
and that it could reason about certain things like 
它所能推断出的东西是
56:39 - 56:45
some sort of some systems you can specify how much temporary buffer space or temporary disk space, the queries allowed to use 
在某些系统中，你可以指定这些查询可以使用的临时buffer space有多大或者临时磁盘空间有多大
56:45 - 56:46
and then if you exceed that 
如果你超过了指定的空间大小
56.46-56.47
your query fails 
那么你的查询就会执行失败
56.47-56.50
,and it throws you back hey you know creases parameter if you want to keep running 
然后它会给你抛出：hey，如果你想保证运行这个查询的话，你需要增加参数值
56:51 - 56:58
so it could potentially figure out at planning time ,oh I'm gonna copy over more data than I have space for this particular query, and throw an error 
so 它可能在查询计划执行前这个时间去指出（知秋注：在执行前提前把数据准备到位）：oh，这个查询对我来说，要复制数据大于我本地的空间，于是就抛了个error
56:59 - 57:02
but if like you disk physically runs out of space
但如果你的磁盘是物理意义上的空间耗尽
57.02-57.08
even though you're not within that you know  you haven't exceeded that limit on a  query basis 
甚至于你并没有超过这个查询限制（知秋注：因为同时可能有多个查询并行进行，那对于某一个查询来说，空间可能就是够的，只是多个查询的话，就不够了）
57:09 - 57:13
the query optimizer usually does not think about or cannot reason about what queries are running the same time 
查询优化器通常不会推断出哪些查询在同一时间执行
57:13 - 57:14
Because that just makes your life harder 
因为这会让你处理起来变得更加困难
57:15 - 57:19
Because like this query first-years up I plan it, I say all right nothing's running now 
因为就像我先拿到了这个查询，为它设定了查询计划，我会说，all right ，这会并没有什么join操作
57:19 - 57:23
let me go ahead and choose one kind of plan because I'm running by myself 
于是选择一种查询计划，并执行它
57:23 - 57:24
and start running
然后开始执行
57.24-57.25
 and then this other query now shows up 
接着，我们又拿到了另一个查询
57:25 - 57:30
I don't want to go back and modify that other queries plan to say hey now you're also be running another query at the same time 
我不想去回头并修改其他查询计划：即说，hey，现在，在同一时刻，我们还有另一个查询要执行
57:30 - 57:31
that's way too hard
这太难了
57:39 - 57:39
No no no
No No No
57.39-57.41
 my comment was that,
我是这样说的
57.41-57.43
 sometimes you see in data warehouses 
有时你会在数据仓库中看到
57:44 - 57:53
so the database system itself could support schema enforcing integrity constraints , foreign keys , primary keys referential integrity and stuff like that 
数据库系统本身可以支持强制完整性约束，外键，主键引用完整性和类似内容的架构
57:54 - 57.55
it could support those
它可以支持这些东西
57.55-58.00
 ,but the application developer the person building the data warehouse 
但对于那些构建数据仓库的应用程序开发者来说
58:00 - 58:03
may say I don't want to pay the penalty to check for foreign keys
他们可能会说：我们不想为检查外键而付出成本
58.03-58.04
let me turn all that off 
让我把这个功能关掉吧
58:20 - 58:21
this question is
他的问题是
58.21-58.22
 I said that
我之前说过
58.22-58.23
in the previous slide
在前一张幻灯片中
58.23-58.27
, that in general
通常来讲
58.27-
 most shared-nothing ,actually shared disk


58:32 - 58:40
Most distributed OLAP databases do not support query fault tolerance or query resiliency
大部分分布式（使用shared-nothing架构）OLAP数据库并不支持对查询进行容错，或者query resiliency（弹性）
58:40 - 58:47
where if a query if they know crashes that's a response for executing query halfway through the query execution 
即如果在执行查询的途中发生了崩溃，
58:47 - 58:54
they are not able to recover the enemy results of that query it was had or computed 
他们无法恢复曾经查询或计算过的查询的中间结果
58:54 - 58:57
and then picked the query up where it left off 
然后，也就无法从该查询崩溃的地方开始继续执行
58:57 - 58.57
in general
通常
58.57-58.59
 they just killed the whole thing throw an error
它们会干掉整个查询并抛出一个异常
58.59-59.01
 or maybe restart it silently for you 
或者安静地为你重新执行这个查询
59:02 - 59:04
that has nothing to do with logging and recovery 
这与logging和recovery并没有什么关系
59.04-59.06
,we absolutely still need logging and recovery 
我们肯定依然需要用到logging和recovery
59:06 - 59:08
and we still do that 
我们依然要用到它们
59:09 - 59:14
this is more about while the queries running do I can I take snapshots as I go along 
当查询在执行的时候，我是否该制作snapshot
59:14 - 59:18
so that I can pick up midway where the query was running if I know Goes Down 
So，这样的话，如果查询崩溃了，那么我就可以从查询崩溃的地方开始执行
59:18 - 59:22
and I'm in the the statement I made was that
我之前有说过一句话
59.22-59.26
 ,to the best of my knowledge ,most distributed OLAP systems do not support that query resiliency 
据我所知，大部分分布式OLAP数据库系统并不支持query resiliency
59:26 - 59:30
because taking the check points the snapshots of the intermediate result is expensive 
因为制作这种中间结果的snapshot所要付出的成本非常高
59:37 - 59:39
Logging of what so what logging of what
你要记录什么日志
59:42 - 59:44
the query of the database 
记录对该数据库的查询？
59:45 - 59:53
we're still all the fault on all the the D the D in acid stuff that we talked about before, we are still doing that 
我们之前谈论过的ACID中的D，这里我们依然会这样做
59:53 - 59:55
we are still making sure that 
我们依然会去确保
59.55-59.58
if we ingest data from the outside world up to our database
如果我们从外界提取数据到我们的数据库时
59.58-1.00.00
, we don't want to lose that 
我们不想丢失这些东西



01:00:00 - 01:00:04
and so what they will all provide durability guarantees
so 它们会提供那些持久性保证
01:00:05 - 01:00:07
again I am I have a hundred petabytes of Walmart 
我有沃尔玛一百PB的数据
01:00:07 - 01:00:09
they don't want to lose that data 
沃尔玛不想丢失这些数据
01:00:09 - 01:00:12
all right and DBMS will guarantee that they won't 
all right，DBMS 会保证它们不会丢失数据的
01:00:24 - 01:00:30
when you say operation again what do you mean like the an update to it or we are query, I got like a select query 
当你要进行一个操作，比如对它进行一个update更新或者我们进行一个select查询
01:00:37 - 01:00:41
let's take us let's go let's sit down afterward ok ？I'll go through this 
请先坐下，我等会儿会讲这个的
01:00:42 - 01:00:43
I think there's something you're missing here 
我想关于这块你应该是少考虑了什么
01:00:45 - 01:00:45
okay 



01:00:47 - 01:00:53
so to his question about what am I actually sending, when I do these to semi-joins 
他的问题是，当我做这些semi-join时，实际上我会发送什么
01:00:53 - 01:00:58
and I`m sending the whole tuple or I`m sending them sending an identifier 
我是要发送一整条tuple呢，还是只发送一个id
01:00:58 - 01:01:06
and I always say the answer is that , you're sending, at the very least you're sending the minimum amount of information that you need in order to compute the join 
我一直有强调，你所要发送的数据，取决于你进行join计算要使用的最小信息量
01:01:07 - 01:01:10
and then the worst-case scenario you're sending the entire tuple
那最坏的情况就是你要发送一整条tuple
01:01:10 - 01:01:16
and in general, again  the good distributed databases 
一般来说，对于一个优秀的分布式数据库系统
01:01:16 - 01:01:19
that are doing analytics will minimize the amount that data that you actually need 
它们会使用最少数量的数据来做分析
01:01:21 - 01:01:26
and so they're petite have a query would be called using what is called a semi-join
so，我们为这种查询行为专门起了个名字，叫semi-join
01:01:27 - 01:01:37
a semi-join is like a regular join, but the idea is that ,we're not really gonna do a join on the on the right table or the inner table 
semi-join其实也是一个常规得到join，但这里的思路是，我们实际上不会去基于右表（right table）或内表（inner table）去做join操作，而是会以左表（left join）为基础来做
01:01:37 - 01:01:42
we're only gonna check to see whether if we dare to join， a tuple would match 
如果我们执行一个join，我们只会检查这个tuple是否匹配这个join条件 
01:01:43 - 01:01:46
so the query optimizer can recognize that 
so 查询优化器就可以认知到，
01:01:46 - 01:01:54
it  may not need any values or attributes are from the columns of the inner table
它可能不需要这个内部表那么多列属性
01:01:55 - 01:01:59
and therefore it can rewrite the query just as do an existence check 
因此，它可能会对查询重写，即只作存在性检查
01:01:59 - 01:02:06
and send the middleman information back and forth we knows to do that semi-join rather than copying the entire tuple 
然后，将结果给中间人发送回去，由此来看，使用semi-join要比复制整条tuple要靠谱的多
01:02:07 - 01:02:10
again like a natural join  you would do the join
对于 natural join来说，你在做这种join操作时，
01:02:10 - 01:02:17
and then the output would be all the result in the combination or the concatenated, tuple on the right and the left table 
输出的是左表和右表tuple进行级联结果
01:02:18 - 01:02:23
and a semi-join it's just the attributes that needed to compute to join from the outer table 
semi-join 就是只需外表（outer table ）参与计算的属性(知秋注：图中的话就是R表的id)
01:02:24 - 01:02:33
so in this case here say I have a query like this ,select R.id from R , doing outer join on S and we'll just match my ID 
就好比这里的这个查询select R.id from R left outer join on S，我们只需要匹配我的id就好
01:02:33 - 01:02:40
and then this is a poorly written query ,because they're basically saying , where R.id is not null, then match up with what on s 
这个查询写的不咋地，这个查询基本上在说，R.id不能为null，然后判断R.id=S.id





01:02:41 - 01:02:47
so if we didn't do a semi-join, we either have to copy s over here or R over here
如果我们不做semi-join，我们就要将S复制到这里或者将R复制到这里
01:02:48 - 01:02:51
again we're copying the entire tuple that would be expensive 
这样去复制整条tuple是很消耗性能的（知秋注：其实更多还是网络性能和占据大量的内存）




01:02:51 - 01:03:00
but we could rewrite this query to be like this, where we just check to see whether there exists a tuple in S, that has the same id as R.id 
我们可以将此查询重写为这样，我们就是去检查下，看看S中是否存在这样的tuple，它的id和R.id相同
01:03:01 - 01:03:05
and then if that's true then we just spit out all our R.ids that match 
然后，如果为true，我们就将所有匹配的 R.id输出


01:03:05 - 01:03:09
so in this case here the only thing I need to send over is just maybe just the R.ids 
在此处这个例子中，我们只想输出这个R.id
01:03:09 - 01:03:13
because that's the bare minimum information I need in order to compute this join 
这就是我们计算这个join所需的最小信息量
01:03:16 - 01:03:26
so some systems like Claude hours and Paulo actually has an explicit semi-join keyword you can give it 
一些数据库实际上会有显式的semi-join关键字供我们使用
01:03:26 - 01:03:28
otherwise, you can try to fake it with it exist 
否则，若没有，你可以尝试使用EXISTS
01:03:29 - 01:03:33
the high-end systems can check to try to rewrite your query into a semi-join 
高端的系统会去检查并尝试将你的查询重写为semi-join
01:03:33 - 01:03:37
again it's part is figuring out what data I need is transmitted between the different nodes 
一部分是要弄清楚我需要在不同节点之间传输哪些数据
01:03:37 - 01:03:45
and includes that in its cost calculations in the optimizers cost model or to decide, how much data might transmit in between different nodes as one plan better than another 
包括通过优化器optimizer的成本模型计算来决定在不同节点间可能要传输数据量的大小，一次来确定那个计划方案会更好


01:03:49 - 01:03:56
so like I could have I could say select R.id from R semi-join on S.id 
so 这里，我可以这么写：select R.id from R semi-join on S.id
01:03:57 - 01:04:02
explicit SQL that does that says hey you're doing a semi-join, or I can rewrite it as this 
通过这种显式的SQL来表达，我们这里做的是一个semi-join，或者我可以像这样进行重写
01:04:02 - 01:04:06
most systems you have to do this because they don't have I don't think semi-joins in the SQL standard
大多数系统你只能这么做，即使用EXISTS的方式，因为我不认为semi-join是SQL标准
01:04:08 - 01:04:08
Yes
请讲
01:04:11 - 01:04:15
questions who does who does this work and this example here, this is the application programmer 
她的问题是这块儿的工作谁在做，拿这个例子来说，这是程序开发者的事情
01:04:17 - 01:04:24
the the high-end enterprise systems that have good query optimizers could figure out how to potentially rewrite this for you 
对于高端的企业系统来说，它们有很好的查询优化器，可以很清楚该怎样去重写这个sql语句的
01:04:24 -  01:04:36
They can do that for you 
它们可以为你做到这些
01:04:29 - 01:04:32
her quite your question is the query optimizer mind  another node ？another like what do you mind another  node
她的问题是查询优化器介意在另一个节点，你在意它什么呢？

01:04:42 - 01:04:44
even so this is like a partition that you think 
比如，这个节点是一个分区
01:04:46 - 01:04:50
it doesn't matter, doesn't matter what the query optimizer is running this node here or another node, doesn't matter
没必要在意这些，你无须在意查询优化器是放在这个节点还是另一个节点
01:04:50 - 01:04:55
did for more talked about here, this is just sort of like the semantics of the semi-join
不要在意太多，这里只是做了一个semi-join


01:04:56 -01:05:01
so from relational algebra standpoint, it just looks like this, I'm joining R and S
 so 从关系型代数的角度，这就像是，我在做R join S


01:05:02 - 01:05:11
and then this is the semi-join operator, the output would be just can to be A_ID B_ID that I used to compute the join 
然后，对于semi-join操作来说，我在计算join时，它的输出就可以是a_id、b_id（如ppt所示）
01:05:11 - 01:05:13
and nothing from the inner table 
不会从内表中拿任何东西的，所有的结果都从外表去拿（知秋注：外表是R，内表是S，默认是左连接join）
01:05:17 - 01:05:18
so this clear
so 这些清楚了吧
01:05:19 - 01:05:20
there's  have a question on it 
对于它还有其他问题么
01:05:23 - 01:05:26
All right, so we have ten minutes 
All right, so 我们还有十分钟



01:05:27 - 01:05:33
so as I said this is to sort of crash course on
so 我讲了这一系列关于故障的解决应对课程
01:05:33 - 01:05:39
the main design decisions in the issues in tuning and available database 
在碰到的这些数据库问题中主要的设计需求在于
01:05:39 - 01:05:46
if you're not going to be building a you know an available DBMS ,actually working on the internals of the system, you just want to be a user of them 
如果你要构建这么一个可用的dbms，它实际是工作在一堆内部dbms系统之上的，你就是想去使用它们而已
01:05:46- 01:05:49
the main issue you're going to deal with is that partition key 
主要问题在于你要去处理这些 partition key 
01:05:49 - 01:05:56
how to pick that in such a way that most your joins can operate on a local node without having to do a broadcast or our shuffle 
以便你选择一个方式去将你绝大多数的join操作在一个本地节点上执行，无须去做那种广播（broadcast）或对我们的数据进行shuffle
01:05:57 - 01:06:04
and the various systems that are out there, mostly the enterprise guys have tools to help you try to figure these things out for you 
绝大部分的企业级系统有工具来帮你弄清楚这些事情
01:06:05 - 01:06:05
okay 

01:06:07 - 01:06:09
all right let's talk bout Cloud databases 
all right，我们来讨论下 Cloud databases 
01:06:09 - 01:06:17
so the definition of comp cloud databases a bit nebulous, no pun intended 
 cloud databases的定义有点朦胧
01:06:17 - 01:06:18
the the

01:06:19 - 01:06:25
typically what it means is if some vendor offering you what is called a database-as-a-service from DBaaS 
通常来说，它的意思是，一些企业给我们提供了一种叫做database-as-a-service（DBaaS）的服务，即将数据库作为一个服务提供给大家
01:06:26 - 01:06:29
the idea is that you give Amazon whoever your credit card 
这里的思路是，就像你拿你的信用卡给Amazon交个费，
01:06:29 - 01:06:34
and they will say here's your JDBC or no ODBC connection ,port number and hostname 
然后Amazon会给你属于你的JDBC 或 ODBC connection 、端口号和 hostname 

01:06:35 - 01:06:37
That you should start shoving queries into it
然后，你就可以在它之上进行你的数据库操作
01:06:37 - 01:06:42
and you don't worry about how to manage the nodes, you don't worry about how to do backups 
你根本就不要管该如何去管理这些节点，也无须担心要去做备份啥的
01:06:43 - 01:06:44
they take care of all of that for you 
它们都给你做好了


01:06:46 - 01:06:49
and so I already sort of mentioned this before 
so 关于下面这点，我已经在前面涉及过了
01:06:49 - 01:06:56
but the in these some major cloud vendors people that control the whole stack, like Amazon, like Google, like Microsoft 
在这些主要的云供应商中，比如Amazon、Google、Microsoft 
01:06:57 - 01:07:03
the lines between what is a shared disk system versus a share nothing system is starting to get really blurry 
关于shared disk system vs share nothing system之间的使用界限已经非常模糊了
01:07:04 - 01:07:10
because they can push down database logic between up and down the different layers of the system stack 
因为它们可以在系统的不同层之间上下移动数据库逻辑
01:07:10 - 01:07:14
in a way that you typically can't do unless you control the hardware yourself 
这样的话，你通常很难去做一些事情，除非你自己控制着这些硬件
01:07:15 - 01:07:22
so for example Amazon has something called Aurora, Aurora is a shared disk version of MySQL in Postgres 
so ，举个例子来讲，Amazon 有个叫Aurora的产品，Aurora是一个shared disk类型的数据库产品，它有MySqL和Postgres两个版本（知秋注：想要对Aurora有更多了解，可以参考MIT 6.824对应的课程部分）
01:07:22 - 01:07:29
but they actually push logic to do transaction management down into the storage layer in the EBS
他们实际上将事务管理逻辑推入到EBS的存储层
01:07:29 - 01:07:31
again at the shared disk level 
即是在shared disk 级别来做的
01:07:31 - 01:07:38
and so now it's not pure shared disk system anymore, it starts to look a bit more like a shared-nothing system 
so，现在，它不再是纯粹的shared disk system了，它开始有点像shared-nothing system（知秋注：因为在存储层面涉及到了事务处理）
01:07:40 - 01:07:47
so alright so I think in the next ten years, I think the cloud systems will receive the more you see the most innovation in the cloud systems 
so，我认为在未来十年，你将会看到云系统中更多特别具有创新性的东西
01:07:47 - 01:07:49
and I think it's really really interesting what they can kind of do 
我想这是我们很感兴趣的东西



01:07:51 - 01:07:59
alright so I think I've really talked about this in general, a cloud system could either be a managed database or a cloud-native database 
alright ，大体来说，一个cloud system可以是托管数据库（managed database）或云本地数据库（cloud-native database即我们常说的云原生数据库）
01:08:00 - 01:08:06
so managed database would be just taking an off-the-shelf database, that system that was written, to run on-premise on dedicated hardware 
so所谓managed database就是只是采用现成的数据库在专用硬件上本地运行
01:08:06 - 01:08:10
and now you're just running this for you as a service, take MySQL take Postgres 
now，你就是将运行的MySQL 、Postgres 作为一个服务（提供出来）
01:08:11 - 01:08:15
and then you plop it into an EC2 instance ,and then have people connect to it 
然后将它放入一个Amazon EC2实例，接着让人们去connect它
01:08:16 - 01:08:21
and they don't know and don't care that you're managing EC2 for them, that they could have done that themselves 
人们无须关心去管理这些EC2，Amazon 已经帮他们管了
01:08:22 - 01:08:26
but you're just providing a service to do all the backup and other management stuff for them 
你只是提供一项服务来执行一些备份和其他管理工作
01:08:27 - 01:08:32
so most of the times when you see when you get a cloud database, it's gonna be it's gonna be the first one
so，大多数时候，当你拿到一个cloud database，它往往会是ppt中的第一种情况
01:08:32 - 01:08:39
now there are some systems where though they'll refer to themselves as being a cloud-native DBMS 
now，在某些系统中，它们会将自己称为cloud-native DBMS 
01:08:39 - 01:08:45
and these are ones where they're designed to operate in a cloud environment, and typically they're going to be a shared disk architecture 
这些都是为了在云环境中运行而设计的，通常它们用的是shared disk架构
01:08:46 - 01:08:50
because they don't want actually have to build you know replicate EBS or S3 
因为他们并不想去构建类似于有可复制能力的EBS 或S3系统
01:08:50 - 01:08:55
so they'll build on top of the existing storage infrastructure that these cloud vendors provide you 
它们会基于这些云供应商为我们提供的现有存储基础架构来构建云系统
01:08:55 - 01:08:58
and they're providing the compute layer on top of it 
它们在这些存储基础架构之上提供一个计算层
01:08:58 - 01:09:02
you know Stefan do query planning, you still have to do all the fault-tolerant stuff we talked about before 
比如在这一层做查询计划工作，你也可以做我们之前讨论过的容错（fault-tolerant）等
01:09:02 - 01:09:05
but they don't worry about actually how to you know persist things to disk 
但你无须担心它们实际上是如何将数据持久化到磁盘的
01:09:05 - 01:09:08
they just let cloud vendor provide that for you 
持久化这个东西，云服务上会提供给你的



01:09:11 - 01:09:15
so there is now also a new class of systems that label themselves as being serverless, yes
现在还有一类新的系统，我们称之为 serverless，请讲




01:09:25 - 01:09:29
yeah question is what would it like what is it what is truly the difference here
她的问题是这两块的区别在哪里
01:09:29 - 01:09:33
so this would be a manager this would be just using MySQL example 
so 对于第一种方案来说，这里可以这么说，它托管了一个MysqL实例
01:09:34 - 01:09:40
I take MySQL, I didn't run it, without making any changes to it ,I run it in a container or I run it in a VM 
我得到了MySQL服务，不需要对它进行任何的改变，它运行在一个容器中
01:09:41 - 01:09:44
and it's the same software that I would run on my local machine 
它和我们在本地机器运行的软件一样
01:09:45 - 01:09:47
but just now I'm running it in a cloud for you
只不过这里它运行在了云上而已
01:09:48 - 01:09:56
right and then old and then the , the service provider will then also do the backups and recovery and all the other stuff for you 
然后，作为服务提供商，它背地里还为我们提供了备机（backups）、恢复（recovery）还有其他一些东西
01:09:56 - 01:10:00
this would be like I'm building a new database system scratch, or I take an existing one 
这就像是我是要从头开始构建新的数据库系统，还是我会采用一个已存在的数据库系统
01:10:00 - 01:10:05
and I make heavy changes to it to be designed to work in a cloud environment 
我对其进行了重大更改，以使其可以在云环境中工作
01:10:06 - 01:10:08
so up here running MySQL 
so，就好比这里我运行了一个MySqL
01:10:08 - 01:10:12
MySQL doesn't know anything S3, doesn't know anything about the performance implications of 
MySqL对S3一无所知，它不清楚任何关于S3性能上的东西
01:10:12 - 01:10:15
you know reading EBS and things like that just have a disk it has these properties 
你在读EBS的时候，所做的事情就像是在读一个给我们提供了这些潜在功能的磁盘
01:10:15 - 01:10:19
this would be like oh I've designed the system's explicity to work on S3 
我们会在S3上来设计这个系统进行工作
01:10:19 - 01:10:21
S3 provides me these guarantees provide me these properties 
S3 给我们提供了这些功能性的保证
01:10:21 - 01:10:24
that information now permeates all throughout the system 
这些信息会在整个系统中可见
01:10:24 - 01:10:31
my query optimizers cost model can reason about,you know what's the speed of writing S3, or what you know what can you do a S3, you can't do on EBS ,stuff like that 
我的查询优化器成本模型可以推断出，你写入S3的速度是怎样的，你可以在S3上做什么，不能再EBS上做什么，等诸如此类的事情


01:10:35 - 01:10:38
alright so there's a the buzzword going around now is serverless
alright ，so，这里有一个当下极为流行的词汇，即serverless
01:10:39 - 01:10:40
so of course  serverless databases 
即这里的serverless databases
01:10:41 - 01:10:45
and so it's the same everything's the same that we talked about before
so 它和我们之前谈到的都是一样的
01:10:45 - 01:10:51
it's just that the idea is that when your machine goes idle, your database connection those idle
这里的思路是，当我们的机器清闲了，你的database connection处于闲置状态
01:10:51 - 01:10:53
because your application is no longer sending queries
因为你的应用程序不再发送查询请求
01:10:54 - 01:11:00
they will try to then deeper vision or have you  not pay for hardware that you're not actually using 
服务商会尝试更进一步的做法，即当你实际上并未使用数据库服务的时候，你并不需要为硬件来支付开支的
01:11:01 - 01:11:06
so let's say that I have again a managed database system, I have a single node, it's running MySQL 
so 假设我有一个托管数据库系统（managed database system），我有一个节点，它正在运行MySQL
01:11:06 - 01:11:10
and so I pay for the instance I pay for the storage 
so 我要为数据库实例以及底层存储支付金钱
01:11:11 - 01:11:13
and I have to provision it to be running all the time 
我得一直运行着它


01:11:13 - 01:11:17
so my application sends queries to this guy, and you know and gets results comes back 
so ，我的应用程序会发送查询请求到这里，然后得到返回的结果


01:11:17 - 01:11:22
but now if my application goes to sleep ,or walks away, goes the bathroom, does whatever 
如果我的应用程序清闲了，比如它是个人的话，那就是睡觉了，出去溜达了，去厕所了，等等


01:11:23 - 01:11:28
then I'm paying for these resources I'm not actually using 
实际上，此时，我并没有使用节点处的资源，但我还得为它掏钱
01:11:28 - 01:11:32
right because I have to provision the hard right that provision the EC2 instance, I provisioned the EBS storage 
因为我必须配置EC2实例，所以配置了EBS存储
01:11:32 - 01:11:36
right so I'm paying for the stuff to run and not actually do anything 

so，哪怕我实际上并没有做任何工作，我也得支付相关费用



01:11:37 - 01:11:42
so the idea with the serverless database is it's almost always a shared disk architecture 
serverless database的思路是，它几乎总是 shared disk 体系结构


01:11:44 - 01:11:47
is that I can do all my queries that I had before 
我可以和我之前一样，去做我的查询工作



01:11:47 - 01:11:52
but now when I go to sleep, I decommission the compute side of things 
但现在，当我要去睡觉的时候，我停下我计算的脚步


01:11:52 - 01:11:57
this goes away but before I do I basically take a snapshot of what pages are my buffer pool 
在走之前，基本上我会制作一个我自己buffer pool的一个快照
01:11:58 - 01:12:03
I take a checkpoint then record all the page IDs or what's in my buffer pool , write that out the shared disk 
 通过这个checkpoint，来记录我buffer pool中所有的pageId，将这些写出到shared disk 


01:12:03 - 01:12:05
then kill off my computer 
 然后关闭我的电脑



01:12:06 - 01:12:11
and so now the only thing I'm paying for is storage, just paying for my database rest at idle on disk 

so，现在，我要支付的只是存储我自己数据的那一部分（知秋注：Node实例就不需要花钱了）



01:12:12 - 01:12:17
and then if I ever wake up and come back and execute a query ,the very first thing we're gonna do is say 
然后，当我醒来回到工作岗位，要执行查询的时候，要做的第一件事是，


01:12:17 - 01:12:20
alright well before I shut down the last time, what was in my buffer pool 
我上次关闭的时候，我buffer pool的那些东西呢
01:12:20 - 01:12:24
and got it trying to fetch that in and make it as if I was still running all the time
然后将这些东西从存储位置处拿到，并放入node，并让我可以继续进行工作
01:12:28 - 01:12:33
so this is one way to do it if you assume that there's only one customer I'm running on a single node
so 这里，我们假设的是当下这个单节点上只有一个Customer运行
01:12:33 - 01:12:38
another common setup would be you run multiple customers or multiple tenants on a single node 
另一个常见的设置是你在单个节点上运行多个customers 
01:12:38 - 01:12:41
and then you just recognize that this customer has not sent me to query in a while 
然后，你就会意识到，在一段时间内，这个customer并没有发送给我查询请求
01:12:41 - 01:12:45
and then I write out its buffer buffer pool page contents out the disk before 
然后，我就将它的buffer pool page内容写出到磁盘
01:12:46 - 01:12:46
yes
请讲


01:12:55 - 01:12.56
so this question is 
So，他的问题是
1.12.56-1.12.59
why am I doing this section step
为什么我要做这一步
1.12.59-1.13.05
,why am I actually writing the buffer pool page contents out to disk ,before you know why do I care about this 
为什么我要将这些buffer pool page内容写出到磁盘，为什么我会如此关心这个东西，
01:13:06 - 01:13:11
because you want to make it as if like the the most expensive thing is fetching things from disk 
因为你知道，成本最高的事情就是从磁盘中获取数据
01:13:12 - 01:13:12
so ideally 
So，这里的想法是

I want to have you know my query the next query shows up a minute later 
我想在一分钟中去执行下一条查询
01:13:17 - 01:13:20
I don't want to have everything all now evicted from my buffer pool 
我不想现在从缓冲池中逐出所有内容
01:13:20 - 01:13:25
and I have to pay this big upfront cost of warming all my data I'm bringing into memory 
这样我需要付出很大的代价来warm （预加载）我们的数据，即将它们放入内存中
01:13:26 - 01:13:30
so you record all this information so that when you come back again a minute later 
so，你记录了所有这些信息，当你在一分钟后回来时，
01:13:31 - 01:13:35
it looks like the thing was still running, and you don't like you still paid a penalty case you have to fetch it in 
它看起来就像是依然在运行状态中，你不想要那种你还得要从磁盘中获取数据这种消耗性的惩罚
01:13:35 - 01:13:37
but you don't have to wait to fetch everything in 
你不必等待去获取所有内容
01:13:38 - 01:13:39
you try to maybe prefetch some stuff
你会尝试预获取一些东西
01:13:40 - 01:13:42
you pre-warming with creamer in the cache yeah 
你可以在缓存中进行预热
01:13:43 -  01:13:47
every database system does this when you call shut down do it like a crash shut down they're already doing this 
每个数据库系统在调用shutdown时都会这样做，崩溃时也一样

01:13:58 - 01:14:04
correct so his question is, if I don't do this step well this still work 
对的，so 他的问题是，如果我不去做这一步，它依然可以工作
01:14:04 - 01:14:10
yeah it'll be correct it'll be slow, right, this is just an optimization to can warm the cache 
yeah，没问题，它会比较慢， right,这就是一个优化，即预热缓存
01:14:21 - 01:14:28
in a cigarette-like we're just storing the page ID, we had in the buffer pool  at the moment did a shutdown 
在我们执行shutdown的时候，我们要存储我们在buffer pool中对应的pageId（知秋注：因为你这个应用都停了，你就是要落地你在buffer pool中对应的数据）
01:14:29 - 01:14:34
so then the first query shows up, and then say the first query says go give me page 1、2、3 
so 第一个查询说，给我Page123
01:14:34 - 01:14:39
it says oh I also had 456789 also in there let me go fetch them and fetch the Nolan 
它会说，我也会将4、5、6、7、8、9拿过来（知秋注：也就是预热缓存的操作）
01:14:41 - 01:14:41
Yes

01:14:44 - 01:14:48
this question her question is how does it like to be stateless as in the serverless
她的问题是，在serverless中，该如何才能做到无状态（stateless ）
01:14:48 - 01:14:51
it's usually what people mean my service is stateless 
通常，人们口中所说的他的服务是无状态的
01:14:52 - 01:14:54
But it isn't right 
但，它并不是
01:14:55 - 01:14:59
so if  serverless from the perspective of the end-user
so, 如果从终端用户（end-user）的角度来看serverless 
01:15:00 - 01:15:06
meaning like I don't have to say provision me this machine, so a lot of cloud vendors, they're not doing a serverless architecture
这就好像是在说，没有必要给我提供这么一台某种意义上单独的主器，so，对于很多云服务厂商来说，他们没有做serverless 架构
01:15:06 - 01:15:12
you basically say, you know I want to pay for this dedicated resources to be available for me at all time 
你基本就是在说，我想为此专用资源付费，以便随时可以使用
01:15:13 - 01:15:15
if you're not using them they're so charging you they're happy 
如果你没有使用这些资源，而他们依然收着你的钱，他们当然很高兴
01:15:16 - 01:15:16
right 

01:15:17 - 01:15:23
but if I mean I have one query a minute ,I don't want to me you know, I don't want to provision a whole machine just to send X 60 queries an hour 
但，如果我一分钟才发一个查询请求，我不想去专门搞一台主机，就为了一小时那60个查询？
01:15:23 - 01:15:27
whereas in this architecture I can still have my database still as if it was running all the time 
而在这种serverless 架构中，我仍然可以保持数据库一直运行
01:15:28 - 01:15:33
but I'm not paying that pending, so I pay or not I paid on a per-query basis plus whatever I'm storing over here 
 但我无须去花那么多钱，我只需要为我每次查询花费一次即可，还有就是为存储数据花费一些


01:15:37- 01:15:43
so the main vendors that are in this space would be Amazon has a serverless  version of MySQL 
so 关于这个领域，主要的供应商有Amazon提供的一种MysqL版本的serverless 产品
01:15:43 - 01:15:47
and then faunaDB is a as a separate database startup that does 
FaunaDB是另一个数据库初创企业所做的数据库系统
01:15:48 - 01:15:53
so Amazon will try to all these guys are doing not exactly this where you kill the machine 
这些系统并不会真的将主机停下来
01:15:53 - 01:15:58
they're just recognizing this customer has not sent a query while and then they write everything out to disk 
它们只是会认知到，这个customer在一段时间内不会再发送查询请求，然后，它们就将关于该Customer的一切东西写出到磁盘中
01:15:59 - 01:16:02
so Azure can do this and then Google
So，Azure和Google都能做到这点
1.16.02-1.16.05
, I think it's these icons are so useless 
我觉得这些图标真的很没用
01:16:05 - 01:16:06
because this like doesn't say the name
因为它并没有说出它表示的名字是什么
1.16.06-1.16.07
, I know and I second what this says 
我想了一会儿才知道，它是什么意思
01:16:07 - 01:16:09
I think it's the Google fire store
我觉得它是Google的Firestore
01:16:09 - 01:16:10
so not spanner
So，它并不是Spanner
1.16.10-1.16.12
 or not the big query stuff
它也不是bigquery
1.16.12-1.16.13
 it's only for fire store 
它代表的只是firestore



01:16:15 - 01:16:15
all right

1.16.15-1.16.18
so another interesting thing about this is that 
So，对此，另一件我们感兴趣的事情是
01:16:18 - 01:16:23
you could potentially build a database system without having to write every piece of the system yourself 
你可以在不需要手写系统中每个组件的情况下，就能构建一个数据库系统
01:16:25 - 01:16:30
so there's enough open-source software that's out there now or services 
So，这里有一堆相关方面的开源软件或者服务
01:16:30 - 01:16:33
that are out there you can cobble bunch of these things together 
你可以将这些东西都放在一起
01:16:33 - 01:16:37
and make a new cloud database without having to write everything from scratch 
在不需要从头开始写起的情况下，就能做出一个新的云数据库
01:16:38 - 01:16:39
so we've already talked about
So，我们之前已经讨论过这个
1.16.39-.16.40
 like as I said 
正如我说的
1.16.40-1.16.42
her question was what is the cloud-native database 
她的问题是什么是云原生数据库
01:16:42 - 01:16:46
well it's one where you assume that you can write you know you the sulfur is written to assume you're writing to S3 
假设你要基于S3去写一个数据库
01:16:46 - 01:16:50
and you take those it's guarantees or performance implications into the design
你可以使用这些来做一些约束保证或性能融入到你的设计中
01:16:51 - 01:16:54
so that would be sort of one example,
So，这就是其中一个例子

 you're not the write that you know disk manager 
你不需要去写磁盘管理器
01:16:55 - 01:16:56
you just let S3 handle that for you 
你只需要让S3去为你处理即可
01:16:57 - 01:16:59
you may not have to write your catalogs ,right 
你可能不需要编写你的catalog
01:16:59 - 01:17:04
you get metadata as a service through these bunch of different types of software 
你可以将这些不同类型的软件拿出一个来作为服务，就可以得到metadata
01:17:05 - 01:17:06
you may not the manage your cluster 
你可以不用去管理你的集群
1.17.06-1.17.12
you can relies on Kubernetes, or yarn ,or these other tools that the vendors have to handle all that for you
你可以依靠Kubernetes、Apache YARN或厂商提供的工具来为你管理这些
01:17:12 - 01:17:15
and then you may not even have to build your own query optimizer 
你可能也不需要去构建你自己的查询优化器
01:17:15 - 01:17:21
so there actually are some open source sort of optimizers as a service
So，实际上这里有一些开源的查询优化器
1.17.21-1.17.24
 ,where its runs on a separate node 
你可以在单独的节点上运行它
01:17:24 - 01:17:28
you just feed it a bunch of XML or JSON metadata to say , here's what my data looks like here's my query plan looks like 
你可以往它里面传入一堆XML或者JSON元数据，以此来表示这是我的数据和查询计划
01:17:29 - 01:17:35
and then the separate machine will then crunch on it and spit out a potentially optimized plan for you 
这个单独的机器会对它处理，并为你制定出一个优化过的计划
01:17:35 - 01:17:37
I don't know how good these things are actually are
我不清楚这些东西好不好
1.17.37-1.17.41
, I'm there has been any study actually to evaluate them yet 
目前它们并没有什么相关的评估报告
01:17:41 - 01:17:44
we looked at Orca when we were building our system and we passed on it 
当我们在构建我们的系统时，我们看了下Orca，我们并没有使用它
01:17:44 - 01:17:47
because at the time the documentation was terrible
因为在那个时候，它的文档很糟糕
01:17:47 - 01:17:49
Calcite written in Java 
Calcite是用Java来写的
1.17.49-1.17.50
,so that was a non-starter for us as well 
对我们来说也没啥大用



01:17:51 - 01:17:52
all right
1.17.52-1.17.57
,the last thing I briefly talked about is file formats 
最后我要简要讨论的东西就是文件格式
01:17:58 - 01:18:01
so pretty much until very recently
So，一直到最近
1.18.01-1.18.06
 every single DBMS always had their own proprietary binary file format 
每个DBMS始终都有它们自己专有的二进制文件格式
01:18:06 - 01:18:07
meaning like
这意味着
1.18.07-1.18.09
 MySQL writes a bunch of files a disk
MySQL会将一堆文件写入到磁盘
01:18:09 - 01:18:11
you can't read those files into Oracle
你无法在Oracle中读取这些文件
1.18.11-1.18.13
 ,well because Oracle has its own file format 
因为Oracle有它自己的文件格式
01:18:13 - 01:18:16
can you do the same thing when you built put your projects on BusTub
当你们使用Bustub来构建你们自己的DBMS时，你们也可以做相同的事情
01:18:16 - 01:18:18
BusTub has its own page formats
Bustub有它自己的page格式
1.18.18-1.18.20
 ,that can't be read by anybody else 
它无法被其他DBMS所读取
01:18:21 - 01:18:22
so this is problematic now, 
So，现在这就很成问题了
1.18.22-1.18.24
if you're in a cloud environment
如果你在一个云环境下
1.18.24-1.18.25
 where you have a bunch of different services 
你有一堆不同的服务
01:18:26 - 01:18:27
that may want to share data
它们想要共享数据
1.18.27-1.18.30
, like a bunch of data I want to generate my OLTP databases 
比如：我的OLTP数据库生成了一堆数据
01:18:31 - 01:18:33
and maybe I want to run that data through spark
可能我想将这些数据放入Spark中处理
1.18.33-1.18.36
 or learn that data through through Vertica or some other distributed database 
或者我想将这些数据放入Vertica或者其他分布式数据库中
01:18:36 - 01:18:37
so there right now
So，此时
1.18.37-1.18.40
 if everything's based on a proprietary binary format 
如果所有数据都是以一种专有的二进制格式保存的话
01:18:40 - 01:18:43
the only way you can get data out from one system and put into another system
你将这些数据从一个系统放入另一个系统的唯一方法是
01:18:43 - 01:18:48
It`s to make copies and put it into one of these human readable text readable formats 
复制这些数据，并将这些数据转化为我们能看得懂的可读文件格式
01:18:48 - 01:18:52
and so now instead is that there's these open source binary formats
相反，这里有一些开源二进制格式
1.18.52-1.19.00
, that a bunch of cloud vendors ,or and distributed databases ,or data science ecosystem tools are now supporting to that
现在有一堆云厂商、分布式数据库或者数据科学生态工具支持这些格式
01:18:58 - 01:19:04
 I could use write out to S3 or EBS or my distributed file system 
我可以将这些格式的数据写入到S3或EBS或我的分布式文件系统中
01:19:04 - 01:19:07
a bunch of these files in this format,that might get my DBMS generated 
我的DBMS可以生成一系列这种格式的文件
01:19:07 - 01:19:12
and then I could suck them in and read them into another database without having to do that any deserialization or conversion 
在不需要做任何解密或转换的情况下，我可以将这些数据塞入另一个数据库，并在该数据库中读取这些数据




01:19:14 - 01:19:17
so some of these you may have heard of 
So，你们可能之前听过其中一些
1.19.17-1.19.19
,but these are just these are sort of a main ones
但这些都是属于比较主流的那一类
01:19:19 - 01:19:21
Parquet and ORC two most common ones
Parquet和ORC是其中最常见的两个
1.19.21-1.19.26
Parquet Cloudera and twitter and ORC came out of hive 
Twitter和Cloudera使用的是Parquet，ORC则是根据Hive所修改出来的
01:19:26 - 01:19:29
and again think of these are like binary column store formats
这些都是二进制列式存储格式
1.19.29-1.19.32
, that are not tied to any one specific database system 
这些不与任何特定的数据库系统挂钩
01:19:32 - 01:19:33
like an open specification
这就像是一种开放标准
1.19.33-1.19.35
 that anybody can modify their database system
任何人都可以修改他们的数据库系统.
1.19.35-1.19.38
modify their application to read this data natively 
修改他们的应用程序让它们做到原生读取这些数据
01:19:39 - 01:19:43
CarbonData is a newer one actually 2016 I think 
我记得CarbonData是一个比较新的格式，它实际是在2016年推出的
01:19:43 - 01:19:47
that's like ORC and the parquet that came out of Huawei in China 
它和ORC以及Parquet类似，它是由中国的华为推出的
01:19:48 - 01:19:52
Iceberg is another new one from Netflix
Iceberg则是由Netflix所推出的一种新型存储格式
1.19.52-1.19.54
 that I've just notified recently by somebody 
这也是最近才有人告诉我的
01:19:55 - 01:19.56
I have looked into much into yet
我还没有怎么看这个东西
1.19.56-1.19.57
but they claim that
但他们声称
1.19.57-1.20.00
 they can support schema evolution
他们支持schema evolution（schema 进阶版）
1.20.00-1.20.00
 like I can do change columns 
就比如我可以对列进行修改





01:20:01 - 01:20:04
and you know change names ,change column types, the way these other guys can 
比如修改名称，修改列类型之类的事情
01:20:05 - 01:20:06
these other guys are read-only
其他这些都是只读的
1.20.06-1.20.06
 like I create the file
比如：我创建了文件
1.20.06-1.20.07
 , and then I freeze it
接着，我就将这个文件冻结
1.20.07-1.20.09
 and I can't go back and change them 
然后，我就没法回过头去修改它了
01:20:09 - 01:20:17
HDF5 is not typically used in cloud systems or in sort of traditional Silicon Valley database for tech companies 
在cloud system或者传统的硅谷那些数据库科技公司通常不会去使用HDF5
01:20:17 - 01:20:20
this is mostly found in HPC in the scientific world 
这种格式最常见于高性能计算领域和科学领域
01:20:21 - 01:20:28
this is for like a Readata like you can have your particle collider can well spit out bunch of these files in this type 
通过它，你可以通过这个类型来管理大量的文件（知秋注：有点类似于文件管理系统）
01:20:28 - 01:20:31
and then Arrow is an in-memory column format
Arrow是一种内存型列式格式
1.20.31-1.20.33
 that came out of Pandas/Dremio 
它是由Pandas/Dremio所推出的
01:20:34 - 01:20:38
that think of this is like it's like parquet ORC, but it's for in-memory data 
你们可以将它想象成Parquet和ORC之类的东西，但它是内存型数据
01:20:38 - 01:20:41
so our database system is building here at Carnegie Mellon
So，我们的数据库系统是在CMU构建的
1.20.41-1.20.44
, our native stored format is actually arrow 
实际上，我们的原生存储格式使用的就是Arrow
01:20:44 - 01:20:52
so you could take data that our DBMS generates , dump it out and feed it into pandas or whatever it's I can't learn you want  , anything that reads the arrow format 
So，你可以将我们DBMS所生成的数据转储出去，并放入Pandas之类的东西中进行处理，或者任何能读取这张Arrow格式的数据的工具（知秋注：pandas 是基于NumPy 的一种工具，该工具是为解决数据分析任务而创建的）

01:20:53 - 01:20:55
so I think this is the right way to go 
So，我觉得这是正确的做法
01:20:55 - 01:20:57
it is sort of you get to the lowest common denominator 
我觉得这就像你拿到了最小公分母一样
01:20:57 - 01:21:005
like there's you know the compression scheme and all these these different these formats may not be the best for all possible applications
对于所有应用程序来说，这些压缩方案和不同格式可能并不是最好的
01:21:02 - 01:21:06
 and certainly if you write a custom one 
当然，如果你自己写了一个
01:21:07 - 01:21:09
you could probably get much better compression or better performance 
你可能会得到更好的压缩比或更好的性能
01:21:10 - 01:21:13
but this this provides you interoperability, okay 
但这个会给你提供数据库间的互操作性，okay


01:21:14 - 01:21:17
all right so just a finish up, OLAP
so，我们来对OLAP做个收尾
01:21:18 - 01:21:25
it means if you if you need analytical database , that needs to scale, then you have money, because you're getting data,right 
如果你需要一个分析型数据库，需要去scale，那你就需要money，因为你要更多的数据，
01:21:25 - 01:21:29
people actually using whatever application you have , and you're actually able to process it 
人们实际上正在使用你提供的任何应用程序（浏览器端、手机端等），并且实际上能够处理人们发来的请求
01:21:29 - 01:21:31
but the more data you get and more problems you're gonna have 
但，随着数据积累越来越多，随意而来的就是越来越多的问题
01:21:31 -  01:21:39 
because these distributed databases you know all the additional management concerns and problems, that you have with the distributed system you do have to account for 
因为对于分布式数据库来说，有太多管理上的问题需要你去考虑



01:21:40 - 01:21:48
all right, so here's question last time can I names what are some interesting or now these weren't good 
all right, 这里有个问题，我之前说了这些命名不是很好
01:21:48 - 01:21:52
but what are the main OLAP systems you could possibly look at 
但，你能看到的主要的OLAP system就这些
01:21:52 - 01:21:57
well in the cloud REDSHIFT and snowflake are the key to two players
Amazon的REDSHIFT 和snowflake是关键的两个玩家
01:21:57 - 01:22:01
Oracle and Microsoft and Google also have their own Pacific services 
Oracle 、Microsoft 和Google 也有它们自己的产品服务
01:22:01 - 01:22:04
I think these two are the probably the biggest ones 
我认为这两个是最大的存在
01:22:04 - 01:22:08
so this is run if you're running on a cloud， if you want to run on-premise
so，这是在云上运行的这些数据库（知秋注：可以看作是公有云），如果你想要运行的是on-premise（知秋注：可以看作是私有云）
01:22:09 - 01:22:12
the one that I'm actually super interested in is ClickHouse 
我实际上最感兴趣的是ClickHouse 
01:22:13 - 01:22:17
so distributed in-memory column-store system out of Russia from the Yandex guys 
ClickHouse是俄罗斯第一大搜索引擎Yandex开发的内存型列式储存数据库.
01:22:18 - 01:22:23
and when you read their web page the list of the things that they support, it's actually amazing and it's open-source to 
当你去它们的web网页去看它们的支持列表时，你会感觉到不可思议，而且它是开源的！！
01:22:25 - 01:22:30
presto is runs on top of I think SPARK and Hadoop 
presto ，我想，它是运行在SPARK 和 Hadoop 之上的
01:22:30 - 01:22:37
splice machine is HBase plus SPARK, Greenplum is the fourth version of Postgres that somebody made distributed 
splice machine是HBase + SPARK,Greenplum 是Postgres 的一个分布式版本
01:22:37 - 01:22:41
it was a startup back in the 2000s they got bought by EMC 
它开始于2000年，它被EMC买了
01:22:41 - 01:22:46
the EMC says we don't wanna be a databases company ,so then they merged with VMware became pivotal 
EMC说他们不想成为一个数据库公司，然后它们合并到VMware，变成了pivotal 
01:22:46 - 01:22:47
but in May open-sourced it 
它是开源的
01:22:47 - 01:22:53
Vertica was founded by my advisors back in New England
Vertica 是由数据库研究人员Michael Stonebraker和Andrew Palmer于2005年创立，其中一个有作为我们的客座演讲者
01:22:53 - 01:22:59
they got bought by HP the guy came gave a talk a few weeks ago or the semester
它被惠普收购了，这家伙前几周有给我们做过分享
01:22:59 - 01:23:03
so the way to think about this is like if you have no money start here 
你可以这么去思考，如果你没钱，你就从这里开始
01:23:04 - 01:23:05
if you have the money 
如果你有钱
01:23:06 - 01:23:09
you could start over here like , Oracle EXADATA and Teradata are super super expensive 
你可以从这里开始，Oracle EXADATA 和Teradata 是真的贵！
01:23:10 - 01:23:14
like EXADATA I don't think you can buy a machine for less than like you know two million dollars
拿EXADATA 来说，我不认为你可以花不到200万美元买到一台机器
01:23:14 - 01:23:15
because you're buying custom hardware
因为你买的是定制化硬件
01:23:17 - 01:23:17
okay 

01:23:20 - 01:23:24
alright if you don't want to go down this route its setting up a whole distribute database 
如果你不想通过这条路去建立一个完整的分布式系统


01:23:25 - 01:23:28
there's a newer system and I was all very excited about called DuckDB 
有一个更新的系统，我可以很兴奋告诉大家，它叫DuckDB 
01:23:29 - 01:23:35
it's out of Europe, think of this DuckDB as like SQLite for analytics 
DuckDB 可以认为是用于SQLite 的分析
01:23:35 - 01:23:39
so you can run it as an embedded database , it actually you can connect to it through the SQLite terminal 
so，你可以将其作为嵌入式数据库运行，实际上你可以通过SQLite终端连接到它
01:23:40 - 01:23:45
but it's a column store and can do you can do analytics on in the same kind of way you can do here 
但它是列式存储，你可以采用与此处相同的方式进行分析
01:23:45 - 01:23:49
so if your data can fit on a single node, DuckDB might be the right thing 
so，如果你的数据可以放在单个节点上，DuckDB 可能是一个不错的选择
01:23:49 - 01:23:52
And it's open source ,okay
它是开源的


01:23:53 - 01:23:57
All right ,so that's it ,for the the main material and the semester 
All right ，这就是本学期的主要内容了
01:23:58 - 01:24:02
the next class after the holiday we'll have a guest speaker from Oracle 
下节课会有来自Oracle的客座讲师来给大家分享
01:24:02 - 01:24:05
and again we'll have the final review and the potpourri session on Wednesday 
我们将在周三进行最终审查
01:24:06 - 01:24:15
so at this point again ,you should be confident enough to go out in the real world , and either manager database or use database , and know enough about to opine whether
再次，在此时此刻，你应该有足够的信心在走到社会中，无论是管理数据库还是使用数据库，你都有足够的能力
01:24:15 - 01:24:17
you know it the system you're working with is making good ideas 
你可以为你正在使用的系统提供好的建议
01:24:17 - 01:24:17
okay

01:24:18 - 01:24:31
we've covered a lot and you guys hopefully,^^^^^^
我已经讲了好多了，希望大伙可以对数据库开发有足够的信心！相信自己！
 you guys okay okay don't know you can't



