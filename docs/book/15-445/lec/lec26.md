

00:17 - 00:19    
all right last class and it's snowing
最后一堂课了，还是个雪天

00:20 - 00:22   
DJ drop tables thank you 
再次感谢我们的DJ drop tables

00:24 - 00:27   
so I noticed you don't have your deck today ,what happened 
我看你今天没带家伙事儿，出啥事儿了

00:29 - 00:32   
yes you had to buy her a gift now 
你得给她买个礼物

00:33 - 00:34   
yeah so what'd you do
你干啥了

00:35 - 00:40   
you sold your deck ,yeah how you gonna how you to drop the beats now 
你把家伙事儿卖了，那你现在怎么打拍呢

00:45 - 00:46   
but this is getting interfere with your album now right 
这会影响到你的专辑
00:49 - 00:51
how long you don't have anything
那得多长时间能缓过来啊
00:55 - 01:01
all right well he's got his problems ,we we have database problems 
他遇到了他的问题，而我们有数据库的问题
01:01 - 01:05
but this is the last class and as I said it's just gonna be the final review 
这是最后一堂课了，我这作个期末总结吧

01:05 - 01:06
and then a system Potpourri 
顺带提下System Potpourri

1.06-1.09
what you guys voted on for what you want to be talked about 
以及投票选下你们想要听我讲些什么


01:10 - 01:13
so just real quickly the remaining things for the semester 
So，快速看下这学期我们剩下的东西
01:13 - 01:15
project 4 is due next week on the 10th 
Project 4会在下周的时候截止
01:16 - 01:18
and it's way off when's the 10th is that Monday Wednesday 
你们记得10号是周一还是周三么？
01:18 - 01:20
it's not even close right 
现在你们还有一定的余裕
01:20 - 01:26
so Tuesday December 10th with the the final project , and the and the extra credit will be due 
10号的时候，Extra Credit也会结束
01:27 - 01:29
we promised you feedback 
我们会给你们写下反馈
01:29 - 01:31
we found people plagiarizing 
我们抓到了一些抄袭的人
01:31 - 01:32
so we have to deal with that first 
So，我们必须先处理掉他们
01:32 - 01:40 
and then we'll hopefully be able to put out, the reviews for the feedback for everyone else within like the next day or so okay
我们希望我们会在截止后的一天放出我们对你们每个人的反馈
01:40 - 01:42
so apologize for the delay  
So，要原谅下这种延迟
01:42 - 01:45
over all they were pretty good ,some are better than others obviously 
总的来说，你们都做的不错，有些人做的要比其他人更好
01:45 - 01:48
but the feedback will help guide you to finish it up yes 
但这些反馈会指导你让你做的更好，请问
01:52 - 01:55
so you remember how when you filled out the form you know click the check box
So，你还记得当你填写表格的时候，你点了checkbox
01:55 - 01:59
yes I agree I'm not gonna plagiarizing , big they still plagiarize, yes 
你表示，你同意你不会去抄袭，但有些人还是抄袭，请问
02:02 - 02:05
well the very least zero
Well，最差情况就是0分处理
2.05-2.08
 , where I go talk to the Provos or not that's another story,yeah 
我会和其他负责这块的人讨论下，但这就是题外话了
02:10 - 02:13
again CMU does around they take this seriously 
再说一遍，CMU会严肃对待这件事情
02:13 - 02:15
and the fact that I made you check click that check box 
并且，我告诉你们让你们点checkbox这件事
02:15 - 02:18
and it's on video saying me saying don't plagiarize 
我已经用视频记录下来了，我跟你们说过不要抄袭
02:18 - 02:19
you don't have any evidence
你没有其他证据能反驳我没说过这句话
02:20 - 02:23
not you understand whoever did it yeah yeah

02:25 - 02:31
and then the final exam is due on a Monday Dec 9th after you going back to extra credit 
期末考试会在12月9号的时候举行
02:31 - 02:33
this is why we have it's like a wiki stylus ,we have revisions
这其实就像是wiki那样，我们会有多个版本数据
02:33 - 02:38
so the person could go back and try to remove the text 
So，人们可以回到之前的版本，并移除掉文本
02:38 - 02:39
but it's still in the Databases
但这些数据依然是在数据库中的
02:39 - 02:41
we can go see always go see it anyway
我们始终能看到这些数据
2.41-2.42
 again not you ,but in general 
你不一定能看到这些数据，但总的来讲，是可以看到的
02:42 - 02:48
all right final exam is on Monday at 5:30 p.m. in posner hall
All right，期末考试是在周一下午五点半的时候举行
2.48-2.51
I which I think is over there ,the old business school 
我觉得应该是在老的商科学院那里
02:53 - 02:55
yeah so that so we'll cover that first okay 
So，我们会先去讲这这块内容
02:55 - 02:57
so any questions about the extra credit or project 4
So，对于Project 4或Extra Credit你们有任何问题吗？
02:58 - 03:06
and then for homework five, we will have that graded and released by Friday , whatever I think he was due yesterday 
再讲下Homework 5，我们已经打好分，我们会在周五的时候告诉你们成绩如何，Homework 5应该是昨天才截止的
03:06 - 03:07
so four days after that 
So，应该是在4天后，我会给出你们的成绩
03:07 - 03:09
unless everyone has already turned them in we'll send it out, okay 
除非所有人都已经交了，我们才会立刻发布你们的成绩


03:12 - 03:14
all right the final exam 
All right，来讲下期末考试
03:14 - 03:15
all right

3.15-3.16
so you have to take it
So，你必须参加期末考试
3.16-3.20
 ,uh or you don't have to,but you should 
或者，你也不一定要参加，但你应该要参加
03:20 - 03:23
this is not published yet ,but I'll post this after class 
我还没将它发布出去，但我会在课后发布这个
03:23 - 03:29
this will be basically a summary of everything that I've talked about like ,it's the the same thing, I did for the midterm 
这就和我在期中考试前所说的一样
03:29 - 03:34
it's like the sheet of everything you need to know, what chapters in the book ,what homeworks matter and things like that
你们可以去带一张纸，纸上可以写满你需要用到的那些知识
03:34 - 03:37
again posner hall 100 at 5:30 p.m. don't come to this room 
考试的地点是在Posner Hall 100，考试时间是在下午5点半，所以你们不要来这个教室参加考试
03:37 - 03:40
and then if you're curious why you should take this other than you want to pass the class 
除非你只想及格线飘过，不然你别好奇为什么我让你们带这张纸
03:41 - 03:41
you can watch that video 
你可以去看下这个视频


03:42 - 03:44
all right so what do you need to bring
So，你们需要带哪些东西呢？
03:45 - 03:46
you should bring your CMU ID 
你们应该带你的CMU学生卡
03:46 - 03:50
because it's Class of 95 people or 96 people, I don't know I know everyone 
因为选这门课的人有95个或者96个，我没法一一说出你们的名字
03:50 - 03:51
so I need to check your ID
So，我需要检查你的学生卡
03:51 - 03:56
this like the midterm it's one page of handwritten notes double-sided,
就和期中考试一样，你们能带一张双面写满笔记的纸
3.56-3.59
 no shrinking down the slides, no copy and pasting the text
没必要直接把幻灯片给复制下来，也不要复制粘贴某些文本
03:59 - 04:03
if you write it by hand on your iPad，want to print that that's okay 
如果你是在iPad上手写了这些笔记，你想将它们打印下来，这是Ok的
04:03 - 04:07
but again just know like text text ,right from a word processor 
但不要是从Word那些软件上打印下来的文本
04:07 - 04:13
and then if you were in the class on Monday bring your extra credit coupon 
到了那天，你需要携带你的Extra Credit Coupn
04:13 - 04:16
and you have to turn that in when you turn in your exam 
你考试的时候，必须上交这个东西
04:17 - 04:19
so the thing is you optional
So，再来讲下Optional部分
4.19-4.20
 if you want to change your clothes halfway through 
如果考试中途你想换下衣服
04:20 - 04:22
somebody did that two years ago, I'm fine with that 
有人两年前这么干了，我是无所谓
04:22 - 04:26
because also it's at 5:30 ,uh you could bring food 
因为考试的时间是在下午五点半，所以你可以带些吃的
04:26 - 04:27
I think there's will give our candy 
我觉得他们会提供些糖果给你们
04:27 - 04:28
I'll try to do something better than that
我会试着提供些更好的东西
04:28 - 04:30
but don't you know I can't can't promise anything 
但我没法保证什么
04:30 - 04:33
right it's not gonna be a full four-course meal before the exam
考前准备的,也不是什么像样的四种吃的

04:34 - 04:39
what not to bring again , I think we talked about all the problems on the midterm everyone brought weird stuff in previous years 
别啥都带，我们在期中的时候说过那些事，前几年有人带进了奇怪的东西
04:40 - 04:44
two years ago again ,so when you brought the roommate, just to hang out ,don't do that okay 
就两年前，有人把室友领进来了，还只是闲逛，别干这事了，好吧


04:47 - 04:52
all right before we get into the the course material for that for the for the exam ,yes sorry go ahead
在我们开始了解考试的课程资料之前，  你先说



04:55 - 04:58
is that what pH means all right then yeah I did whatever that is yes 
PH么，哦哦 是的

04:59 - 05:02
hey where's Porter Hall oh that's the one over there yeah 
Porter Hall吗，那就是那边的人

05:02 - 05:04
if it's not gates, I don't know where it is right
如果不是那个门的，我就不知道对不对了

05:04 - 05:06
I'll be honest that this is my seventh year 



05:08 - 05:13
okay so right so the announcer some Piazza as well 

05:14 - 05:16
I need everyone to fill out the course evaluations 
我需要大家填写课程评估

05:16 - 05:18
I don't care whether you say I'm an awful person 
我不在乎你说我是一个可怕的人

05:18 - 05:21
I have bad hygiene or you hate the class, that actually is useful to me 
我不讲卫生或者你讨厌这个课，实际上对我有用

05:22 - 05:25
I you know I actually read these things 
实际上我读到过

05:25 - 05:31
so the things that I mentioned in getting feedback from or like , was the particular mark assignment you thought was unnecessary or stupid 

05:32 - 05:38
anything about the projects ,you've seen my announcement of Piazza , that we're looking for people to help with expanding busTub further 
关于project 的事，你已经看过我在Piazza 的公告，就是我们要招人来扩展BusTub项目。

05:38 - 05:41
you know fixing all the things and maybe went wrong this semester 
修复所有东西，可能是这个学期的错误

05:41 - 05:44
but if there's again something about the project that you thought was too hard 
但如果有你认为比较难的project 

05:44 - 05:45
the pacing was not right
寸步难行的那种
05:45 - 05:48
you wanted more documentation ,less documentation into that was too easy too hard 
你想要更多文档的话就比较容易了

05:48 - 05:51
again that feedback is actually super useful to me 
这种反馈对我来说很有用

05:51 - 05:56
all right so undergrads are awesome at at filling out course evaluations 
本科生在填写课程评估时就很好

05:56 - 05:57
like, if your stinks they'll tell you 
就像：如果你有体味，他们会告诉你

05:58 - 06:03
the master students you guys , you guys to click five five five five on everything ,Andy's the great professor 
研究生呐，评估里无脑选5，啊安迪贼牛逼

06:03 - 06:04
I don't want any that the right
我不想这样

06:04 - 06:08
just like I actually read this ,the university apparently reads it too 
我能读到，学院那也能读到

06:08 - 06:09
I don't care about them 
我并不在意

06:09 - 06:11
but I actually make the course better based on your feedback right 
基于你们的反馈我会更好的备课

06:12 - 06:16
one year a kids psychoanalyze me with a Myer Briggs test in the feedback， that was useful 
有一年，一个小伙子在反馈中用 Myer Briggs test 精神分析我，挺有用的

06:17 - 06:20
so again please go for the cell and I'll send a reminder on Piazza 



06:21 - 06:24
all right so I didn't have office hours on Monday 
周一我不在办公室

06:24 - 06:29
but I'll have office hours extra office hours this Friday at 3:30 in my office 
但 周五下午3:30 我会在办公室

06:29 - 06:34
and then I'll have my regular office hours at 1:30 on the day of the exam 
在考试当天下午 1:30我在

06:34 - 06:36
if you can't make either of these and you're dying to talk to me
如果你这2个时间都不行

06:36 - 06:38
please send me an email and I'll try to make arrangements
请给我发邮件，我会尝试安排

06:39 - 06:44
I may have to like do it over Skype or hangouts , because because of the thing the baby 
我可能需要用Skype 和hangouts ,因为有宝宝的事

06:45 - 06:50 
and then all the TAs will have the regular office hours up until and including September December 14th  
所有TA 都将在常规办公时间截止到9月14日，包括9月14日

06:51 - 06:55
right so the due date for the project 4 is on the 10th 
project  4的截止日期是 10号

06:55 - 06:57
they're gonna get for late days, so it'll go out into the 14th  
他们会晚几天。就是到14号

06:58 - 07:01
ok any questions about office hour stuff 
关于办公时间，还有问题吗


07:03 - 07:08
alright so this always comes up every year , what do you need to know from before the midterm 
每年都会提到的事：期中考之前，你想知道什么

07:08 - 07:13
so the exam is not cumulative meaning like, I'm not gonna ask you questions specifically about like buffer pools, right 
考试不会超纲的，比如我不会问你buffer pool 的问题

07:13 - 07:15
you know what how does this eviction policy work
像 buffer pool 的清理机制

07:15 - 07:18
but you obviously didn't know ,you know it's a part of a DBMS
但是你显然不知道,它是DBMS的一部分

07:18 - 07:22
we've covered through the full stack , you need to know how all these different pieces work together 
我们覆盖了全部堆栈，你需要知道所有的这些部分如何在一起工作

07:22 - 07:34
so the things that you have to know about from the previous prior to the term would be the buffer Pool management ,hash tables, B+trees ,storage models and then Inter-query parallelism, which is again running multiple queries at the same time 
你必须在这个术语前了解已下：buffer Pool management ,hash tables, B+trees ,storage models 和 Inter-query parallelism ，它同时运行多个查询

07:34 - 07:36
and you obviously didn't know how that works
而你还不知道那是怎么运行的

07:36 - 07:38
because you have to do transactions, I could be updating the database at the same time 
因为你要做事务，同时我要更新数据库

07:39 - 07:44
ok so this is clear , we're not asking you separate questions， like in the earlier homeworks before the midterm
我们不会单独问你问题，比如期中之前更早的家庭作业那样

07:45 - 07:49
but this is background knowledge that you just you know if you forgot this already you have other problems ok 
这些就是考试范围了，如果你忘了，那就是你的问题了



07:51 - 07:54
all right so the main thing we spent time talking about was about transactions
我们花时间谈论的主要是事务

07:55 - 07:59
right you should be aware of what the basic concept of ACID
你应该了解 ACID的基本概念

07:59 - 08:01
what the different properties that are combined in the acronym 
这些缩写里组合不同的属性

08:01 - 08:03
and what the data's meant is supposed to provide 
数据应该提供什么属性

08:04 - 08:11
then we did a we talked about the difference between conflicts serializability and view  serializability ,number of you serializability, nobody can actually do 
然后我们讲了 conflicts serializability 和 view  serializability的区别

08:11 - 08:14
so there's no way to actually check this, it's just a higher level concept 
没有方法来验证它，它是一个高级的概念

08:15 - 08:17
but for conflicts serializability you want to know how to check this
但是对于 conflicts serializability， 你想知道怎么验证这个

08:17 - 08:24
and how the database system can ensure that it generates a schedule ,for you know that's guaranteed we conflict serializability 
数据库怎样确保它生成计划，来保证我们的 conflict serializability 

08:26 - 08:29
what it means to have a recoverable schedule, right 
这意味着我们要有一个可恢复的调度

08:29 - 08:30
amazing basically means no cascading aborts 
就意味着没有级联终止

08:31 - 08:35
and then isolation levels and the anomalies

08:35 - 08:38
right ,dirty reads ,unrepeatable reads and phantoms
脏读、不可重复读、幻读


08:41 - 08:51
then we talk about concurrent protocols to actually generate schedules on the fly for arbitrary transactions, that are conflicts are reliable 

08:52 - 08:55
so we spent a whole class talking about two things locking 
我们花了一整堂课来讲这两件事

08:55 - 08:57
so you know what the basic protocol is 
你知道了基本协议是什么

08:57 - 09:00
but the difference between the non rigorous and rigorous one is 

09:00 - 09:03
what's the difference , what is rigorous two phase locking mean 
有什么区别，严格的两阶段锁是什么含义

09:06 - 09:07
in the back ,yes
来，你说

09:09 - 09:12
thanks Man ,yeah so rigorous is you've released all the locks at the end 

09:12 - 09:15
there is no shrinking phase ,regular two-phase locking 

09:15 - 09:19
so as you release one block then you're now in the shrinking phase ,and you can't acquire any new locks 

09:20 - 09:25
then we talked about multiple granite multiple mostly granularity in locking ,right 
然后我们讲到了 多粒度锁


09:25 - 09:27
and the big thing there was the intention locks
大部分都是 意向锁

09:27 - 09:36
right how do I notify or how do I post information about , what I'm gonna do at the lower levels of the lock hierarchy in the upper level 


09:36 - 09:37
so I don't have to take me locks on everything
我不用锁定所有

09:38 - 09:44
right if I have to take a lock on if I want a go and update a billion tuples , and my tables at billion tuples 
如果我想去更新一百万的 tuples ，以及所涉及到的表

09:44 - 09:49
I better up just taking a single lock on the table , rather than locking every single individual tuple 
那我最好一个锁加在表上好过加在每个tuple 上

09:50 - 09:55
and then important to know how you release these locks, like in what order is it top down or bottom up
然后重点是 怎么释放这些锁，比如 自上而下或自下而上



09:57 - 10:01
then we spent a lecture on talking about timestamp ordering concurrency control ,alright 
接着  我们讲了 timestamp ordering concurrency control 

10:02 - 10:06
so you know what the Thomas Write rule is the base of protocol that we talked about 
你知道 Thomas Write rule 我们提到协议的基础

10:06 - 10:08
then we spent time talking my optimistic concurrency control
然后花时间讲了 optimistic concurrency control

10:08 - 10:12
what are the three phases ,read phase, validation phase and write phase 
它有3个阶段 read phase, validation phase 和 write phase 

10:12 - 10:15
when do we actually acquire a timestamp for transaction in these different protocols
我们何时获取这些不同协议的事务时间戳

10:17 - 10:19
in the basic timestamp ordering what when you get a timestamp 
在 basic T/O 中何时获取时间戳

10:22 - 10:24
when the transaction starts 
何时开启事务，

10:24 - 10:26
in optimistic control, when you get a timestamp,
在  optimistic control 中何时获取时间戳

10:28 - 10:28 
what`s that 
你说啥？

10:29 - 10:31
when you validate yes when you finish the write the read phase 
当你验证时，当你完成写、读步骤时

10:32 - 10:34
then we talk about multi-version concurrency control 
然后我们讲了MVCC

10:34 - 10:36
I'm not gonna worry so much about the 


10:36 - 10:44
again the the concurrency control protocol of that you would use in MVCC, like either you know MV 2PL or MV OCC 
你在MVCC中使用并发控制，比如你知道的 MV 2PL或MV OCC

10:44 - 10:51
I'm more care about the Version storage / ordering of the Delta records are doing the pen only or you're doing the time travel tables
我更多关心的是 Version storage / ordering

10:51 - 10:52
and then how you want to do garbage collection
然后是你想怎样做GC



10:56 - 10:58
then we spend all the time talking about crash recovery 
接着我们讲到了 crash recovery 

10:59 - 10:59
question yes 
你说 



11:06 - 11:12
right so the isolation levels again it's uh it's it's sort of a the very top you have serializable isolation 
隔离级别最高的是 可序列化 级别

11:13 - 11:18
and then below that you have repeatable read ,below that you have read committed, and below that you have uncommitted read 
接着是 可重复读，然后是读已提交的，最后是读未提交的

11:19 - 11:21
and so basically as you go down that hierarchy
基本上就是这个层级结构

11:22 - 11:29
the the DBMS is not enforcing or protecting you from different entities certain kind of all not anomalies 
DBMS不会强制执行或者保护你免受不同实体的的异常

11:29 - 11:33
so if your Serializable isolation ,then you don't have phantoms 
如果你是可序列化 级别，你就没有幻读

11:33 - 11:35
you don't have dirty reads and you don't have unrepeatable reads 
也没有脏读和不可重复读

11:35 - 11:40
but then if you go down to repeatable read ,then you're not gonna do phantom checking 
但是你调到可重复读，你就不会做幻读检查

11:41 - 11:47
if you go down to read committed ,now you're not doing , now you're not doing on repeatable reads 
如果你调到读已提交的，你就不会做可重复读

11:48 - 11:52
and then read uncommitted read or read uncommitted is is no protections
读未提交没有保护

11:53 - 11:53
yes 

11:57 - 11:59
so question is it's snapshot isolation when we levels 
他的问题是 快照隔离级别是哪一层

11:59 - 12:01
snapshot isolation is a weird one
快照隔离级别是特殊的一个

12:01 - 12:09
it is it's almost a throg '''l to some answers no it's not in that main hierarchy 
答案是 它不在主流的隔离层级

12:10 - 12:12
if you take the advanced class we'll discuss a bit further 
如果你选了高级课程，我们将进一步讨论它

12:12 - 12:17
but basically in 1992 ,when they invented , when they came up the ANSI standard for these isolation levels 
基本上在1992年，当他们发明并提出这些隔离级别就成了ANSI标准

12:18 - 12:23
the guy that was supposed to double check the other guy, didn't double check that they missed snapshot isolation 
那些人应该是没有仔细检查，就落下了快照隔离

12:23 - 12:30
so there's an anomaly that can occur on our snapshot isolation , that the that cannot occur the other ones 
我们的快照隔离可能发生异常，不能发生另一个

12:30 - 12:33
and but the ANSI standard doesn't support it 
但是 ANSI标准不支持它

12:33 - 12:36
you don't the worry about that, just go with the ANSI standards 
你不用担心那个，只是ANSI标准

12:36 - 12:41
so serializable, repeatable read ,read committed, uncommitted reads, read uncommitted
所以只有 序列化、可重复读、读已提交、读未提交


12:45 - 12:52
yes snapshot isolation is a it's not a straight hierarchy the tree actually way more complicated 
快照隔离级别不在这个树形结构层次，实际上是更复杂的

12:52 - 12:55
but we'll cover that meet in the advanced class if you take that 
如果你感兴趣，我们会在高级版课程提到

12:55 - 12:57
the auntie ones are fine for the for the final exam 



12:58 - 13:00
alright so we talked about crash recovery 
我们来讲  crash recovery 

13:00 - 13:06
we talked about the different puffer Pool policies steal vs no-steal ,what a steal mean
我们讨论了 缓冲池策略 steal和no-steal的区别， 那这个steal是什么意思

13:11 - 13:11
yes 

13:16 - 13:24
So she said ,so with steal policies means that, a the DBMS was allowed to write dirty records or dirty pages out the disk from uncommitted transaction 
她说，steal策略就是 允许DBMS 把 未提交的事务的脏数据或脏页写入到磁盘

13:24 - 13:26
under no-steal you're not allowed to do that 
no-steal就是不能作这种事

13:27 - 13:31
and forces and no-force somebody other than Paulina
那forces 和no-force 区别呢？ 有人回答吗    Paulina

13:37 - 13:45
correct so she said you have to write all you have to write all dirty records out to disk, before transactions allowed to be saying that it's committed 
她说 在事务提交之前，你必须把所有肮数据写给磁盘

13:45 - 13:47
and then under no-force you don't have to do that
no-force就是不能作这种事

13:48 - 13:52
so with write-ahead logging is it using steal vs vs no-steal 
 write-ahead logging它使用了steal策略还是no-stral粗略？

13:54 - 13:57
just no-steal, no sorry it's already using steal ,wrong ,right 
no-steal策略吗，不它使用了 steal 策略，你答错了

13:57 - 14:05
because the with write-ahead logging I have to make sure that the log records , that correspond to the changes to the data the data pages 
因为 write-ahead logging 中我需要做日志记录来应对数据页的修改

14:05 - 14:08
the log records have to be written to disk ,before  my transactions a lot of commits 
日志记录必须在事务做提交之前写到磁盘

14:08 - 14:14
but I'm allowed to at some later point write out the the dirty records,  you know after the transactions already committed 
但是我允许在事务提交之后写入当数据

14:15 - 14:16
Which means that it's also no force 
哪一个是 no force 策略

14:19 - 14:21
we talked about different logging schemes
我们说了不同的logging schemes

14:21 - 14:24
the main distinction I care about is logical vs physical 
我关心的主要区别是逻辑日志还是物理日志

14:25 - 14:33
right physical is where you're actually writing out the the the low-level bits or bytes ,that got changed to the database system in your log records 
物理日志实际上是在底层写出  bits 或 bytes，来更改DBMS中的日志记录

14:34 - 14:37
and then logical log and you're just writing a high-level command that made the change 
逻辑日志，您只需编写高级命令来更改

14:37 - 14:39
so like the SQL query
就像SQL 语句

14:39 - 14:40
and so there's trade-offs these things
这些东西是有权衡的

14:40 - 14:47
right so if my query is gonna update a billion tuples , with logical logging all I have to have is that update query in a single log record 
如果我的语句要更新十亿个tuples ，那使用逻辑日志，我只需要在单个日志中更新更新

14:47 - 14:51
and that'll battle that's enough information for me to record what change that made
这些信息足够记下我的变化

14:51 - 14:57
and physical logging,  I have to have a billion log records that correspond all the change I made to every single tuple
而物理日志中，我必须有十亿条日志记录来对应我修改的每个tuple

14:59 - 15:01
then we talked about how do you checkpoints 
接下来说下检查点

15:01 - 15:06
so you should know about the sort of the difference between the fuzzy vs no-fuzzy right
你应该知道fuzzy 和no-fuzzy的区别

15:07 - 15:14
fuzzy means that I'm allowed to write out inconsistent data to the database system or to the disk , when I'm taking the checkpoints
fuzzy 的意思是，当我到检查点时，我可以将不一致的数据写入到数据库或磁盘

15:14 - 15:19
but I need to know what was going on at the time in my system , when I when I'm taking the check point
但是当我到检查点时，我需要知道我的系统中发生了什么

15:19 - 15:22
So that I can reconcile after recovery
我可以在恢复之后调和

15:22 - 15:28
what pages may or may not gotten written , during the checkpoint or may have been modified , while I was taking the checkpoint 
在检查点期间时，哪些页可能会被写入，哪些页可能不会写入
或者在我处理检查点的时候被修改过。

15:28 - 15:31
right in the in the non fuzzy checkpoint case 
no-fuzzy检查点那

15:31 - 15:35
they're basically stopping the world for a brief period while you write everything out 
在你把一切写出来的期间，它基本上让系统短时间内停一会

15:35 - 15:38
so that way you're guaranteed to have a consistent checkpoint 
这样你就可以保证有一个一致的检查点

15:39 - 15:42
and then we talked about ARIES recovery for a lecture right 
接下来我们用一节课的时间讲了 ARIES recovery

15:42 - 15:46
what are the three phases ,like the analyze ,the redo and the undo 
有三阶段：analyze 、redo 和undo 

15:46 - 15:48
so you know how far back in the log 

15:48 - 15:50
you need to look at for each of those phases potentially 

15:52 - 15:56
right it should know about the conversation log records ,when you write them, when you read them 
当你读或写的时候，它应该有会话级别日志记录

15:58 - 16:02
right if I have a compensation log record in my log ,and I apply it, but then I crash, before I you know finish my recovery ,when I come back around the second time ,do I need to make another CLR for that first one 
如果我的日志中有一个补偿日志记录，并且我应用了它，但是我崩溃了，在我完成恢复之前，当我第二次回来时，我是否需要为第一次创建另一个CLR


16:11 - 16:13
no right because you've already done it 
不用了，因为你已经做过了

16:13 - 16:16
the CLR has information to tell you how to how to undo the original update 
CLR有信息告诉你如何用undo原始更新



16:20 - 16:24
all right and then we briefly talked about distributed databases 
让后我们简单的说了分布式数据库

16:25 - 16:30
you know that we can't obviously go real deep into this ,beyond what was what was covered in homework 5
我们显然不能学的更深入，对于作业5优点超纲了

16:31 - 16:37
so this you know about the different system architectures we talked about, shared everything, share disk, shared memory, shared nothing
我们讨论了不同的 system architectures ，共享一切，共享磁盘，共享内存，什么都不共享

16:37 - 16:40 
what are the trade-offs for these or when would one be better than another 
这些都有什么取舍，或者啥时候一个比另一个好

16:41 - 16:45
how did we actually gonna do replication and in these different environments at the distributed environment 
我们实际上是如何在分布式环境中的这些不同环境中进行复制的

16:45 - 16:49
how do we make sure that the the database system is fault tolerant 
我们怎样确保DBMS的容错

16:49 - 16:53
how to make sure that database system，if we're making updates is consistent ,across all the copies of the data 
如果我们刚好在更新的时候，如何确保数据库系统中数据在所有副本中保持一致


16:54 - 16:56
how we do partitioning 
我们怎样分区

16:56 - 17:01
again high level things you know we talk about hash partitioning, what are the benefits of them how to actually find the data you need 
在高层级上我们讨论hash partitioning，它的优势是什么，如何找到所需数据

17:02 - 17:03
and then two-phase commit 
然后是两阶段提交

17:03 - 17:09
you know under what circumstances, you know at what steps would you do at different phases of the protocol 
在什么情况下，在协议的不同阶段你会采取什么步骤

17:10 - 17:14
don't worry about Paxos , that's too hard for for final exam in databases 
别担心这个，这对数据库期末考试来说太难了

17:16 - 17:16
okay 

17:18 - 17:19
so any questions about the fire 
这还有什么问题么

17:19 - 17:23
to sweeten the deal, I forgot to announce this earlier 

17:23 - 17:27
if you take the final exam , when you turn it in, I will give you a busTub sticker 
如果你参加期末考试，当你交卷的时候，我会你个busTub 贴纸

17:27 - 17:29
you can put on your laptop okay 
你就可以贴在你的笔记本上

17:29 - 17:31
we have enough for everyone
我们有足够的量给每个人

17:32 - 17:32
yes 

17:36 - 17:40
this question is what is the duration of the exam, it will be the same as the midterm 
这个问题是 考试考多长时间？它和期中考试一样

17:40 - 17:42
but you have three hours
但是你有3个小时

17:42 - 17:44
and there's always somebody who takes three hours 
总有人要3个小时的

17:46 - 17:48 - 17:52 
yeah yes yes 

17:55 - 17:57
yes who questions question is will they be present yes

17:57 - 18:03
when I post the review guide, I will post a same way I did in the midterm I post the practical exam, yes
我会发布复习指南，和其中考试一样，发布实践考试

18:05 - 18:08
these will be my own choice yes ,it's easier to grade 
这些将是我自己的选择，评分更容易

18:10 - 18:10
yes 


18:18 - 18:26
all right ,so this question is I made the distinction between force vs no-force or steel vs no-steel , would there be a case where you would have won like, won't you won't want to use one 
这个问题是，我区分了 force 和 no-force 策略，steel 和no-steel 策略 。。。。。

18:28 - 18:34
so no I think you have to be one you have to be sort of like steel no-force or no-steel force ,right 
不对，我想你想要排组合 比如  steel no-force 或者 no-steel force

18:34 - 18:35
we talked about shadow paging
我们讲下分页

18:35 - 18:39
shadow paging was an example of no-steel force 
分页是一个no-steel force 的例子

18:39 - 18:43
because I wasn't allowed to over write dirty pages from uncommitted transactions 

18:44 - 18:48
because I had this shadow copy on the side I was making all my updates there 

18:48 - 18:51
right so that's that's that's the no-steal part 

18:51 - 18:55
and then the fourth part is when my transaction went to kokum it with shadow paging 

18:56 - 18:58
I had to make sure all those dirty pages were flush to disk 

18:58 - 19:08
and then I flipped the pointer the root pointer to now , you know point to the the old the new the shadow becomes the new master ,when I do that I make sure I have to make sure everything's all have already flushed 

19:08 - 19:14
so again write-ahead logging is steel no-force, shadow paging is no-steel force

19:16 - 19:26
and then the the the main takeaway was that the read-ahead logging it is ,almost always I can't think of maybe some cases I haven't thought of it , it's almost always better 

19:27 - 19:30
and it's what every system uses ,most systems

19:35 - 19:35
any other questions

19:38 - 19:38
yes 


19:41 - 19:46
yes so this question is unlike the midterm do I not need a calculator I forgot to highlight that yes 

19:46 - 19:47
you do not you don't need it 

19:48 - 19:52
I can think of what we did in the homeworks ,we didn't estimate joins we didn't 

19:54 - 19:57
yeah I don't think it like it live is that 





19:58 - 20:01
there we no questions on like query optimization stuff 

20:01 - 20:05
that's the only thing I can think of may need a calculator,right say don't worry about that 

20:08 - 20:09
you know the questions

20:10 - 20:10
yes 

20:18 - 20:21
like two phase locking stuff or about the hierarchy 

20:24 - 20:29
yes sir so her question is are there more questions about like multi-granular granularity locking 

20:30 - 20:36
so so in in the in the textbook all the odd problems the solutions are online 

20:37 - 20:39
I haven't looked at I haven't looked at the newer version

20:39 - 20:40
but like there might be some questions are there you can look at 

20:41 - 20:47
and then we do the odd ones and you go to the DB, like DB the links on the web on the course websites are DBDB.com 

20:47 - 20:50
they'll have the solutions to our problems, you can just follow those 

20:51 - 20:55
I'll put a link on the the final review web page yes 

20:58 - 21:03
especially can you bring a notes on midterm ,one cheap ,one cheap 

21:04 - 21:05
copy whatever you need 

21:09 - 21:09
okay


21:13 - 21:18
all right so this is my one this this is pie my favorite one of the favorite lectures, well they're all good ,but 

21:19 - 21:21
cuz he talked about more databases 

21:21 - 21:27
alright so again I asked everyone in the class to vote on what systems they were most interested in learning about 

21:27 - 21:32
so here's the tally from last year, for the top 10 CockroachDB Spanner and MongoDB 


21:32 - 21:35
and here's what we ended up with this year 

21:35 - 21:40 
which is very surprising scuba came up first , followed by MongoDB ,fought by CockroachDB 

21:40 - 21:46
and surprisingly this is my I think fifth or sixth year teaching like this puppy thing

21:46 - 21:48
spanners always been in the top three, right 

21:49 - 21:53
and so the only thing I could think of that people why people didn't vote for this as much is

21:53 - 21:55
because the name got changed a cloud spanner 

21:56 - 21:58
right and maybe people didn't think it was the same thing 

21:58 - 21:59
so that's okay

21:59 - 22:03
but scuba is a really interesting system, so we'll start that for that first okay


22:04 - 22:06
all right so Facebook Scuba 

22:06 - 22:09
now so I also noticed that people vote for the ones or has like 

22:09 - 22:14
if has a name of a company in front of it like, Amazon, Aurora, Baidu, OceanBase, then people vote for those things more 

22:15 - 22:18
so all right all right Facebook Scuba 


22:18 - 22:24
so what the you know what we're trying to do here in the system potpourri

22:24 - 22:27
It`s to show you that we can now look at real world systems 

22:28 - 22:35
and start using the vernacular that we've discussed all through it an entire semester , to sort of start to understand ,what this thing is actually doing 

22:36 - 22:40
right so now I could say this things out to shared nothing to shoot the system and you know what that means 

22:40 - 22:42
you know what the implications of that are 

22:42 - 22:46
you know what the performance characteristic performance challenges you would have in a system like this 

22:47 - 22:54
so scuba is an internal database system ,that Facebook isn't working on for several years now , was first announced in vldb in 2013 

22:55 - 22:57
and they've been still working on it 

22:57 - 22:59
it's it's not open source ,right 

22:59 - 23:05 
there's it's only till very recently, is there now some public information about the newer version 

23:05 - 23:12
and actually turns out, because the guy that that's actually leading the project is CMU database alum, like he got his PhD here before I showed up 

23:12 - 23:17
and he's now running that whole the the running the the development of this database system 

23:17 - 23:20
and then his boss is actually a 

23:20 - 23:27
his boss's is the mother of another CS student here in the department, who actually worked on busTub over the summer 

23:28 - 23:30
all right so it's all to all one giant CMU family

23:30 - 23:34
all right so scuba is a it's a it's MIT 

23:34 - 23:41
it's designed for having low latency queries and ingestion of, internal metric data generated from from Facebook's different services 

23:42 - 23:44
all right so this is not running an OLTP to the application 

23:44 - 23:47
this is not running ,it's not a good joins warehouse

23:48 - 23:52 
think of like every single time you click something on Facebook,right on the website

23:52 - 23:55
that causes a bunch of functions are going to vote on the servers 

23:55 - 24:01
and you can they can have their developers instrument those function calls to keep track of the performance metrics

24:02 - 24:03
right throughout the entire stack 

24:04 - 24:06
and then all that data then gets shoved over to scuba 

24:06 - 24:10
now they can use that and then run queries on that data to try to figure out

24:10 - 24:13
you know why does this function take you know run it slower

24:13 - 24:16
what are some problems I'm seeing and my giant fleet 

24:16 - 24:20
so the newer version of scuba is now a column store 

24:20 - 24:23
it's a distributed/shared-nothing system 

24:23 - 24:28
it's using a tiered-storage, just means that this just means that like you can have a

24:28 - 24:33
you know you can have a in-memory cache, flash cache, and then may be slower disk below that 

24:34 - 24:38
and then it's gonna be using a heterogeneous hierarchical distributed architecture 

24:38 - 24:43
and so one interesting about the system as well is that since they are trying to have this thing be really fast 

24:44 - 24:46
but you want to run your queries very quickly over a lot of data

24:47 - 24:49
they are not a lot that can support SQL

24:49 - 24:50
they're not going to support joins

24:50 - 24:52
they're not going to support global sorting 

24:53 - 24:55
so you can only write queries that access one single table 

24:55 - 25:01
and you have a where clause that do you know yes simple filtering, and then you can aggregate aggregations 

25:02 - 25:06
another interesting about it, that's gonna be different than everything we talked about before is that 

25:06 - 25:11
they're gonna have to have replication to have or done a deployment of an entire Scuba cluster

25:12 - 25:16
so you would think about like you have a bunch of machines, I send all my data to this this this cluster 

25:17 - 25:20
but I'm also going to send it to other clusters running in different data centers or different regions 

25:21 - 25:25
but they're gonna allow for lossy fault-tolerance in this environment 

25:25 - 25:31
because the data that they're collecting , it's valuable would not like bank account invaluable 

25:31 - 25:35
right so like say you know you go click on something in your time on a Facebook

25:35 - 25:37
that generates a bunch of performance metric data 

25:37 - 25:40
if that data gets lost yeah right 

25:40 - 25:43
it's not that not the end of the world , you know actually I want to lose everything 

25:44 - 25:49
but they're an allow they're gonna tolerate queries to end a you know may have false negatives false positives 

25:49 - 25:52
Because they're gonna end up missing data that just end up again getting missing 

25:52 - 25:56
and so the way they're gonna try to avoid that is to running multiple points

25:56 - 25:58
they'll run the query in different regions at the same time 

25:59 - 26:07
and then when they get back the result they see , which query actually read them , had the the fewer number the fewest number of missing data 

26:07 - 26:09
and then they use that as the CREP result 

26:10 - 26:12
but if they lose some data it's not a big deal 

26:12 - 26:19
and actually going to be they're gonna have retention policy, where you can say any data stored in this table, after after you know seven days just thrown away 

26:19 - 26:20
And who cares 


26:22 - 26:27
all right so here's here's the high-level pipeline of what they're trying to do 

26:27 - 26:29
so you have your different application servers

26:29 - 26:31
right these are all running the the 

26:32 - 26:35
you know the running the website, running all the back-end stuff you need to support the website 


26:36 - 26:39
so these guys can be generating structured debug logs 

26:39 - 26:44
so think of this as like a JSON, document, that the application server spits out to say

26:44 - 26:47
you know here's how much time I spent in the CPU for this function in that function 

26:48 - 26:52
so then they're gonna load this into this internal tool that they develop called scribe 

26:53 - 26:55
I think of this it's sort of like a Kafka kind of thing 

26:55 - 26:57
we have a bunch of log records coming in 

26:57 - 27:00
and then you can have the pub sub system to say 

27:00 - 27:02
here's how to categorize the data I've collected

27:02 - 27:06
and here's in other systems get notified when new information arrives to it 

27:06 - 27:10
so this is an older thing that since over ten years old 

27:10 - 27:12
there's an open-source version that they have on github

27:12 - 27:13
but that was like abandon a decade ago 

27:14 - 27:18
so that supposedly the the internal version it's much better than what's on online now 


27:19 - 27:25
all right so the scribe is now gonna take the structured logs look at some tag to say 

27:25 - 27:30
you know it's it's generated for you know this particular application type or this service 

27:31 - 27:34
and it's gonna combine them and get together based on that category 

27:34 - 27:38
and then it's gonna send them to this streaming platform or they call it the tailor service 

27:38 - 27:40
because they're just tailing the log 

27:40 - 27:44
and this is just gonna bash together a bunch of these log records they've gotten from scribe 


27:44 - 27:47
and then when they have a large enough batch 

27:47 - 27:50
they're gonna convert that into a columnar data file 

27:50 - 27:53
think of like park' or orc that we talked about last week

27:53 - 27:58
all right this thing's like a standalone file, almost like a CSV ,but it's actually a binary column store 

27:59 - 28:04
and so if they have a larger batch ,they're gonna generate these these column store files


28:04 - 28:08
and then now they're gonna feed this into the leaf nodes in scuba 

28:08 - 28:11
and so we're talking to 16 aggregation notes and leaf nodes 

28:11 - 28:16
but this is basically the storage nodes think of this as like the the shared disk architecture 

28:16 - 28:19
but you can actually have you know run queries down here

28:19 - 28:22
or the the last class when Shasank talked about Oracle Exadata 

28:22 - 28:28
but they had the storage nodes at the bottom could actually do filtering and predicate evaluation down there, same same idea here 

28:29 - 28:31
so now one additional thing they're going to do 


28:32 - 28:39
is that they're gonna each of these leaf nodes are going to update this validation service, with information about the number of tuples 

28:39 - 28:40
that they've inserted for each table 

28:42 - 28:45
and we'll see it in a second, this is how they're gonna determine 

28:45 - 28:50
which which when they run the query on multiple deployments or multiple clusters 

28:50 - 28:53
you'll check that thing to say well how much data is actually missing 

28:54 - 28:55
so I know I sorted a million tuples 

28:56 - 28:59
but I only read maybe 500,000, so half of my data went missing

28:59 - 29:03
again they're not they're not gonna go freak out, that's okay, in their environment 

29:03 - 29:07
but just they keep track of this ,and they know that which query is producing the most accurate result 


29:08 - 29:10
so now it's again it's a SQL system 

29:10 - 29:17
so they have a vistas SQL interface or these dashboards, that people can use internally at Facebook 


29:17 - 29:19
that sends SQL queries to this execution layer 

29:19 - 29:21
which then is gonna send it to these aggregators

29:21 - 29:24
who then are gonna farm it down to these leaf nodes 

29:24 - 29:26
so I'll discuss this hierarchy and a few more slides 

29:27 - 29:30
but this is an important distinction between all the distributed databases we talked about 

29:31 - 29:33
because it's a heterogeneous environment 

29:33 - 29:38
so the leaf nodes are doing things that are separate or different than what the aggregator notes are doing

29:38 - 29:41
right and you obviously have more of these ,because you're have more data 

29:41 -  29:41
yes

29:44 - 29:46
the question is the categories name is accurate yes 

29:46 - 29:54
think of like it's some internal tag , that that Facebook is ascribing to a particular class of log records 

29:54 - 30:00
right so again say it's like I don't know the facebook inbox or messenger right 

30:00 - 30:02
so you'd say that would be one category 

30:02 - 30:06
so all the the log records from the messenger app go to get combined together 


30:09 - 30:14
okay so again all right it's a header juice architecture

30:14 - 30:16
and we have leaf nodes , aggregator nodes

30:16 - 30:19
so the leaf nodes are going to store the column our data 

30:19 - 30:25
that we're getting out from the the record batchers in a columnar format 

30:26 - 30:31
and the for each query every query is gonna go to every single leaf node 

30:31 - 30:35
so the may not be data that the query needs on that leaf node 

30:35 - 30:44
but they're not going to store any additional metadata or maintain any indexes on these leaf nodes ,to give me it'll figure out whether I need to touch data at them 

30:44 - 30:49
so they're trying to make this thing be as fast as possible to trying to make this both in terms of how fast the query can execute 

30:49 - 30:51
but also how fast you can ingest new data 

30:52 - 30:58
so if I don't have to maintain any metadata or catalog information about which data is that what leaf node 

30:59 - 31:01
then I can ingest new data very quickly 

31:01 - 31:04
and then every query shows up and just scans everything 

31:04 - 31:08
and then that determines whether the you know you have data that you actually need 

31:10 - 31:11
that makes sense right

31:11 - 31:13
so like when we talk about partitioning before 

31:13 - 31:16
we talk about how there's and we'll see this a Mongo and cockroach

31:16 - 31:19
there to maintain this this state table that says 

31:19 - 31:23
if you want data within this range or this hash value go to these nodes 

31:23 - 31:24
they're not gonna have any of that

31:25 - 31:27
they just blast everything to everything every query goes everywhere 

31:29 - 31:31
so the aggregator nodes when they get a query 

31:31 - 31:35
they're going to dispatch plan fragments to the leaf it's shown in leaf nodes 

31:35 - 31:36
there Dan gonna do on the leaf nodes

31:36 - 31:40
they're gonna do the scan and some some basic computation 

31:40 - 31:41
but I'm gonna send the result up to the aggregator nodes 

31:42 - 31:47
we're then going to combine the results from multiple leaf nodes to produce a single result 

31:47 - 31:50
and then send that up to a root node who combines combines the final result 

31:51 - 31:54
so this is an allow them to scale out the system very easily 

31:54 - 31:58
because I know if my aggregator are running slow I can add those nodes 

31:58 - 31:59
because they don't have any state 

31:59 - 32:01
right that's not where the data is actually stored 

32:01 - 32:05
or if I need to scale out my leaf nodes, I just add more of those, and keep my a Grenadiers ,but the same 

32:05 - 32:12
but it's a very interesting architecture that shows up in a lot of other Facebook system design 

32:12 - 32:15
the MemSQL guys when they before they started MemSQL 

32:15 - 32:17
they spend their time at Facebook , and they saw this kind of architecture 

32:17 - 32:23
and then they sort of copied it or inspired by it, when they went off MemSQL

32:23 - 32:24
so MenSQL works the same way 

32:26 - 32:32
okay so again we talked about this this tolerating lost or missing data 

32:32 - 32:37
so if a leaf node either has no data or that it's down 

32:37 - 32:41
and it can't produce any results within a time out then they just ignore it and that's okay 

32:42 - 32:46
and they use the validation service to figure out well how much data was this thing actually have 

32:46 - 32:48
and that'll determine the quality of the query a query result 


32:49 - 32:51
so let's see what the end the full pipeline looks like 

32:52 - 32:55
so again this is this is be considered one scoop of cluster 

32:55 - 33:01
they're gonna they're gonna have multiple instances of the same kind of cluster running in, different data centers, different regions

33:01 - 33:03
and they're gonna vote a query on all than at the same time

33:03 - 33:06
and they all get back the same result, and you pick out which one is the best one 


33:07 - 33:13
so my query shows up I want to do a an aggregation of the number of vents , that crashed on there was a crash on a Monday 

33:14 - 33:16
again so I can only do single table queries

33:16 - 33:19
I can do aggregations, I can do group bys, I can do filtering under scans 

33:20 - 33:23
but I can't do global global sorting on I can do joins


33:23 - 33:27
so the routes gonna get the query ,and that's going to break it up to query plan fragments

33:27 - 33:29
that's can then distribute down to the aggregators 

33:29 - 33:33
and the aggregators are gonna tribute them down to their delete nodes 

33:33 - 33:36
all the leaf nodes are now going to do a scan and then send results back up 


33:37 - 33:41
but let's say while we're executing this query this node goes down which happens ,right

33:41 - 33:43
if you have a large large cluster 


33:44 - 33:49
so everybody else I'm still wrong, everybody else is still gonna be able to do the computation for this query 


33:50 - 33:55
and then they send the results back up to the aggregators prepend just can't combine it together, right 

33:55 - 33:58
so I'm doing accounts I want to know the number of events that occurred 

33:58 - 34:01
so this guy says has I have 10 events, he has 20 events 

34:01 - 34:05
so this guy just takes 10 plus 20 and produces 30 ,and then sends it up to the root 


34:06 - 34:09
and the root does the same thing just adds all the results at different aggregators

34:09 - 34:11
and then produces the final result 

34:13 - 34:13
so this is clear 

34:17 - 34:28
yes yes 

34:33 - 34:35
so this query is to scan everything right 

34:35 - 34:39
so there is no again, there's no metadata
 
34:39 - 34:45
there's no catalog to tell me anything about whether or not these leaf nodes had the data in my that I have my where clause

34:46 - 34:47
so I don't know ,but I don't know where type is 

34:47 - 34:49
I don't know where time is, so I send it everyone 

34:49 - 34:52
so everyone essentially he's gonna in this example here 

34:52 - 34:55
everyone's gonna do the exact same query run it on the leaf nodes 

34:56 - 35:00
alright so he's doing a count star work type II type equals crash time ,he goes Monday, he has 10 

35:00 - 35:03
he just shoves that up, and this guy knows well I'm doing this count thing 

35:04 - 35:07
so I just added some up numbers together please the result 

35:12 - 35:12
yes

35:18 - 35:21
So question ah is it true that for every single query  

35:21 - 35:25
you would always copy the same query down to the leaf nodes 

35:25 - 35:29
no because like for averages the way you do that is account on the sum

35:29 - 35:31
and then you can put the average up at the top 

35:31 - 35:33
so that that would be different ,yeah 

35:37 - 35:41
so there really isn't like since there's no joins 

35:41 - 35:43
the cook there's not really a query optimizer here 

35:43 - 35:48
it's just I need to know what the I just convert the SQL to a query plan and to shut that down

35:48 - 35:51
and there's some basic heuristic they probably do to figure out like 

35:51 - 35:54
you know how to actually break it up and send it down leaf node ,yes 

36:03 - 36:06
so this question is why was it why was it decided to have a root node here 

36:06 - 36:12
and how all the clients go to this versus having everyone can penally go to any aggregator 

36:13 - 36:14
and then any aggregator can talk to anybody else

36:15 - 36:23
so the my understanding is that there's actually a there's a layer above this as well

36:23 - 36:25
the root were they're doing a mission control 

36:26 - 36:32
so they can do some things like, oh if a query if a node is sending too many queries 

36:32 - 36:35
a client sending too many queries ,maybe want to throttle them or like

36:35 - 36:39
if I know I'm getting bad data from some some set of these nodes 

36:39 - 36:44
then I can have someone to make a decision about what to exclude or not include them in my query execution

36:44 - 36:50
so it's allows them to have a single location to have a global view of what's going on in the cluster 

36:51 - 36:52
doesn't know about what's in the database right 

36:52 - 36:56
because that's that would be to extensive maintain and keep keep it fresh all the time

36:56 - 36:59
it just knows that like these nodes are performing and well they're not well 

37:01 - 37:01
yes 

37:13 - 37:20
his question is say that this node had a ton of results ton of that would match my predicate

37:21 - 37:24
I wouldn't be better for me just abort this query 

37:24 - 37:27
because that because I'm my counts me way off

37:27 - 37:31
how do you know that this node has all the date has a lot of data that matters for your clearing 

37:33 - 37:34
You don't know yeah 

37:37 - 37:40
so again think about what this is design for

37:40 - 37:43
this is design for logs being generated by machines right 

37:43 - 37:45
it's not like hey here's my bank account 

37:45 - 37:47
I want that to be you know you know to the penny 

37:48 - 37:54
so if you get loosey-goosey results that still was probably okay 

37:55 - 37:59
and then the way they sort of overcome that again is by having that validation service

37:59 - 38:01
they can determine, how much data did they actually not read

38:02 - 38:07
so then also again then they have a bunch of a Dunant copies of the system running at the same time 

38:08 - 38:09
they're all gonna produce the same query 

38:09 - 38:13
so the likelihood that every single every single cluster 

38:13 - 38:15
that's running Scuba for a single query is gonna have this node fail 

38:16 - 38:19
exactly you know exactly this node has this data fail is pretty low 

38:20 - 38:24
so at least one of those clusters will have a more active result , and that's the one they'll use 

38:26 - 38:28
that's way different we talk about entire semester 

38:28 - 38:29
that's why I like the system 

38:29 - 38:31
because it's like we talked about never losing data 

38:32 - 38:33
now I'm saying it's okay to lose data 

38:34 - 38:37
because and it's like if the tree falls in the woods and no one's around to hear it who cares 

38:37 - 38:42
if this node goes down and no you know no one knows and it cares about what data is actually on it, does it matter 

38:44 - 38:51
and because they're not in ,you know running wrap or patos every single time , or they're updating data, this thing can run really fast 


38:54 - 38:57
all right so this is summarizes our whatever I already said before right 

38:57 - 39:04
so for every scuba deployment, there's me multiple ,there's a really multiple scuba deployments running in their regions 

39:05 - 39:07
we run the query at the same time on all the regions 

39:08 - 39:09
and then they come back with a result

39:09 - 39:14
and then the result is annotated with information from the validation service that says

39:14 - 39:17
how many how much data did I end up not reading 

39:17 - 39:22
right picked the one that has the that has the most the read the most data okay

39:26 - 39:34
again so we know the guy Stavros that runs us he got his PhD from CMU in 2007-2008 

39:34 - 39:37
and now he's in charge of us so any questions 

39:39 - 39:40
all right good step


39:41 - 39:45
all right number two was MongoDB ,MongoDB is always always picked every year 

39:46 - 39:49
so let me ask you guys I asked this every year ,why did you guys pick Mongo 

39:51 - 39:54
I assumed you picked Facebook cause some of you want to get jobs at Facebook 

39:54 - 39:57
because you think like I go in the job interview I could talk intelligently about scuba

39:58 - 40:01
same thing for MongoDB you want to work at MongoDB or do you want to use MongoDB 




40:03 - 40:07
some people already done internships, some people are going to new internships there ,yes 

40:09 - 40:11
he says it's much different than what we normally do ok 

40:12 - 40:14
so I've known the MongoDB guys for a long time

40:14 - 40:21
the one of the co-founders actually went to brown, before I did 

40:22 - 40:30
he so he like we've known Elliot since like 2009 ,when he first started assessment can you come give talks at,this year's Sparta they're all the time 


40:30 - 40:32
and I actually was there in August 

40:32 - 40:34
they have a new office building 

40:34 - 40:37
they used to be right around the corner from Times Square which is awful 

40:37 - 40:39
they're still in Midtown but they have a nice office building 

40:40 - 40:45
so you know they're a big deal if you have the sign for your company outside the building, right 

40:45 - 40:47
Above that says Warner Music Group, they have the money 

40:47 - 40:49
Mongo is doing pretty well it's all for the databases 


40:49 - 40:53
so this is their Lobby ,the lobbies nice looks familiar, right yeah 


40:53 - 40:54
um the view is amazing 

40:55 - 40:59
so this is from the kitchen, I mean it was a cloudy day, the view is absolutely stellar uh 

41:00 - 41:02
and again all paid for by databases it's amazing 


41:03 - 41:08
all right so what is Mongo, Mongo is a distributed document model DBMS

41:09 - 41:14
so when I say document think of JSON like a JSON object 

41:15 - 41:21
and in the MongoDB world the language for when we talked about the entire semester, has been slightly different 

41:21 - 41:26
so again instead of saying tuple to say document instead of saying table, they or a relation they say collection 

41:26 - 41:28
but the high-level concepts are still the same 

41:29 - 41:33
so it's one of the earliest of our original NoSQL systems 

41:34 - 41:35
it was open source

41:35 - 41:341
it used to be GPL ,but now they switch to the server side license , and this is basically to protect them from Amazon 

41:41 - 41:42
right MongoDB got so popular 

41:42 - 41:45
they were worried about Amazon coming along 

41:45 - 41:48
and just running MongoDB in a hosted environment 

41:48 - 41:50 
and selling it for cheaper than what MongoDB could 

41:50 - 41:51
so they switched their license 

41:52 - 41:57
now Amazon did come out with a system that does clone the mummy protocol called document DB 

41:58 - 42:01
but my understanding of how it works is that underneath the coverage is just Postgres 

42:01 - 42:06
so the wire protocol looks and smells like looks like looks like MongoDB 

42:06 - 42:09
but underneath it is just Postgres 

42:09 - 42:16
so it's going to very centralized shared nothing architecture with a heterogeneous configuration 

42:17 - 42:20
and originally they you know as this NoSQL system 

42:20 - 42:23
they didn't do transactions and they need new joins in a NewSQL 

42:23 - 42:27
so right now the latest versions the Mongo ,that will they do transactions 

42:27 - 42:28
they brought in transactions ,and they also do joins 

42:29 - 42:31
the only thing they haven't brought in is SQL 

42:32 - 42:39
now there are some hacky tools that can convert SQL into MongoDB queries 

42:39 - 42:43
as far as I know I've never run across anybody that runs this and those things in production alright 

42:44 - 42:47
so they're MongoDB have their own API which I'll show in a second 

42:47 - 42:50
that is basically you write JSON queries to read JSON data 


42:52 - 42:59
so one important concept about the document data model, which is quitly different that we talked about the entire semester,is this concept of denormalization 

43:00 - 43:06
so in the relational model ,we would define our tables or define our relations

43:06 - 43:10
and we would use foreign keys to say there's a reference from this table to another table 

43:10 - 43:14
right so if gain if I'm by modeling Amazon is my store information 


43:15 - 43:17
so I have customers ,customers orders ,and orders have order items 

43:17 - 43:21
so in a relational database system, I would define these as separate relations 

43:21 - 43:25
and then if I want to say for a given customer ,give me all the items that they bought 

43:25 - 43:28
I would have to do a three-way join between these three tables 

43:29 - 43:35
and the NoSQL guys argue that that would that was expensive to do, to do these joins 

43:35 - 43:38
you know because it's now you got to run your hash join a nested loop join right 

43:39 - 43:44
so what they would argue you'd want to do in a document data model is to denormalize

43:44 - 43:46
what basically says combined together 

43:46 - 43:53
the the related information about a single entity in your application into single JSON document 


43:54 - 43:57
alright so setup in this getting this environment here, instead of having a three different tables 

43:58 - 43:59
you would have one table called customer 


44:00 - 44:03
and then inside each customer record you would embed their orders 
 
44:04 - 44:07
and then which eaten within each order you would embed their order items 

44:07 - 44:11
so now if I want to go get all the order items that Andy bought

44:11 - 44:14
I go get my customer record from the database system 

44:14 - 44:18
and then I just reversed now inside the JSON document to get what I want 

44:18 - 44:21
now MongoDB did not invent that idea 

44:21 - 44:26
that's an old idea from the 70s ,XML databases did it in the early 2000s late 1990s 

44:26 - 44:29
the object-oriented guys did this in the 1980s 

44:29 - 44:31
so MongoDB did not invent that 

44:31 - 44:37
what MongoDB was sort of famous for was having a you know fast distributed JSON database system

44:37 - 44:40
right when sort of web bill and and JavaScript was becoming prominent 


44:41 - 44:48
so again we just look like this and your your JSON document would have inside of it and array called orders 

44:48 - 44:51
and with inside that you have additional JSON documents for every single order 

44:51 - 44:55
and decide every single order you have an array for order items , and then all the items that they bought 

44:56 - 45:00
right so again we've talked to us and they're actually the first class 

45:00 - 45:03
some a performance standpoint this is we really fast

45:03 - 45:07
again it's one read to go get you know all Andy's order items 

45:08 - 45:14
the bad thing is going to be obviously, now we're gonna duplicate a bunch of information about these different order items over and over again 

45:15 - 45:19
and now I need to write my application code to make sure that everything is is is 

45:19 - 45:23
the I maintain the integrity of all of this information 

45:23 - 45:24
so if I change the name of item

45:24 - 45:28
I have to write code to go through all my inside all my customer records and make sure I update everything 


45:30 - 45:35
so the way you panic reacts to you queries is through a JSON-only query API 

45:38 - 45:41
they don't have a query optimizer 

45:42 - 45:45
at least not in a call space one that we talked about before 

45:45 - 45:46
and I think a briefly talked about us when we talk about query optimization 

45:47 -45:49
so they basically do is do your query shows up

45:50 - 45:55
and they're gonna generate sort of every possible combination for that given query 

45:55 - 45:59
and then there's gonna blast every node with different different combinations of that query 

46:00 - 46:02
and then whatever one comes back first 

46:02 - 46:05
then they learn that that's the better one to use because it came back more quickly

46:05 - 46:08
so the next time you actually that same query or similar query 

46:08 - 46:11
they'll just reuse the query plan that they generated before 

46:12 - 46:14
and they'll do this maybe like a thousand times 

46:14 - 46:16
and then just after the thousand query invocation

46:16 - 46:22
they'll do that sort of blast them all out first or blast them all out ,and see which one comes back first thing 

46:23 - 46:24
so you may think is kind of hacky right 

46:24 - 46:28
I spent two lectures talk about ,how hard it is to do a query cost-based scoring optimization 

46:28 - 46:31
and and how hard the problem at you be

46:32 - 46:35
and they chose this cuz back in the day they didn't do joint 

46:35 - 46:38
so you didn't have to worry about joint owner ,he's just basically was picking what index to use

46:38 - 46:41
so the random walk approach would actually work 

46:42 - 46:45
I don't think they do anything sophisticated for the joins now 

46:45 - 46:49
they might just use basic- heuristic, this table or this collection is smaller than this one .

46:49 - 46:51
so it's one versus the aunt once the inner versus 

46:52 - 46:55
the outer they support JavaScript UDFs, we'd actually didn't talk about UDFs for this semester

46:56 - 47:01
it's basically like a function you can write in your query that gets invoked on the data server-side 

47:02 - 47:07
they do now support joins ,and they now support also multi-document transactions 

47:08 - 47:11
though the early benchmark numbers for MongoDB were amazing 

47:11 - 47:12
like you could write data into it very quickly 

47:12 - 47:15
because what they were doing is one they weren't doing transactions 

47:16 - 47:22
and they weren't actually guaranteeing anything was actually written to disk, when you got response back, that you're right succeeded

47:23 -47:27
was actually even worse of that, if the database ever got the packet for your right 

47:27 - 47:30
they'd immediately come back when to acknowledge and say yeah we got it

47:30 - 47:35
then at some later point would act to the right would actually be executed and in log to disk 

47:35 - 47:38
so if you wanted to see whether you're right actually made it to disk 

47:38 - 47:42
yet when you got back the response you had to go back again and say hey did my right make it 

47:42 - 47:46
alright she had to do two round-trips to see whether your right was actually successful 

47:46 - 47:48
that used to be default for a long time

47:48 - 47:51
so if you look at the early benchmark numbers for MongoDB, they're amazing

47:52 - 47:57
then 2013 or 2012 they actually turned that off it's known MongoDB 

47:57 - 48:02
All right now they're actually doing wirte ahead log,and we'll talk about a second

48:02 - 48:06
and they support multi-document transactions across multiple machines, which is impressive it's not not easy to do 


48:08 - 48:11
alright so the system architecture I think we've already talked about before

48:11 - 48:15
it's a heterogeneous distributed components ,shared-nothing the centralized query router 

48:15 - 48:17
they're doing master-slave applications

48:17 - 48:22
you can have your rights from from ,when masternode a master partitioner shard will go off to other shards 

48:22 - 48:27
so part of reason I think MongoDB was very successful in the early days 

48:27 - 48:29
was they actually supported Auto-sharding 

48:30 - 48:35
so the idea was here that like you just start shoving data into your database

48:35 - 48:38
and you know it'll be distribute across multiple nodes 

48:39 - 48:41
but then if one of those nodes gets too full 

48:41 - 48:44
MongoDB will automatically move your data around to balance things out 

48:45 - 48:48
it didn't always work the way people thought it was going to work 

48:49 - 48:54
but you know back in like 2010, when I would go out to Silicon Valley 

48:54 - 48:58
and you ask people like you know oh they're doing a startup and they're basing it on MongoDB 

48:58 - 49:02
you ask him why and maybe say oh, because this Auto-sharding thing was a big deal for them 

49:02 - 49:05
because you know at the very beginning most people don't have much data 

49:05 - 49:07
because you're you know you're start-up you have a stupid Twitter app 

49:07 - 49:08
no one's using it you can run on single machine 

49:08 - 49:12
but of course no one doesn't start out thinking that they're gonna fail ,everything everyone thinks they're gonna be big 

49:12 - 49:14
so of course I'm gonna need you know 20 machines in the future 

49:15 - 49:17
so I wanna make sure MongoDB can scale it with me 

49:17 - 49:21
right with with MySQL and Postgres and other relation databases at the time you couldn't do that 

49:22 - 49:24
so that I think this was a big deal 

49:24 - 49:29
but there's a famous Foursquare outage 51 it was super Foursquare is 

49:29 - 49:31
think of like how to describe this

49:31 - 49:34
it was app you could check in what location you were in, it's still around 

49:34 - 49:38
but it's not not sort of the not not a phone app everyone uses 

49:38 - 49:42
but they were using MongoDB ,and there was a famous like you know multi-day outage

49:42 - 49:43
because the auto sharding stuff got stuck


49:45 - 49:48
alright so here's the architecture ,again we talked about this before 

49:48 - 49:50
and we have different type node types 

49:50 - 49:52
we have routers ,we have config server ,and we have a shards 


49:52 - 49:55
so every query from the application always goes to the router 

49:55 - 49:58
and the router says oh I know I want to look up ID 10

49:58 - 49:59
but I don't know information about where that is 


50:00 - 50:04
so I go to the config server to config server tells me and it looks in the chart table it says 


50:04 - 50:06
the data you want is that it's located it's node 

50:06 - 50:10
so then now the router knows how to send the query to the right location

50:10 - 50:11
or it could blast it out to all of them 

50:11 - 50:15
so again this is important distinction between the scuba stuff and we're talking about here 

50:15 - 50:17
scuba doesn't haunt have it this partition or sharding table 

50:18 - 50:19
it blasts the query to everyone 

50:20 - 50:22
in MongoDB since you don't want to waste resources 

50:22 - 50:25
because you're trying to run you know update query ISM for your website real quickly 

50:25 - 50:28
you you maintain this information to figure out exactly where the query needs to go 

50:28 - 50:32
so you only touch them the data ,you only touch you can do that has the exact data, you need yes

50:35 - 50:38
so a question is it sort of catching on the router yes 

50:38 - 50:39
so like you wouldn't always have to do this 

50:40 - 50:43
but this would be the this would consider to be stateless

50:43 - 50:47
so if I crash I come back I just I get my cache version from this again 

50:48 - 50:51
but any update to like if I say I'm doing the auto shouting stuff 

50:51 - 50:54
and I add a new node right move some of data to another thing 

50:54 - 50:56
I update this thing and that's done as a transaction 

50:58 - 50:58
yes 

51:02 - 51:04
what is the origin of this what do you mean 

51:08 - 51:12
oh the naming ,oh I think Mongo just means humungous, I thought it is 

51:13 - 51:16
right so and so this is like actually the s I don't know that is 

51:16 - 51:19
but like maith backwards I don't know no

51:22 - 51:25
yeah it's this demon or data is P1 just demon 

51:25 - 51:28
and Mongo asks I don't know what that means, yeah I don't know the ask means 


51:33 - 51:38
so another thing I said during this semester ,was that never use MF for your database

51:38 - 51:43
well when month when MongoDB first came out, they were using MF their database

51:43 - 51:47
and we actually did a little study on MF database systems 

51:47 - 51:50
in my in our opinion we haven't published us yet 

51:50 - 51:54
the MongoDB implementation of using MF is probably the most sophisticated one that we've ever seen 

51:55 - 51:56
it's probably the best one 

51:56 - 51:59
But it still sucks, it's still bad alright 

51:59 - 52:03
so basically what happened is the way they would use a map is 

52:03 - 52:09
they would have they'd basically maintain multiple copies of the database in memory 

52:09 - 52:14
and you would do your updates to this like sort of private copy that would then get read to disk 

52:14 - 52:17
and then you had to replay the log to update the sort of the master copy 

52:18 - 52:22
and so the OS could swap things out anytime it wanted ,it was a big big pace for them 

52:23 - 52:27
the other big problem they had was they had a single lock for the database ,now there's a not in time 

52:27 - 52:28
because they were using MMAP 

52:28 - 52:33
but I think it's sort of probably probably helped make things easier using MMAP  

52:34 - 52:37
so that means that in the entire database system ,even though you may be split across multiple machines 

52:38 - 52:43
in the earlier versions as a MongoDB ,you could only have one writer in the entire cluster at a time 

52:43 - 52:46
so if I have 20 machines I have one query that comes along 

52:46 - 52:48
I basic Lock all 20 machines to do update on one of them 

52:50 - 52:53
and then in the newer versions of Mongo since V3, they got rid of this 

52:54 - 52:59
so what they end up doing was they bought this this storage engine startup called WiredTiger

52:59 -53:04
which was founded by the guy that one of the guys that invented BerkeleyDB 

53:04 - 53:07
BerkeleyDB is an DBMS, WiredTiger at a DBMS

53:07 - 53:09
we're gonna talk about this too much

53:09 - 53:15
but Postgres ,MySQL these are all database systems that like run as a standalone demon or standalone system

53:16 - 53:17
and you have multiple clients connecting them 

53:18 - 53:22
SQLite is typically used as an embedded database, you embed it inside your application 

53:23 - 53:24
and it provides you the database functionality 

53:24 - 53:27
but when your application closes the database closes

53:27 - 53:29
so WiredTiger is sort of like SQLite  

53:30 - 53:33
but it doesn't support SQL it supports like a key value store API 

53:34 - 53:39
so probably one of the best acquisitions in a long time 

53:39 - 53:43
they bought WiredTiger and they replace the mmap stuff with with the WiredTiger engine 

53:44 - 53:46
so when you run when you run MongoDB now 

53:46 - 53:49
you get the WiredTiger engine by default and it's amazing 

53:51 - 53:52 
questions are yes 

53:57 - 54:00
this question is if you think MF is so complex why would you use this 

54:01 - 54:06
so again I can't I want to prove it scientifically 

54:06 - 54:09
I can't yet so just take everything what I say like this is my opinion 

54:10 - 54:12
mmap is like a seductress ,right 

54:12 - 54:17
it's like it's this sexy thing that looks like gives you what you want for a buffer pool manager

54:17 - 54:19
right without having to do it, because the OS does it for you

54:19 - 54:21
you don't need an eviction policy the OS does that 

54:21 - 54:25
you don't need to worry about paging, you know keeping track of dirty pages and things ,I think OS doesn't that 

54:26 - 54:28
so it's this thing that looks like it's gonna give you everything you want

54:29 - 54:35
but it's like it's the extra 5% you actually need to actually make it be durable and safe

54:35 - 54:36
that's when all the problems come in 

54:37 - 54:39
so like it's sort of like the query optimizer

54:39 - 54:44
it's a Tibetan query optimizer ,let's do something really simple and just blast everything out and see what comes back first 

54:44 - 54:49
so set it bearing a buffer pool manager, I'll just use the os's buffer pool cache or the mmap cache

54:55 - 54:57
let's take that one off line 

55:01 - 55:04
hey me Elliot I mean L is a really smart guy the co-founder 

55:05 - 55:10
and I've wildly successful right like it's a public company right 

55:14 - 55:19
I think you know by using mmap although it causes problems in 

55:21 - 55:25
it can cause problems when you really start to try to scale up and scale and Hammer the system 

55:25 - 55:27
it allows you to build a system pretty quickly

55:28 - 55:30
so instead of spending six months having to develop a buffer manager 

55:30 - 55:33
that's you know safe and transactional and things like that 

55:33 - 55:35
use these OS you know mmap 

55:36 - 55:38
and then when they got enough customers, they got enough money 

55:38 - 55:40
they bought WiredTiger and did it right 

55:43 - 55:47
MySQL was the same way ,right Mike well he didn't use mmap please I don't think 

55:48 - 55:50
like MySQL InnoDB is amazing

55:50 - 55:53
InneDB is fantastic it's a solid database engine storage engine 

55:54 - 56:00
but the original engine they use was MyISAM ,that thing sucks

56:00 - 56:02
that thing lost data all the time ,right you it would you have corruption 

56:03 - 56:07
and so but it got MySQL go up and running pretty quickly a lot of people are using it 

56:07 - 56:10
and then eventually InnoDB came along and MySQL bought them out and started using it 

56:12 - 56:15
that's a not a very uncommon strategy for database business 

56:18 - 56:20
all right let's do a demo 

56:22 - 56:25
so when we have to tell you is semi semi illegal

56:27 - 56:29
alright anybody is about Mongo, yes 

56:33 - 56:36
this question is why would you want to maybe use Mongo for others 

56:36 - 56:41
so the I think that being in a stupid architecture is matters a lot

56:43 - 56:49
the the document model is actually better for application development right

56:49 - 56:52
think about you when you write Python code ,you write Java code 

56:52 - 56:54
you're writing an object ,it is object oriented programming languages 

56:55 - 57:01
and so it kind of if you can have now your object just be written out as a JSON document 

57:01 - 57:04
and then putting the database and fetch it back in and we instantiate in your application code 

57:05 - 57:07
that could potentially be faster

57:09 - 57:13
but the thing I would stress though there's nothing about what MongoDB is doing 

57:14 - 57:15
because it's document database system

57:15 - 57:18
that is different than anything we talked about this entire semester 

57:18 - 57:22
why are tired doesn't know that it's actually being used for a document database system 

57:22 - 57:24
it's doing write ahead logging 

57:24 - 57:27
it's doing all the crash recovery stuff we're doing it's doing transactions 

57:27 - 57:29
underneath our covers in the same way that we talked about 

57:30 - 57:36
so there's nothing about in the document that invalidates or changes anything we talked about here today 

57:36 - 57:38
it's all at the application level ,hey

57:38 - 57:43
it's all sort of client level to quit what the query looks like, what the query is actually gonna do 

57:43 - 57:46
everything else below is everything we talked about the entire semester 

57:47 - 57:47
yes 

57:51 - 57:53
the question is do they use special compression enemies 

57:53 - 57:56
so that would be WiredTiger, I think it's just snappy 

57:57 - 57:58
yeah yes 

58:04 - 58:09
again the the the older versions the user name the default username passwords test test 

58:09 - 58:11
it's not that way anymore yes 


58:16 - 58:22
ok all right so the last one we have 10 minutes left is CockroachDB 

58:22 - 58:23
actually I was there in the summer as well 

58:24 - 58:28
so when I went MongoDB and CockroachDB their headquarters are in in New York City 


58:28 - 58:32
so when I was out visiting CockroachDB whatever I visited both of them at the same time 

58:32 - 58:35
this is their old office they had this nice little pixel art thing for the cockroach 


58:36 - 58:40
house and this is their newer office though, I was actually surprised how big it actually is

58:40 - 58:43
I gave a game I gave a talk there 

58:43 - 58:49
they just raised 55 million dollars as a Series C round ,which is a little money for a database company 

58:49 - 58:52
so that's actually very impressive how do 

58:52 - 58:54
I assume that means they're doing well so that's good 


58:55 - 59:00
right so CockroachDB started in 2015 by some ex-Google employees 

59:00 - 59:07
they were I think incorrectly characterized as the open-source version a spanner from Google spanner 

59:08 - 59:11
I don't think they ever said that people sort of tributed that to them 

59:12 - 59:13
I would not say that's true at all 

59:13 - 59:18
CockroachDB is sort of a core architecture is fundamentally different than spanner 

59:18 - 59:22
and spanner has one magic piece that nobody else has that cockroach doesn't do ,and nobody else does 

59:23 - 59:28
so is a decentralized shared-nothing, homogeneous database architecture 

59:28 - 59:29
and they're going to be doing range partitioning 

59:30 - 59:34
the internal storage engine they're gonna use is RocksDB 

59:34 - 59:36
so the same question is why would anyone want to use mmap 

59:37 - 59:40
right well my answer was because so you don't have to build that piece yourself 

59:40 - 59:44
so CockroachDB, they then wouldn't spend time writing a storage manager 

59:45 - 59:47
right the same way that you know the WiredTiger piece from MongoDB 

59:47 - 59:50
so a lot of these newer database system startups 

59:50 - 59:53
they're using RocksDB or LevelDB, these embedded storage engines 

59:54 - 59:56
so they don't have to worry about right reading writing data from disk 

59:56 - 60:00
and they worry about the high level parts of like you know managing transactions in a trivet environment 




01:00:01 - 01:00:06
so they're going to be doing MVCC, you know OCC

01:00:07 - 01:00:09
and I know but this is still true 

01:00:09 - 01:00:12
but they're only going to support serializable snapshot isolation

01:00:12 - 01:00:17
so I I don't think they support the other assistant isolation levels 

01:00:17 - 01:00:19
They also speak the Postgres wire protocol 

01:00:20 - 01:00:23
so that means that if you have an existing Postgres application 

01:00:23 - 01:00:28
you can just in theory point it to CockroachDB, it's pointing at progress now 

01:00:28 - 01:00:30
you migrate your data bit over to CockroachDB 

01:00:30 - 01:00:34
and you don't have to change potentially any of the SQL 

01:00:34 - 01:00:38
it's not entirely true for everything like there's some things that are different like Auto increment keys

01:00:39 - 01:00:44
but in general it's migrating to a CockroachDB even Postgres is a should not be an owner snek 


01:00:46 - 01:00:51
so at its CockroachDB is a distributed key-value store transaction attributed key value store 

01:00:51 - 01:00:54
so that means that the lower storage engine of the system 

01:00:55 - 01:00:56
and RocksDB is a key value system 

01:00:56 - 01:00:59
but then the way they're doing transactions or sorry

01:00:59 - 01:01:02
the way they're managing the the database across multiple machines 

01:01:02 - 01:01:03
It`s essentially just going to be a key value store 

01:01:04 - 01:01:11
and so so using that now they can build layers above that, to provide the full sort of SQL compatibility that you would want 

01:01:12 - 01:01:15
all right so once you get past like the SQL query shows up

01:01:15 - 01:01:21
they can then convert that SQL query ,the query plan is essentially going to be a bunch of key value API calls 

01:01:22 - 01:01:24
that can then read and write data from from different nodes 

01:01:26 - 01:01:31
so the way they're going to coordinate the updates throughout the system is using raft 

01:01:32 - 01:01:36
raft is essentially at a high level which same thing as paxos 

01:01:36 - 01:01:39
it's a consensus protocol that allows me to say, when I make an update 

01:01:39 - 01:01:41
and I want to commit a transaction across multiple machine 

01:01:41 - 01:01:48
everyone's gonna agree or quorum has to agree that the updates allowed to proceed or to occur 

01:01:48 - 01:01:50
in order for that transaction to be able to commit 


01:01:52 - 01:01:57
so the way they're going to do OCC is through using timestamps ,

01:01:57 - 01:01:59
it's using what are called hybrid clocks 

01:02:00 - 01:02:06
so I think I talked about this a little bit or Prashant Agra this one we talked about timestamp concurrency control

01:02:06 - 01:02:11
we need a way to have a clock that we can you nuclei identify every transaction 

01:02:11 - 01:02:13
and that clock needs to always be always increasing 

01:02:13 - 01:02:17
so we know what transaction in the Serializable will order as their neck solution , what transaction came before the other 

01:02:18 - 01:02:25
so you could just use the physical clock, which is you know from from the actual CPU , like I'm a machine itself what's the system time

01:02:25 - 01:02:30
but the problem in that one is that's not going to be guaranteed to be highly synchronized across multiple machines

01:02:31 - 01:02:34
right there's every computer has a clock 

01:02:34 - 01:02:37
that clock is not super super accurate it's not like an atomic clock 

01:02:38 - 01:02:42
right this can mean you know counting electrons coming off of an atom 

01:02:42 - 01:02:45
it's gonna be some kind of quartz crystal thing that it has to approximate 

01:02:45 - 01:02:47
so now you can have a bunch of skew and your clocks 

01:02:48 - 01:02:51
and so now what could happen is you know transaction rise on one node 

01:02:51 - 01:02:54
it thinks it's timestamp 1 ,another transaction Rises another node it thinks it's timestamp 1 

01:02:55 - 01:03:01
and now I have a conflict I have pie of a yeah complicated that I've resolved 

01:03:02 - 01:03:08
so the way they're going to handle that is through a hybrid clock , where you'll still use the system clock to try to get what the current time is

01:03:08 - 01:03:10
but then you also used a logical clock 

01:03:10 - 01:03:15
that allows you to have globally ordered transactions without having to synchronize every single time 

01:03:15 - 01:03:18
so basically took a little calendar that says , here's my current timestamp

01:03:18 - 01:03:22
and I just my machine would add one to a disk illogical counter to increase it 

01:03:24 - 01:03:28
so what's gonna happen is under OCC, transactions gonna stage all those rights

01:03:28 - 01:03:31
and the system says these are the modifications I want to make yes question 

01:03:36 - 01:03:39
his question is how's a hybrid clock different than the Google spender clock 

01:03:40 - 01:03:45
the Google spanner clock is not, the Google spent to the treatment

01:03:45 - 01:03:46
I don't get too much into spanner

01:03:47 - 01:03:57
Google spanner is relying on the the,  harbour clocks to have super accurate times, that are synchronized across all the machines 

01:03:57 - 01:04:04
and the true time API gives you a bound of how long we have to wait for someone to show up with a lower timestamp 

01:04:05 - 01:04:10
it's a way to basically is like this is like true time ,but using only software 

01:04:11 - 01:04:15
so if you have really bad clock drift ,like this one machines one hour behind another machine 

01:04:15 - 01:04:18
no transaction could potentially have a complete 

01:04:18 - 01:04:21
because you yeah every transaction think they're gonna in the past in the future get messed up 

01:04:22 - 01:04:27
so you have to make sure that you try to keep the clocks in the sink using like NTP 

01:04:27 - 01:04:29
spanner doesn't do that 

01:04:29 - 01:04:31
spanner does that you know the GPS satellites plus the atomic locks 

01:04:31 - 01:04:33
they have super fine granularity 

01:04:36 - 01:04:40
all right so again different than scuba 

01:04:40 - 01:04:43
all the metadata about where the data exists what the transaction state is 

01:04:43 - 01:04:46
that's when the Vista exists in their own key value store as well 

01:04:46 - 01:04:47
and that's all considered transaction 


01:04:48 - 01:04:49
So let's look at a simple example here 

01:04:50 - 01:04:55
so we have our application comes along , and what may be update data and this classter


01:04:56 - 01:05:04
so again different from scuba ,we maintain a partition table just like a MongoDB to keep track of what node is responsible for ,what range of data 

01:05:04 - 01:05:06
so when my query shows up 


01:05:06 - 01:05:12
the first thing like the consult is my is this partition table, which actually would be replicated on every single machine 

01:05:14 - 01:05:20
and say well I want to access key or ID=50 ,what machine what what node is the leader for that


01:05:21 - 01:05:23
and then in this case here it's this guy 


01:05:23 - 
so now my right has to go here did you my update 


01:05:26 - 01:05:29
and then the update has to get propagated to the other nodes 

01:05:29 - 01:05:34
and then we use raft to to get everyone to agree that we're gonna go ahead and commit this transaction

01:05:36 - 01:05:36
all right 

01:05:38 - 01:05:40
so instead of using two phase commit they're just using raft 


01:05:41 - 01:05:45
so now if I want to do now I read say for this this ID=150 


01:05:45 - 01:05:50
well I'm always going to go to the leader where this guy is located 

01:05:50 - 01:05:56
even though I have multiple copies of the data on different notes ,and I in theory I could read at these other nodes 

01:05:56 - 01:06:03
but I always want to do my read on the leader to make sure, that I'm reading the most you know the most consistent or up-to-date version of it

01:06:03 - 01:06:06
because maybe these other nodes have not seen the latest update yet 

01:06:06 - 01:06:08
because you and a raft you only need a quorum 

01:06:08 - 01:06:14
you don't need to have a every node agree ,which is different you know to Bayes committee everyone this one you always need a quorum 

01:06:14 - 01:06:15
so some node could be behind 

01:06:15 - 01:06:17
but you were still allowed of committed transaction 


01:06:17 - 01:06:22
so to handle that all the reads are always going to go to whatever the leader node is for an ID

01:06:23 - 01:06:26
and to avoid having every query go to just a one node 

01:06:26 - 01:06:29
that's why we have this table appeared to tell us which one's the leader

01:06:29 - 01:06:31
and we're going to distribute that across multiple nodes 

01:06:31 - 01:06:35
so the guest so the reads can then be scattered across multiple machines 

01:06:38 - 01:06:42
again it's just doing the consensus protocol 

01:06:42 - 01:06:45
the replicated replicated rights that we talked about in the semester 

01:06:46 - 01:06:52
the hard so the core concepts aren't any error arm aren't mind blowing aren't dramatically different , than what we've already talked about 

01:06:52 - 01:06:54
the hard part is the engineering making this actually work 

01:06:55 - 01:06:58
and that's what you know that that's what they're spending all their time to make happen

01:06:58 - 01:07:02
so that you don't have you know ,lost writes or missing updates and things like that 

01:07:04 - 01:07:04
okay 


01:07:06 - 01:07:08
all right so let's finish up 

01:07:08 - 01:07:12
so hopefully I've conveyed throughout the entire semester

01:07:12 - 01:07:14
that I love databases they're awesome

01:07:14 - 01:07:17
Your hit him you're gonna hit him throughout the rest your life 

01:07:17 - 01:07:21
and so what hopefully this course has provided for you, is the ability to say 

01:07:22 - 01:07:25
you know something's running slow in our application 

01:07:25 - 01:07:29
because it's the database what kind of database system am i running on

01:07:29 - 01:07:31
what is my data look like, what does my query look like 

01:07:31 - 01:07:35
it allows you to now make an informed decision about

01:07:35 - 01:07:40
how these systems work ,and whether you're you know ,you're choosing the right database system for your given application or workload environment

01:07:41 - 01:07:44
right because not everyone's gonna go off and build a database system 

01:07:45 - 01:07:50
but I guarantee you no matter what you do especially you mean even if you don't stay in the tech field

01:07:50 - 01:07:52
you're gonna chrome across databases

01:07:52 - 01:07:55
like Excel is a database, I said we're using Excel ,you're using a database 

01:07:56 - 01:08:01
so and the other thing I would say also to that's super-important is

01:08:01 - 01:08:06
I would avoid premature optimizations in deciding what database system to use 

01:08:07 - 01:08:12
always start with something that is is maybe it's just good enough of what you need right now

01:08:12 - 01:08:17
and don't worry about you know potentially scaling up to you know millions of users in the future 

01:08:17 - 01:08:19
all right that was sort of MongoDB example

01:08:19 - 01:08:22
everyone said oh my startup is gonna be huge, I don't have a million customers 

01:08:22 - 01:08:23
of course I wanted to distributed database that can scale out 

01:08:24 - 01:08:28
well know in the very beginning you probably just use Postgres or MySQL on a single box 

01:08:28 - 01:08:31
and that'll get you you know maybe for the next two years that'll be good enough 

01:08:31 - 01:08:35
but maybe you buy some better hardware and scale up a little bit more as your domain increases 

01:08:35 - 01:08:37
but don't worry about bringing in distributed system

01:08:36 - 01:08:40
because that's gonna bring more complexities that you maybe need right now

01:08:40 - 01:08:44
you should focus on what makes your app , you know what you need for your application to succeed 

01:08:44 - 01:08:47
so people often ask me what database system should I start with 

01:08:47 - 01:08:50
if I'm building an application ,my interest Postgres or MySQL 

01:08:51 - 01:08:54
for you know 99% of applications that's gonna be good enough 

01:08:54 - 01:08:58
because what will happen is if your application does blow up 

01:08:58 - 01:09:01
and you do have a lot of customers, what are you gonna have 

01:09:02 - 01:09:02
money

01:09:02 - 01:09:06
so now you can go pay me to come tell you how to scale out your database

01:09:06 - 01:09:08
all right or one of my students right 

01:09:09 - 01:09:17
so a avoid you know introducing new complexity by bringing in a you know going overboard with with a database system, you don't need 

01:09:18 - 01:09:20
like maybe starting with Amazon RDS would be a good choice 

01:09:20 - 01:09:23
and then you just scale up and buy a bigger instant size 

01:09:23 - 01:09:28
and you add Azure as your as your data size crows or your need grows okay 

01:09:29 - 01:09:31
all right any questions yes

01:09:33 - 01:09:36
boom okay all right so yes 


01:09:40 - 01:09:44
so uh I did get a my test results came back last week 


01:09:49 - 01:09:56
right this is it, it's mine ,isn't it there's no 

01:09:58 - 01:10:06
so the probability that I am the father of my child is 99.999999 8% 

01:10:07 - 01:10:11
so there is a 0.000002% chance that it's not mine

01:10:11 - 01:10:14
but at this point I've accepted my feet and it's definitely my kid okay

01:10:18 - 01:10:25
Yeah, no it's yeah, all right, yeah, it's mine ,all right ,all right, guys 

01:10:34 - 01:10:37
so I'll see you at the final exam on Monday 

01:10:37 - 01:10:39
and then the I'll have office hours on Friday 

01:10:40 - 01:10:42
and then again this is the last class with DJ drop table 

01:10:42 - 01:10:46
so again rattleballs for him staying with his time Spencer ,okay

01:10:46 - 01:10:48
all right guys take care 


