
00:14 - 00:16
alright let's get started 
好了，我们开始吧
00:16 - 00:20
alright DJ drop tables as back,thank you
我们的DJ Drop Table总算回来了
00:22 - 00:24
did you solve all your problems
你解决你的问题了吗
00:24 - 00:25
yeah most of them
emmm，算是解决了大部分问题
0.33-0.38
okay you know you don't want to be in that boat it's awful ,yeah 
你不想脚踏两条船是吧
00:38 - 00:39
Everything else okay 
其他事情一切顺利吗？
00:40 - 00:43
I mean you still gonna have three girlfriends or you try to come back 
我的意思是，你现在依然有3个女朋友，还是说你只是跑路回来了
00:47 - 00:48
one and a half, what's the half 
一个半，那半个女朋友是什么鬼？
00:50 - 00:53
Okay,alright alright alright
好了，不说了
00:53 - 00:56
so we have a lot I think a lot of things to talk about today 
So，我们今天要讨论的内容有很多

00:57 - 01:01
so real quickly before we get into the course the the topic for the lecture today 
So，我们来快速看下今天要讲的内容
01:01 - 01:06
so this is the final docket for everyone this semester 
So，对于你们来说，这是这学期你们最后要面对的东西了
01:06 - 01:08
this is everything that that you have to finish up
你需要完成这一切
这就是你必须完成的一切
1.08-1.10
project 3 obviously was due last night 
很明显，Project 3昨晚就截止了
01:10 - 01:13
homework 5 should go out today ish 
Homework 5今天会放出来
01:13 - 01:17
and that'll be due in two weeks on December 3rd
它会在两周后截止
1.17-1.19
project 4 will go out went out this weekend
Project 4会在这周四放出
01:20 - 01:21
and that'll be due on December 10th 
它的截止日期是12月10号
1.21-1.24
,the extra credit is due on December 10th as well 
额外加分也是在12月10号结束
01:24 - 01:27
and then we'll have the checkpoint which I'll talk about next slide 
我会在下张幻灯片的时候讲下关于额外加分的时间点检查事宜
01:28 - 01:33
and then the final exam is on Monday December 9th at 5:30 p.m. 
期末考试是在12月9号下午5点半
1.33-1.34
not in this room
考试的地点不是这个房间
1.34-1.35
, I don't know what room they'll put us in
我不知道他们把我们的考试地点安排在哪
01:37 - 01:40
but it's sort of a shitty time 
但这有点糟糕
01:40 - 01:45
so maybe we'll do is that new candy maybe a new pizza or something like that something better okay 
So，我们可能会带点糖果，披萨或者其他更好的东西
01:46 - 01:47
so any question about any of these things
So，对于我讲的这些东西，你们有任何问题吗？

01:50 - 01:50
all right

1.50-1.52
 and then some other things that are floating around 
然后，我们来讲下其他相关事情
01:54 - 01.57
we have three more sort of I'm Convio lectures ,
我们会有额外的三节课
1.57-2.05
but like course topic lectures on material that's this week and then one one class next week 
这是这周以及下周课上要讲的主题

02:05 - 02:10
and then when we come back from effort the Thanksgiving break on Monday December 2nd 
当我们结束感恩节假期的时候
02:10 - 02:15
our friends at Oracle will be coming giving a talk about the the stuff that they're working on 
我们在Oracle的朋友会给我们做场演讲，主题与他们的工作方向有关
02:16 - 02:17
and again this is not like a 

02:19 - 02:23
it's not like a lecture where there you know instead of me talking about the material, they're gonna talk about the same material 
那堂课不是我去上，而是他们给你们上，讲的内容和我讲的是一样的
02:24 - 02:27
they're actually gonna talk about what they're been building in in their group 
实际上，他们会讨论下他们组里所构建的东西
02:28 - 02:31
and you'll see how it ties in together all the things various things we talked about through the entire semester 
你们会看到他们是如何将我们这整个学期所讲的内容都串联在一起的
02:32 - 02:35
the other thing that for the second class in the next week 
在下周第二节课的时候
02:36 - 02:37
I do two things
我会做两件事
2.37-2.39
one we'll do a final system review
一件事情是，我们会对系统进行审查
2.39-2.42
and the second one will be what I'll call systems potpourri 
第二件事情就是，我们会去看下一个叫做Potpourri的系统
02:43 - 02:50
where if there's any system you want me to talk about for like 10 or 15 minutes to teach you about ,what it is, how it works ,and why it's interesting 
如果你们想让我花10到15分钟给你们讲下任何你们想了解的系统，它的工作原理以及它令我们感兴趣的地方
02:51 - 02:54
you know and using the vernacular that we talked about through the entire semester 
我会去使用我们这整个学期所讨论的那些专业术语
02:55 - 02:59
we'll have a vote online at this URL, it's just a Google form 
你们可以去点击这个链接，投下票
02:59 - 03:03
and you go select whatever system that you want me to cover for 10 or 15 minutes 
你们可以去选下你们想让我花时间讲的那个系统
03:03 - 03:03
okay

3.03-3.06
so we usually have time to do three or four of them 
So，按照正常情况来看，我们应该是有时间能讲3到4个DBMS
03:07 - 03:11
so the list that I'm showing here is the when you go to the Google Form 
当你去填写这个谷歌表格的时候
03:11 - 03:18
it's the systems on the DBDB.io website
你们可以看到列表中展示了dbdb.io上的那些DBMS
3.18-3.20
 ,that have had the most views for the last two months 
它上面列出了近两个月越来浏览数最多的那些DBMS
03:22 - 03:23
so that they're in that order
So，它们是按照这个顺序排序的
3.23-3.25
 but you don't necessarily have to you know follow that 
但你们不一定需要根据这个顺序来
03:25 - 03:32！！！！！！
but and also there's another one that's on DBDB.io that we want to cover that that's not on that list, just you know that you can even type it in ,okay 
但如果有其他不在dbdb.io上的DBMS，而你想让我去讲这个系统的话，那你就手动输入它
03:33 - 03:36
and you can go back last year and see what I cover
你们可以去回看下去年我所介绍的那些DBMS
3.36-3.38
but I encourage you not to do that before you vote 
但在你们投票前，我不建议你们这样做
03:38 - 03:43
you don't want to sort of take your bias to you just do whatever people did last year 
我不建议你们让我去讲去年已经讲过的那些DBMS
03:43 - 03:46
because I'm always curious to see what you guys are just about 
因为我始终好奇你们这帮人会想听什么内容
3.46-3.49
like, yeah we've covered Postgres a little bit, we covered MySQL Oracle and SQLite a little bit 
比如：我们已经介绍了PostgreSQL、MySQL、Oracle以及SQLite中的一些内容
03:50 - 03:56
easier to see what you guys are seeing on the internet or what you want to do it in your job or on a hobby project 
我想知道你们在网上看到了这方面的哪些内容，或者你们在工作中想做哪些事情
03:56 - 04:00
we know what system you've been thing about maybe using, and I can come teach you about what it is and how it works 
你们可以让我讲下你们正在使用的那些数据库系统，以及它的工作原理
04:02 - 04:02
okay

04:03 - 04:06
all right and then the extra credit feedback
接着，就是关于Extra Credit的反馈
4.06-4.12
again you can submit your extra credit article  this Sunday on November 24th 
你们可以在这周日的时候提交你们的文章到dbdb.io上
04:12 - 04:15
and then the myself and the TA will give you feedback 
然后，我和助教就会给你们一些反馈
4.15-417
and say, what you're doing correctly,what you're not doing correctly 
并告诉你们，你们哪些地方做得没问题，哪些地方没有做到位
04:18 - 04:21
and that way you can fix it up in time for this submission
这样你就可以在你提交前及时修复这些问题
04:21 - 04:22
so they did get full credit 
这样的话，你就能拿到满分
04:23 - 04:23
okay 

04:24 - 04:25
and the questions about any of these things 
对于这些东西你们有疑问吗？
04:26 - 04:28
so I'll put a deadline for when you should go vote right 
So，我会设置一个投票截止日志
04:28 - 04:32
if like you obviously can't vote the day before the lecture ,because then I don't have time that you prepare it 
你们千万不要在我上那节课之前再投票，因为我根本没时间去准备
很明显是为了避免你们在我上那节课之前才投票，因为我根本没时间去准备
04:32 - 04:35
certainly or maybe the week of Thanksgiving deadline for this 
投票的截止时间我可能会设置为感恩节那周

04:36 - 04:40
okay and the last thing is that 
最后要讲的事情是
4.40-4.51
in addition to giving an in-class lecture on that should be December third on the Tuesday December third 
 在12月3号，也就是周二的时候，我们课上会有一堂讲座

04:51 - 04:56
Oracle also be giving a graduate level research talk over  in the CIC building 
Oracle会在CIC楼那里给我们做一场研究生级别的演讲
04:56 - 05:01
I think they're also giving an undergrad talk Monday and Monday afternoon as well 
我觉得他们也会在周一下午的时候给我们做一场本科生层面的演讲
05:01 - 05:03
so there's me three Oracle talks in two days
So，这两天Oracle会为我们做3场演讲
5.03-5.06
 and you know one of them you'll require to come to or not required 
我要求你们来参加其中一场
05:06 - 05:09
but you'll get extra credit for the final if you come to that 
但如果你们来的话，你在期末的时候会有额外加分
5.09-5.11
,and then these ones are optional okay 
其他两场你可来可不来
05:13 - 05:14
Any questions 
还有问题吗？
05:15 - 05:16
we're almost done 
我们基本讲完了我们的规划

05:17 - 05:22
all right so today's class is now 
So，今天要讲的东西是
05:23 - 05:26
the beginning of our discussion on distributed databases 
我们要开始讨论分布式数据库
05:26 - 05:27
and as I said last class
正如我上节课所讲
5.27-5.32
, we can't actually ,you know before we just ,you know jump immediately into distributing databases 
在我们讲分布式数据库之前
05:32 - 05:39
we had to spend however weeks we've gone and so far in this semester to understand, how a single node database system works 
我们在这学期需要花好几周的时间去理解一个单节点数据库系统是如何工作的
05:39 - 05:41
because when now we start going distributed 
因为现在我们要开始讲分布式数据库方面的东西了
05:42 - 05:44
you know just because we have more machines or more hardware,
因为我们现在拥有了更多的机器或硬件
5.44-5.48
 doesn't magically make our system easier to build or better 
这并不会让我们的系统构建起来更加容易或者性能更好
05:49 - 05:54
all the things that we had to talk about for a single node system，we have to still solve them in a distributed system 
所有我们在单节点数据库系统中讨论的问题，我们依然需要在分布式数据库系统中解决它们
05:54- 05:55
and actually they're even harder
实际上，这些问题会变得更难
5.55-5.57
, because now you have to account for the network 
因为现在我们需要考虑网络这方面的事情
05:57 - 06:00
so we've already talked about this before
So，我们之前已经讨论过这些东西了
6.006.06
, when we talked about query executes, this contrast between a parallel DBMS and a distributed DBMS  
当我们讨论查询执行的时候，我们对并行DBMS和分布式DBMS进行了对比
06:06 - 06:08
and when I talk about parallel database system 
当我讨论并行数据库系统时
06:08 - 06:12
we were just assuming that the the DBMS was running on a single box
我们假设DBMS是在单机环境下运行
6.12-6.15
that could have multiple cores and multiple CPUs 
该环境下，我们拥有多个CPU核心
06:15 - 06:21
and then we assumed that the the workers that were executing the queries could communicate very quickly with each other 
接着，我们假设执行这些查询的worker它们之间可以快速通信
6.21-6.23
and that communication was reliable 
并且该通信是可靠的
06:23 - 06:26
because if you're running on the same physical machine 
因为如果你在同一台物理机器上运行你的DBMS
06:26 - 06:29
you're sending things over the interconnect between CPU sockets
你会通过CPU Socket间的interconnect来发送数据
6.29-6.30
that's super fast 
它的速度超级快
06:31 - 06:32
but now under distributed DBMS
但如果是分布式DBMS
6.32-6.37
we still have to you know there's still the things we care about how to do parallel execution
我们在意的依然是如何并行执行
06:37 - 06:41
but now we're doing this potentially across multiple machines 
但现在，我们是通过多台机器来并行执行
06:41 - 06:50
and so now we actually need to be mindful on what the cost is and the reliability of one worker communicating with another worker 
So，实际上，我们现在需要注意的是一个worker和另一个worker进行通信的成本和可靠性
06:50 - 06:52
because it's going over the network 
因为这是通过网络进行通信
6.52-6.54
that you know that other worker might not be in the same data center right 
你知道的，其他worker可能并不在同一个数据中心
06:55 - 06:57
might be on the same continent 
它们可能是在同一个大陆上
06:58 - 07:03
and so now we can't assume that you know we send a message there guarantee to get that 
So，我们无法保证我们发送一条信息就能进行通信
07:03 - 07:07
and that's gonna come from the problem when we start talking about transactions and other things 
当我们开始讨论事务和其他事情时，这就会出现问题

07:09 - 07:10
so as I said 
So，正如我说的
7.10-7.13
the for the distributed DBMS, we could talk about starting today 
对于我们今天开始讨论的分布式DBMS来说
07:13 - 07:16
it's it's building on all the things we've already talked about 
它是建立在我们已经讨论过的东西之上
07:17 - 07:18
so we still have to do logging
So，我们依然需要做日志
7.18-7.22
 ,we stuff to do a concurrency Control ,query optimization 
我们需要并发控制，查询优化
07:22 - 07:25
we have to do query execution，to do joins potentially 
我们需要进行查询优化，进行join操作
07:25 - 07:28
all those things we still have to do in distributed DBMS
这些都是我们需要在分布式DBMS中要做的事情
7.28-7.31
, and now they're just everything is more expensive， everything is harder 
做这些东西的成本就变得更为昂贵，问题也变得更为复杂

07:33 - 07:38
so for today's lecture as I said today's sort of introduction to distributed databases
So，正如我说的，今天这节课上我要向你们介绍下分布式数据库
7.38-7.47
 just to understand  you know what they actually look like， the different designs of them, one of the implications of those designs 
为了让你们理解它们实际是怎么样的，它们中有哪些不同的设计，这些设计有哪些影响
07:47 - 07:49
and then we'll talk about how to do the partitioning,
接着，我们会去讨论如何进行分区
7.49-7.57
 which is the key way we're gonna divide up our database across multiple resources to get the parallelism, we want in a distributed environment 
这是我们在分布式环境下将数据库拆分到多个资源上以获得并行性的关键方式
07:57 - 08:03
and then we'll finish up briefly touching on how distribute concurrency control is 
接着，我们会简单讲下分布式并发控制是怎么做的
08:04 - 08:09
and then that'll segue into Wednesday's class, where we'll spend the entire day talking about how we actually do this
接着，在周三的时候，我们会花一整天去讨论我们实际该如何做
08:10 - 08:12
and again like we're still going to do 2PL potentially 
比如：我们依然会使用两阶段锁
08:12 - 08:13
we're still gonna do timestamp ordering
我们依然会去使用Timestamp Ordering
8.13-8.16
all those things we did on a single DBMS still apply here
我们在单个DBMS上做的这些所有事情都会应用在分布式DBMS上
8.16-8.17
just now it's distributed
我们现在就是将这些东西应用在了分布式上
8.17-8.18
so it's even harder 
So，这样就变得更难了
08:18 - 08:18
okay

08:20 - 08:22 ！！！！！！！
and again stop and ask questions as we go along 
如果遇上问题，随时打断我，我会为你解答

08:24 - 08:26
so the first thing we need to discuss is 
So，我们需要讨论的第一件事情是
8.26-8.28
what is the system architecture of the database system 
这种数据库系统的系统架构是什么
08:29 - 08:32
so as I said before when we talk to parallel systems
So，正如我之前在讨论并行系统的时候，我说过
08:32 - 08:37
we talked about there being these workers 
我们讨论了这些worker
08:37 - 08:41
that are typically tied to either a process or a thread that are running on the CPU 
这些worker通常是和CPU上所运行的进程和线程绑定在一起
08:42 - 08:46
and they're gonna access shared resources like disk and memory 
它们会去访问诸如磁盘和内存这样的共享资源
08:47 - 08:54
and so the design of our database system in an distributed environment depending what our architecture is 
So，分布式环境下我们数据库系统的设计取决于我们的架构是什么
08:54 -09:04
right the variations of these architectures are going to differ in how you actually can coordinate the CPUs and communicate with each other, as you're running queries or transactions in parallel
当你执行查询或者并行执行事务的时候，这些不同架构会以不同的方式去协调CPU并让它们彼此进行通信
09:05 - 09:10
and where is the memory and where is the disk located in contacts to the CPUs 
它们的内存在哪里，磁盘又在那里，它们是如何与CPU进行联系的

09:11 - 09:16
so what we talked about so far the entire semester is what is known as a shared everything system 
So，我们这整个学期中，目前为止所讨论是一种叫做shared-everything的系统
09:16 - 09:19
assume this is a single box, a single rack unit 
假设这是一个单机
09:19 - 09:21
that has CPU 
它拥有CPU
9.21-9.25
and CPU has local memory and you know there's a local disk that you can read and write to 
本地内存，以及一个你可以进行读写的磁盘
09:25 - 09:29
right now any time I want to access something in the binomial architecture, that we're based on,
在任意给定时刻，当我想去访问（我们所使用的）这种架构下的某些东西时
9.29-9.31
anyhow I'm gonna get something from disk 
当我从磁盘中获取数据时
09:31 - 09:33
I got to bring it to my buffer pool into memory 
我需要将这些数据放入内存中buffer pool里面
09:34 - 09:40
and then my worker up above running on my CPU can read and write to things, read write to pages, and then I let you write them out the disk 
那么，我CPU上运行着的worker就可以对这些东西进行读写，对这些page进行读写，然后我将修改过的page写出到磁盘
09:41 - 09:47
again most DBMS that every database system that's not distributed is using this approach
所有非分布式DBMS使用的都是这种方案
9.47-9.48
it`s shared everything system 
即shared-everything系统

09:50 - 09:55
so an alternative in a distribute environment is one alternatives called shared memory
So，在分布式环境下另一种备选方案叫做shared-memory
09:55 - 9.57
and the idea here is that
它的思路是
9.57-*9.58
 you'll have multiple CPU resources
你有多个CPU资源
9.58-10.01
, that are potentially running on different machines 
它们可能是来自于不同的机器
10:02 - 10:09
but there's a communication layer that allows them to have a unified memory view across all those machines 
但这里会有一个通信层，这使得它们在跨机器的情况下拥有一块统一的内存
10:10 - 10:11
all right

10.11-10.14
 assume this is some kind of high speed interconnect like InfiniBand or TCP/IP
假设这是某种类似InfiniBand或者TCP/IP之类的高速interconnect
10.14-10.15
 it doesn't matter 
这都没关系
10.15-10.17
the high level architecture is still the same 
它们高级层面的架构依然是相同的
10:17 - 10:22
and then there's still going to be some local or sorry, some shared disk that everybody's reading writing to 
接着，这里面依然存在着一些所有人都能进行读写的共享磁盘
10:24 - 10:31
I said typically the spoiler be this is actually not, actually I don't know of any database of them actually, that's commercially or open source that actually uses this 
实际上，我并不清楚有任何商业数据库系统或者开源数据库系统使用这种方案
10:31 - 10:36
this kind of architecture is mostly seen in the HPC or high-performance computing world
这种架构最常见于高性能计算领域
10:36 - 10:39
like the people running on supercomputers at like the big National Labs 
就比如，人们在国家实验室中的超级计算机上运行这种东西
10:40 - 10:43
they build software and assuming this model
他们构建软件时会使用这种模型
10.43-10.45
 for databases is there isn't that much 
对于数据库来说，并不需要这样

10:46 - 10:48
another approach is do shared disk 
另一种方案则是shared disk
10:49 - 10:50
and the idea here is that 
这里的思路是
10.50-10.56
the CPU workers are the workers running on the CPU, they have local memory 
这些运行在CPU上的worker有它们自己的本地内存
10:57 - 11:04
but the disk where we have maintained a persistent state of the database, that's some scrum shared architecture shared device 
但我们用来维护数据库持久化状态的磁盘是一种共享设备
11:05 - 11:07
that all these CPUs can read and write in to
 所有的CPU都可以对其进行读写
11:09 - 11:09
right to think of this
思考下这个
11.09-11.12
 if you're running on Amazon
如果你使用的是Amazon的服务
11.12.-11.16
, this is something like s3 or EBS or HDFS and kind of distributed file system 
这其实就像是和S3、EBS、HDFS之类的分布式文件系统
11:17 - 11:20
so all the CPUs are still seeing the same disk 
So，所有的CPU看到的依然是同一个磁盘
11:21 - 11:23
but in order for them to communicate with each other 
但为了让它们彼此间进行通信
11.23-11.25
they have to maybe send messages back and forth between them 
它们之间可能必须来回发送信息
11:26 - 11:29
right because this CPU can't read the memory of this CPU
你知道的，因为这个CPU无法读取另一个CPU的内存（知秋注：共享的是磁盘存储，具体可以参考MIT6.824第一集内容）

11:31 - 11:37
the last architecture is what most people think of when they think of distributed database, just what is going to shared-nothing 
最后一种架构，其实也是大部分人在思考分布式数据库时所考虑的架构，即shared-nothing
11:38 - 11:43
meaning every single worker is running on a sort of island by itself 
即每个worker都是在一个“孤岛”上干活（即自己干自己的事情）
11:44 - 11:46
it has its own local memory has its own local disk 
每个worker都有自己的本地内存和本地磁盘
11:46 - 11:56
and the only wave to coordinate between different workers it to go up above and communicate by using some kind of messages fabric on the top 
协调这些worker的唯一办法就是在它们上面通过某种message fabric来进行通信

11:57 - 12:04
so again this CPU worker here can't read the memory or disk from and any its neighbors or friends in the cluster 
So，这些CPU worker无法读取集群内其他worker内存或磁盘中的数据
12:06 - 12:08
so again we'll go through each of these one by one
So，我们会逐个来看这些系统架构

12:10 - 12:11
so again under shared memory
So，在shared memory这种架构下
12.11-12.12
 as I said
正如我说的
12.12-12.15
 this is not that common in databases 
在数据库中，这种架构并不常见
12:15 - 12:19
I'm not aware of any system that's actually people are using that's based on this 
我并不知道有哪个数据库系统使用了这种架构
12:21 - 12:22
and the basic idea is that
它的基本思路是
12.22-12.26
 the database system is running on these different CPUs 
数据库系统运行在这些不同CPU之上
12:27 - 12:29
it's running in the same operating system instance 
它会运行在同一个操作系统实例
12:29 - 12:34
Right and assumes it has a single global address space 
假设，它有一个全局地址空间
12.34-12.36
,that maybe just aggregated across different machines 
这可能只是将不同的机器聚合在一起
12:38 - 12:42
and then there's some networking layer that allows them to pass messages back and forth to make make this work 
接着，通过某种网络层来让它们之间来回发送信息，以此来让这种架构工作
12:42 - 12:45
right so again this could be InfiniBand ,this could be TCP/IP 
So，它用的可能是InfiniBand，也可能是TCP/IP
12:45 - 12:47
this could be intel's OmniPath 
也可能用的是intel的Omni-Path
12.47-12.50 ！！！
,alright some fast interconnect between them 
总之是用于这些机器之间通信的高速interconnect
12:51 - 12:53
so in this world 
So，在这种架构中
12.53-12.56
the database instance running on one CPU
数据库实例是在一个CPU上运行的
12.56-12.59
 it like the worker is aware of other workers 
某个worker会注意到其他worker
12:59 - 13:01
and so if they want to communicate between each other 
So，如果这些worker彼此之间想互相通信
13:01 - 13:03
they can just do what you would normally do in a shared everything system 
它们可以做那些shared-everything系统下所做的事情
13:03 - 13:09
you can write something into a global data structure or send a message over an IPC 
你可以往一个全局数据结构中写入数据，或者通过IPC发送一条消息
13:10 - 13:15
and you know the other process of the other worker running on on the other another machine would see that 
运行在其他机器上的worker会看到这些东西
13:16 - 13:18
right again in the context of a shared everything system
在这种shared-everything系统下
13.18-13.18-
when we were doing 2PL
当我们使用两阶段锁时
13:20 - 13:25
if we wanted to tell another worker that, hey I hold the lock for this tuple 
如果我们想告诉另一个worker：Hey，我拿着这个tuple对应的锁
13:25 - 13:29
I add an entry to my lock table that's sitting in memory
我就会往内存中的lock table里添加一个条目
13:29 - 13:30
so same thing here
So，这里也是一样的
13.30-13.33
if one worker wants to acquire lock on a tuple
如果一个worker想去获取某个tuple对应的锁
13.33-13.35
just updates the global lock table 
它只需去更新下这个全局lock table
13:36 - 13:41
and then the messaging fabric is guarantee to make sure everything's coherent across all those workers 
接着，Messaging Fabric是用来保证确保所有worker的接触的所有东西都是一致的
13:43 - 13:44
okay as I said 
Ok，正如我说的
13.44-13.45
this is not that common 
这种并不常见
13.45-13.49
,I don't know if anybody actually does this 
我不清楚是否有人真这样做了


13:49 - 13:51
the more common one is share disk
更常见的架构则是这种shared disk
13.51-13.53
and again the idea here is that
它的思路是
13.53-1354
 we have these compute nodes
我们有多个计算节点
13.54-3.57
 that have their own local memory 
每个节点都有它们自己的本地内存
13:57 - 13.59
they can have a disk up there as well
每个节点里面也可以有一个自己的磁盘
13.59-14.05
but that's not the final storage location of any kind of data in the database 
但这并不是数据库中数据所保存的最终位置
14:05 - 14:06
you can just use it for caching 
你可以将这些磁盘用于缓存
14.06-14.08
in case you need to spill the disk on your local machine 
以便根据你的需要，将数据溢出到你本地机器的磁盘中
14:09 - 14:12
but the database the final  resting location is down here 
但数据库真正存储数据的地方则是在下面这个磁盘中（知秋注：比如hdfs）
14:13 - 14:18
so if I say I bring a page into my buffer pool in my local memory say I modify it 
So，假设如果我要将一个page放入我本地内存中的buffer pool中，并对其进行修改
14:18 - 14:19
and then I'm gonna write it out 
然后，我将它写出
14.19-14.20
because it's dirty
因为它变成了dirty page
14.20-14.21
 ,I would write it down here to the shared disk 
我就会将这个page写出到下面的共享磁盘中（比如写到hdfs中）
14:22 - 14:25
and now potentially any other worker can see my change 
现在，可能其他的worker都能看到我所做的这个修改
14:26 - 14:30
how you coordinate that will make sure that they're told about that change will get to later 
关于我们如何协调这些worker以让它们看到这些修改，这个我们之后会讲
14:32 - 14:33
so as I said
So，正如我说的
14.33-14.37
this is the present architecture in today's cloud environment 
这是当下云环境中所使用的架构
14:38 - 14:42
because the disk is going to be you know something Amazon provides like s3 EBS 
因为这里的磁盘会是Amazon所提供的S3或者EBS之类的东西

14:42 - 14:49
so pretty much every sort of cloud-native database system that you've heard about is gonna be running this environment
So，你们所听过的那些云原生数据库系统都会在这种环境下运行
14:50 - 14:52
because one big advantage you can get is that 
因为你所能获得的其中一个优势是
14.52-14.57
you're able to scale up the compute resources and the disk resources separately 
你能够分别对计算资源和磁盘资源进行扩展
14:59 - 15:01
because the compute resources are stateless
因为计算资源都是无状态的
15.01-15.04
, the state of the database is down here 
数据库状态则是在它之下的
15:04 - 15:07
so if these all these compute resources crash and go away
So，如果这些计算资源崩溃消失了
15.07-15.13
assuming I you know of log things out correctly, everything is still here 
假设我做好了日志记录，那么所有数据依然都还在磁盘中
15:13 - 15:18
and then I can bring up another instance and pick up where the other guys left off
那么，我就可以将这些东西放到另一个实例中，并从它们故障的地方开始继续工作
15:19 - 15:20
we'll see in a second
我们稍后会看到这些
15.20-15.24
, that's not so easy to do in a shared-nothing environment 
这在shared-nothing环境下，并不容易做到
15:25 - 15:27
because every node hold state 
因为每个节点都是有状态的

15:29 - 15:30
so let's look at a high-level example like this
So，我们来看个高级例子
15.30-15.32
again so we have our application server 
So，这里我们有一个应用程序服务器
15:32 - 15:36
it's gonna send requests to these front-end compute nodes, right 
它会将请求发送给这些前端计算节点
15:36 - 15:39
this is where we have the workers running on CPUs and the local memory 
我们的worker运行在这些节点上的CPU和本地内存上
15:39 - 15:42
and then we have some back-end storage device
接着，我们还有一些后台存储设备
15.42-15.44
 that everyone can read-write to 
所有人都可以对它们进行读写

15:45 - 15:48
so let's say that the application says I want to get record 101 
So，假设应用程序想获取id为101的记录
15:49 - 15:50
it goes to this node
它会将请求发送到这个节点处
15.50-15.55
how it the application knows to go to this node
应用程序是如何知道它要跑到这个节点去获取数据呢？
15.55-15.56
will cover in a second
我们稍后会介绍这一点
15.56-15.57
 , but assume it does 
但我们假设它就是知道

15:57 - 15.58
so then this says 
So，接着，这个节点表示
15.58-16.01
some kind of look up that says well record 101
它要去查找id为101的记录
16.01-*16.03
 ,if I look at my index, I see that it's on page ABC 
我查看了下我的索引，我看到这条记录是在page ABC上的

16:04 - 16:05
so I go to my shared disk storage
So，我跑到我的共享磁盘存储设备这里
16.05-16.07
 and I say get me page ABC
并说，请给我page ABC
16.07-16.08
 ,and I bring that to my buffer pool
然后，我将这个page放入我的buffer pool中
16:08 - 16:10
same thing this guy wants 200 
如果我要获取id为200的记录也是如此
16:10 - 16:12
he doesn't have it in its buffer pool
如果该节点中的buffer pool里面并没有这个数据
16.12-16.13
so it goes out the disk and fetches it in
So，它就会跑到磁盘这里，获取这个数据

16:15 - 16:20  
so now if I want to scale up compute resources 
So，现在我想去扩展我的计算济资源
16:20 - 16:23
because again the state of the database is always here on own shared disk 
因为数据库状态始终是放在共享磁盘上的

16:25 - 16:27
I can just bring him a new guy here 
这里我可以放一个新的节点

16:27 - 16:29
I don't have to copy anything immediately
我不需要立刻去复制任何东西
16.29-16.32
because if I now request say 101 
如果我的请求要去获取id为101的记录
16:33 - 16:33
same thing
这里我只需做和之前相同的操作即可
16.33-16.36
 I just go to disk and bring it back to my buffer pool 
我只需跑到磁盘那里获取对应数据，并将它放入我的buffer pool中
16:36 - 16:38
and I could serve the request 
这样，我就可以处理这个请求了
16:41 - 16:42
tricky sorry yes
请问
16.42-16.44
his question is
他的问题是
16.44-16.47
 how does the lock manager work in a setting like this 
lock manager在这种情况下是如何工作的
16:47 - 16:49
and then we'll cover this yes
我们稍后会讲
16.49-16.50
not yet
而不是现在
16.50-16.53
to reduce simplicity we're talking about how do we ever get things in now yeah
这里我们是出于方便起见，先不讲这个
17:02 - 17:03
yes so this question is 
So，他的问题是
17.03-17.06
in a shared memory architecture
在这种共享内存架构下
17.06-17.12
how is that different than a Multi multi socket, multiprocessor shared everything system
它和那种多处理器共享系统有什么不同
17.12-17.12
they're the same 
它们是一样的
17:13 - 17:22
but think of like so there are, there are there are, there are distributed DBMS 
它们是分布式DBMS
17:22 - 17:25
that have a unified memory cross multiple physical machines
它们拥有一个跨多台物理机器的统一内存

17:26 - 17:31
so each machine has its own motherboard has its own like physical memory that it can read and write to 
So，每台机器都有自己的主板以及它可以进行读写的物理内存
17:31 - 17:33
but there's a layer 
但这里有一个层
17.33-17.38
that it says all the processors think ,they have this one giant block of memory
它表示所有处理器都会觉得它们有一块很大的内存
17:38 - 17:40
nobody does that for databases
没有人会为数据库做这种事情
17.40-17.41
you see that in HPC world
你会在高性能计算中看到这些东西
17.41-17.45
 like all those people are doing like nuclear bomb or particle physics simulations 
比如那些做核弹或者物理模拟的人们
17:45 - 17:46
they're writing those Fortran programs 
他们会去编写这些Fortran程序
17.46-17.52
assuming they had this like you know terabytes of memory across multiple machines ,they can do computation on
假设他们拥有一块跨多台机器的数TB级大小的内存，他们可以通过这块内存进行计算
17.52-17.53
,yes
请讲
18:05 - 18:06
So this question is
So，他的问题是
18.06-18.08
, if you assume in a shared disk architecture like this
假设，我们是在这种共享磁盘架构下
18:09 - 18:14
if a  databases vendor is using a shared disk architecture 
如果数据库厂商使用的是这种共享磁盘架构
18:14 - 18:20
does that mean that they either have to have all the the data for a database in one location 
这是否意味着他们必须将该数据库的所有数据放在一个地方
18:21 - 18:21
no
No
18:22 - 18:24
well we'll get to that 
Well，我们之后会讲
18:28 - 18:36
again I mean think about this there's an abstraction between the physical location and the logical location of the disk 
在磁盘的物理位置和逻辑位置之间存在一个抽象层
18:36 - 18:39
these guys don't know anything about where these things are 
这些节点并不清楚这些数据存放在磁盘中的什么位置
18:39 - 18:46
right so like it just says ,hey here's this file I can I can read and write to just as you would as if it was a local disk 
So，它表示，这里有一个我可以进行读写的文件，这就像和我在本地磁盘上对文件进行操作一样
18:46 - 18:48
when in case Amazon or Azure
以Amazon和Azure为例
18.48-18.52
you get like a like a block base or object based API
你会通过一个block-based或者Object-Based API
18.52-18.54
 ,give me this bucket ,give me that bucket 
来得到这个bucket的数据
18:54 - 18:58
that thats sending a restful request to go to some back-end service
它会发送一个restful请求到某个后端服务那里
18.58-18.59
you don't know where that data's actually is 
你并不清楚这些数据实际在哪里
19:12 - 19:15
yeah yeah so we'll get to that as well 
So，我们稍后会讲

19:15- 19:16
so his question is
So，他的问题是
19.16-19.18
 in my example here 
在我的例子中
19.18-19.19
when I'm sending this request 
当我发送这个请求时
19:21 - 19:22
in this example here,
在这个例子中
19.22-19.24
this application says 
这个应用程序表示
19.24-19.26
I'm going to this node to get this record 
我会跑到这个节点处拿到这个记录
19:27 - 19:32
you could have something in front of this that could hide that ,or this thing can maintain to say where to go get the thing I need 
你可能会在此前面去维护一些信息，这些信息会告诉我们该去哪里获取我需要的东西
19:33- 19:34
we'll come to that
我们会去获取这个东西
19:35 - 19:38
just the third thing I'm gonna focus on here is like it's like 
这里我重点要讲的第三件事情是


19:39 - 19:43
this guy has no state of the database other than within its buffer pool 
在这个节点中，除了buffer pool中的数据以外，它里面并没有保存数据库的状态
19:43 - 19:49
but that's not considered to be that's not consider be you know durable or persistent its ephemeral 
我们不能将它里面的数据视作是持久化的，它是临时的
19:49 - 19:53
so this guy crashes anything we had in here it goes away
So，如果这个节点出现了故障，那么我们放在该节点中的东西也就灰飞烟灭了

19:56 - 19:57
all right

19.57-19.59
 so now the tricky things gonna be if I do an update 
So，如果我进行更新操作，那么我就会遇上些棘手的事情


20:00 - 20:06
right so update page 101 ,oh sorry, an ID record of 101, I have to update page ABC 
So，这里我要对id为101的记录进行更新，我需要对page ABC进行更新

20:06 - 20:11
these guys all read that same record they have page ABC in the buffer pool 
这些节点都读取到了同一条记录，并且它们的buffer pool中都有page ABC
20:12 - 20:17
but they're not going to know about the the changes, because these shared disk architectures
但它们并不清楚这里的修改，因为它们使用的是shared-disk架构
20:17 - 20:21
they don't provide a notification say hey by the way somebody updated this 
这种架构并没有提供一种通知机制，比如说：hey，有人更新了这个东西

20:23 - 20:31
so I have to have additional messages these nodes to say, hey I think you have page ABC by the way I just modified it 
So，我需要给这些节点发送额外消息，以此来告诉它们：hey，我你手上有page ABC的话，我得通知你一声，我刚刚修改了这个page
20:31 - 20:33
and here's the latest version
这是该page的最新版本
20.33-20.35
 ,or if you're gonna find out what the latest version is come asking me about it
或者，如果你想拿到该page的最新版本，可以来找我
20:36 - 20:40
so that's all the stuff that we have to build in our database system right 
So，这就是我们在构建我们数据库系统时所要做的所有事情
20:40 - 20:43
This is just reading writing to some disk 
这里我们只是对磁盘进行读写
20:43 - 20:47
and so related his question that it's all transparent 
So，这与他所提的问题相关，这里都是透明的
20:47 - 20:52
so right now I'm showing the database and this diagram is on two disk 
So，在这张图上，我的数据库是放在这两个磁盘上的

20:52 - 20:57
but I can easily add a bunch more to now split up the data cross more disk
但我可以很容易地添加更多磁盘，并将数据拆分到这些磁盘上
20:57 - 21:00
so I'm getting better parallelism better better replication better reliability 
So，这样我就获得了更好的并行性、更好的replication以及更好的可靠性
21:01 - 21:04
but none of these guys in the compute layer
但对于计算层面的这些节点来说
21:04 - 21:05
they don't know anything about that
它们对此一无所知
21.05-21.07
, because that's all hidden from me 
因为我将这些东西隐藏了起来
21:07 - 21:09
so you had this nice separation
So，我们就将这些东西很好地隔离开来
21.09-21.10
 where you can scale things out independently 
即我们可以单独对这些东西进行扩展
21:10 - 21:14
but you're gonna pay a penalty in terms of locality of access 
但你要对某个地方进行访问时，你要付出一定成本
21:14 - 21:18
because for the locality  I can't run queries over here
因为我没法在大部分地方执行查询
因为我没办法在数据存储的层面来运行查询
21:19 - 21:21
s3 allows you do some basic filtering
S3允许你做些基本的过滤
21.21-21.23
, but about anything like a join I have to do over here 
但诸如join之类的操作，我就得在这些节点处进行了
21:23 - 21:25
so that means I have pooled the data to my compute nodes
So，这就意味着，我要将数据放入我的计算节点处
21.25-21.26
yes 
请讲
21:35 - 21:36
so his question is,
So，他的问题是
21.36-21.38
 this is different in sharding or partitioning
因为数据存储使用了分片或分区
21.38-21:43
, we have explicit divisions of who has what data, we'll get to there 
我们要去哪个目的地，得明确区分出哪个分片或分区拥有哪些数据才行
21:43 - 21:45
I'm just showing you what shared disk is  at a high level
这里我只是向你们展示，从高级层面来看，什么是shared-disk


21:46 - 21:49
you can still partition at this level
但你依然可以在计算节点层面进行分区
21:50 - 21:53
there's nothing about share disk precludes you from doing partitioning at the compute level 
shared-disk无法让你在计算层面进行分区
21:57 - 21.58
Correct
没错
21.58-22.01
if you're not doing partitioning
如果你不做分区的话，就是这样的
22:16 - 22:16
her question is
她的问题是
22.16-22.18
 in this example here
在这个例子中
22.18-22.18
I said
我说过
22.18-22.21
 when I update page ABC in this node
当我在这个节点处更新page ABC时
22:21 - 22:24
it sent a message to the other guys updated and say hey I haven make a change
该节点会发送消息给其他节点，并说：我更新了这个page
22.24-22.30
is it always like this or what all over the alternative 
就像这里展示的一样或是通过其他备选方案来更新其他节点
22:30 - 22:33
yeah yeah so what I said it in this case here 
So，在这个例子中，我所说的是
22:34 - 22:35
I update a page ABC
我更新了page ABC
22.35-22.38
it has to the compute node at the top is update these compute nodes to the bottom
它得自顶到底将更新信息推送到各个节点的
22.38-22.42
an alternative would be a way to do a push notification 
一种备用方案就是去推送通知
22:42 - 22:44
and say hey I just got an update ABC
并说：hey，我更新了page ABC
22.44-22.47
by the way everybody needs to update and refresh themselves 
你们其他人（node）都需要更新并刷新一下
22:49 - 22:57
I'm not aware of any shared disk architecture, like EBS s3 whatever the address up they don't do that 
我不会意识到我存储数据的地方是类似EBS s3这样的云存储，它们不会这么做的
22:57 - 22:58
because that would be super expensive 
因为这样成本就会变得超级高
22:58 - 22.59
because if you think about it
如果你稍微思考下它
22.59-23.01
 it's like a pub sub system
这就像是一种发布/订阅系统
23.01-23.04
, I need to know who needs to know about my change
我需要去知道哪些节点该知道我所做的修改
23:05 - 23:07
because otherwise I'm sending messages that are wasteful 
不然，我发送信息其实就是在浪费系统性能
23:07 - 23:10
so as far as I know nobody actually does this
So，据我所知，实际没有人使用这种方案
23.10-23.12
you have to coordinate at this layer here
你需要在计算层面进行协调
23.12-23.13
and this is the database system does this 
数据库系统为我们做了这些（知秋注：要涉及到多核并行处理，所以要做这些事情）
23:13 - 23:16
the distributed file system or the object store doesn't do that
分布式文件系统或者对象存储则不会做这些
23:17 - 23:17
yes 
请讲
23:22 - 23:23
his question is 
他的问题是
23.23-
are we assuming here that the notification is reliable or fast enough no 
假设我们这里的通知很可靠，足够快，no！
23:28 - 23:31
I didn't say what this is I'm saying how we're doing the same thing you have to do this 
我要说的不是这个，我这里要说的是同一件事情在不同场景下，我们该如何去做（知秋注：一个是单机多核，一个是分布式情形）
23:36 - 23:36
his question is
他的问题是
23.36-
 can't you run an issue stale reads absolutely yes 
我们能否运行一个stale reads,完全可以的
23:38 - 23:40
that's that's concurrency control we'll get there 
这是关于并发控制方面的东西，我们之后会讲
23:41 - 23:41
yes 
Yes

23:45 - 23:45
okay 

23:47 - 23:54
so the the again most people think about ,when they think about distributed databases is the shared nothing architecture 
So，大部分人在考虑分布式数据库时，他们会考虑这种shared-nothing架构
23:54 - 23:58
where you have each node has its own local disk and local memory 
每个节点都有它自己的本地磁盘和本地内存
23:58 - 24:02
and the only way for me to coordinate as I run queries is to communicate directly between my nodes 
对我来说，唯一能够去协调这些节点的方式是，当我执行查询时，这些节点之间会直接进行通信
24:03 - 24:04
so if I want to get data
So，如果我想去获取数据
24.04-24.08
If I query shows up and need accessed data on another machine 
如果我的查询要去访问另一台机器上的数据
24:08 - 24:11
I can't go to disk and get the shared disk and get it ,because that doesn't exist 
我没法跑到共享磁盘处获取这个数据，因为这个架构中没有共享磁盘
24:11 - 24:13
I can't read the memory from the other guy
我也不能去读取其他机器内存中的数据
24.13-24.15
because I can't do that 
因为我没法这么做
24:15 - 24:22
I have sent a message to say hey I think you have this data either run this query for me and give me back the result or send me that data  
我会发送一条消息说：嘿，我认为你有该数据或者为我运行此查询，并将结果返回给我或发送给我
24:22 - 24:26
and then now you get the issue of like who should have what copy of what data, right we'll get that 
然后，我们就会有一个问题，这份数据拷贝应该给谁，right，正是我们要得到的
24:27 -24:35
so this is gonna be the hardest architecture to increased capacity and ensure consistency
扩容和并确保一致性是这个架构方案最难的地方
24.34-24.36
that's the stale read issue that he talked about 
这就是他所谈论的stale read的问题
24:36 - 24:45
Because, I need to be able to run the system and move data around in a way to I'm not losing things 
因为我需要能够运行系统，并以某种方式移动数据，以避免我丢失数据
24:45 - 24:47
I'm not having false negatives or false positive as I execute queries 
当我执行查询时，我就不会遇上假阴性或假阳性的问题
24:49 - 24:52
right otherwise I shut the whole system down
否则，我就会关闭整个系统
24.52-24.54
 then move data around and add new capacity 
然后，移动数据，并进行增容
24:54 - 24:55
but I don't want to do that 
但我不想这么做
24.55-24.57
because I want my system to always be online 
因为我想让我的系统始终能在线提供服务
24:58 - 25:00
so now you say well that this sounds hard 
So，你现在会说：Well，这听起来好难啊
25.00-25.01
why would I want to do this
为什么我想这么做呢？
25:02 - 25:05
well the advantage you're gonna get over a shared disk system, is that
Well, 我们从这种shared nothing disk系统中所获得的好处是
25.05-25.10
 you're gonna get better performance and better efficiency，if the system is written correctly 
如果该系统写操作正确的话，那么我们会获得更好的性能以及更高的效率

25:11 - 25:16
because I can now be mindful of the locality of data 
因为我现在可以只关注局部性数据

25.16-25.20
and try to move the least amount of data over the network as much as possible 
并试着尽可能多地通过网络来移动最少的数据量
并尽可能多地通过网络移动最少的数据量
25:20 - 25:20
yes
请问
25:36 - 25:36
Yes,it do
没错是这样


25:42 - 25:43
correct
正确
25.43-25.47
so his statement is
So，他所说的是
25.47-25.51
 if you assume that your distributed databases partitioned
假设你的分布式数据库已经分区了
25.51-25.52
which we'll get to in a second 
这个我们稍后会讲
25:54 - 25.56
if now I need to add a new partition 
如果我现在需要添加一个新分区
25.56-25.59
and I need to potential reshuffle data
我可能需要对数据重新进行洗牌
25.59-26.02
depending on how I'm doing partitioning 
洗牌的方式取决于我分区的方式
26:02 - 26:06
I may have to move the whole database I'm do segment of it 
我可能需要移动整个数据库，或者对数据库数据进行切片
26:06 - 26:09
but again I don't want to have to stop the world while I move that 
但当我移动数据库的时候，我不希望让数据库停止工作
26:09 - 26:12
and so depending how much data I have a single node 
So，根据我单节点上所保存数据的数量
26:12 - 26:15 
and I'm going with a network to some other machine ,where's that machine, how long does that take 
我会通过网络来访问其他机器，并询问该机器多久才能完成数据移动
26:15 - 26:18
right it's if I'm doing this
如果我这样做的话
26.18-26.21
 if I don't care about consistency which we haven't talked about yet 
如果我不关心一致性，虽然我们现在还没讨论一致性
26:22 - 26:23
then who cares
那么谁会在意这些事情呢？
26.23-26.24
 just move data around
我们只管移动数据就行了
26.24-26.26
 and if you miss a read， whatever 
如果你错过了一次读操作，那又怎么样
26:26 - 26:28
because I do care and I am running transaction
因为当我执行事务时才会关心这些问题
26.28-26.31-
 then I'm gonna be very careful how I do this 
那么，关于我该怎么做，我就会非常小心
因为这执行事务的时候，我对这些非常敏感
26:31 - 26:32
and people get burned by this a lot
人们会因此而特别上火
26:34 - 26:35
so as I said 
So，正如我说的
26.35-26.47
this is just a brief smattering of ,or a very limited subset of the some of, the shared-nothing distributed DBMS that was out there 
这些还只是shared-nothing架构分布式DBMS系统中的很小一部分
26:47 - 26:52
most of the time for the NoSQL systems that came in around maybe ten years ago 
对于过去十年间所出现的大部分NoSQL系统来说
26:52 - 26:54
they're all considered shared nothing 
你可以认为它们使用的都是shared-nothing这种架构

P2->ID:151-300
26:55 - 26:57
so let's look how this works again 
So，我们来看下这是如何工作的
26:57 - 26.59
so no longer we have a shared disk
So，这里我们不再使用共享磁盘
26.59-27.00
 on every single node 
在每个节点上
27:00 - 27:06
we have the CPU workers, we have our local memory, we have our local disk 
我们有CPU worker、本地内存以及本地磁盘
27:06 - 27:08
and then now what I'm showing is 
现在，我这里所展示的是
27.08-27.13
we've partitioned the database or shard the database into subsets
这里我们将数据库拆分成多个节点
27.13-27.18
 such that each node has some portion of of the database 
每个节点上都拥有该数据库中的一部分数据
27:18 - 27:23
and so now I have explicit information about what data I'm having in each node
So，这里我在每个节点上显式标注了该节点所拥有的数据范围

27:24 - 27:25
so now the application says
So，应用程序表示
27.25-27.27
 well if I want to get ID equal 200
Well，如果我想去获取id等于200的那条记录
27.27-27.31
, it has to know that this node has the data that it needs 
它需要知道拥有它所需要的数据的那个节点
27:32 - 27:33
so go ahead and get that
So，跑到那个节点，并获取该数据
27.33-27.38
and again now this is operating as the same single node shared everything data so we had before 
这和我们之前在shared-everything那种架构下所做的事情是一样的
27:39 - 27:41
like it's it's not my buffer pool， I go to disk， bring it in 
如果该page不在我的buffer pool中，我就会跑到磁盘中，将这个page放入buffer pool中
27:41 - 27:44
and then do whatever it is that I want to do to answer the query and return results 
接着，做我想做的事情，以此给出该查询所要的答案，并返回结果

27:45 - 27:49
so if all your queries are accessing a single node 
So，如果你所有的查询访问的都是单个节点
27:50 - 27:51
this is super fast
它的速度就超级快
27.51-27.54
because again this is just a single node database system 
因为这就是一个单节点数据库系统
27:54 - 27:56
the tricky thing is 
棘手的地方在于
27.56-27.58
then when you start touching data that's across multiple machines 
当你开始接触的数据是跨多台机器时

27:59 - 28:05
so let's say I have a transaction that says one get ID = 10 and get ID = 200, like a single query wants to do this 
So，假设有一个事务要去获取id等于10和200的记录，就比如说某个查询想做这种事情

28:05 - 28:11
so now I need to somehow get that data that this other guy has up here 
So，我现在需要通过某种方式来获取另一个节点上的数据
28:11 - 28:12!!!!!!!1
but what am I sending
但我这里所发送的是
28.12-28.14
, I`m sending the request to run the query 
我发送请求来执行查询
28:15 - 28:17
or am I just asking this guy
或者，我直接去询问下面这个节点
28.17-28.20
 hey I know you have this piece of data send it up to me and I'll run the query up here 
并说：hey，我知道你有这份数据，请将它发往上面这个节点，我在上面这个节点处执行查询，需要这份数据才能继续前进
28:20 - 28:22
alright back back in every

28:24 - 28:27
now in terms of the the scale out issue
我们现在来讨论下水平扩展的问题
28:27 - 28:29
right on the shared disk architecture
在shared-disk架构下
28.29-28.31
I just bring up a new compute node
我只需添加一个新的计算节点即可
28.3128.33
 every compute node it's stateless 
每个计算节点都是无状态的
28:33 - 28:37
so therefore it comes along and start executing queries
So，因此当我们添加完新节点，并开始执行查询后
28.37-28.40
and brings things from the back end shared disk into a buffer pool as needed 
我们会根据需要将数据从后端的共享磁盘处放入buffer pool中

P2->ID:151-300
28:40 - 28:42
but now and I shared nothing architecture,
但现在，我使用的是shared-nothing架构
28.42-28.44
 if I have to say bring up a new node 
如果我需要添加一个新的节点
28:44 - 28:48
it now needs to get some portion of the database from these other nodes here,
它现在需要从其他节点处获取该数据库的某一部分数据
28.48-28.50
 so that I balance things out.
这样的话，我需要将各个节点数据都匀给它一点

28:51 - 28:52
right so let's say that
So，也就是说
28.52-29.01
 this guy is gonna send it you know a some some number tuples from this guy from this bottom partition 
下面这个节点会将它保存的一部分tuple发送给中间这个节点
28:59 - 29:01
the other guy here is it sent some other tuples with another partition 
最上面这个节点会发送一些tuple给这个中间这个新分区

P2->ID:201-300
29:01 - 29:04
and then once I've new I've copied the data
接着，一旦我复制完了数据
29.04-29.05
now I update some global state to say
我会去更新下某些全局状态，并说
29.05-29.10
all right well this node is now responsible for the range 101 to 200
这个节点是负责处理id为101到200这个范围的记录
29:10 - 29:12
this guy's 201 to 300, a guy up about 1 to 100 
下面这个节点处理的范围是201到300，最上面那个节点负责的是1到100
29:13 - 29:17
and I was saying to his point like this is this would be hard to do 
就像他所说的，这做起来很难
29:17 - 29:20
if I care about transactions and I don't want to lose any data 
如果我在意事务的话，我不想丢失任何数据
29:20 - 29:26
because I don't want to have a query show up but maybe that wants to access ID equal 150 
因为我不想有这么一个查询，我想去访问id=150的数据条目


29:26 - 29:29
and I'll land here
我落到了这个节点处
29.29-29.32
and the data hasn't been transferred yet 
该节点处的数据还未被转移到中间这个节点
29:32 - 29:34
so maybe I can answer but maybe it has been transferred yet 
so 也许我可以回应这个查询，但也许该数据已经被迁移走了
29:35 - 29:38
and I go here and it says I don't have that data anymore 
那在我走到这里的时候，它就会说，我不再拥有这个数据了
29:38 - 29:39
so it returns back nothing
So，它就什么也不返回


29.39-29.42
 even though it existed this node down here 
虽然我们要的数据存在于该节点下方这个节点
29:42 - 29:47
so how did it how to actually do this in a transactional safe manner is tricky ,and not not easy to do 
So，实际上，我们在保证事务安全的情况下要做到这点非常棘手，并且也不容易做
29:48 - 29:50
yes in the back
后面那位同学请问
29:58 - 30.00
his question is 
他的问题是
30.00-30.03
how often you have to scale capacity 
通常我们需要每隔多久进行一次扩容呢？
30:03 - 30:07
can I shut the database down once a month and add new nodes 
我是否能每个月关闭下数据库，并添加新节点？
30:07 - 30:08
but what if I won't go to the other way 
但如果我使用的是其他方式呢？
30:09 - 30:13
but so what so let's say it's singles day or black or Black Friday or Cyber Monday
So，我们假设我们处于黑五或者剁手星期一的时候
30:13 - 30:16
it's the one day of the year where like I have a huge spike
这是一年中我数据库负载最大的一天
30.16-30.18
that one I can plan ,I know it's coming so I can prepare ahead of time
我知道它什么时候来，我可以提前为它进行准备
30:19 - 30:20
but let's say I have like a flash mob
假设我是一个快闪族
30:21 - 30:24
right everybody wants DJ drop tables new album all of a sudden
所有人突然之间都想要DJ Drop Table的新专辑
30.24-30.27
so also we have a huge spike in traffic that's unexpected 
So，我们就会遇上一个意外的流量巅峰
30:28 - 30:32
I want to be able to scale up without having to shut everything down and scale up gradually 
我想能够在不关闭任何东西的情况下渐进式地进行扩展
30:33 - 30:36
the older systems will do exactly you're saying 
那些较老的系统会做你所说的那些事情
30:36 - 30:41
any time you see any kind of financial website says we're down you know Sunday at 3:00 a.m. 
每当你们看到那些金融网站说他们在早上3点的时候会暂停网站使用
30:41 - 30:44
there probably may probably not running a distributed system
他们用的可能并不是分布式系统
30.44-30.46
but they they're moving data around and doing maintenance things 
但他们会去移动数据，并做些维护工作
30:47 - 30:48
but if you're an online website
但如果你是一个在线网站，比如电商网站之类的
3.0.48-30.49
 you can't do that 
你就不能这么做
30:49 - 30:50
yes
请问
31:02 - 31:02
your question is 
你的问题是
31.02-31.06
what's the advantage of doing this versus having a single node with like 
这种做法与单节点数据库系统相比有什么优势
31:06 - 31:13
like you know instead of having two nodes run these to hold these two partitions what have a single node 
如果我们不使用这两个节点来保存这两个分区的数据，而是用一个节点来保存，那会怎么样呢？
31:13 - 31:17
and you say well this CPU socket has this disk in this memory to run this partition 
假设这个节点上，我们有CPU、磁盘以及内存，以此来处理这个分区的相关工作
31:17 - 31:20
and then another socket has this memory and this disk 
接着，另一台机器上，也拥有CPU、内存以及磁盘
31:20 - 31:22
that you're asking 
这就是你要问的吗？
31:29 - 31:31
all right so her question yeah
So，她的问题是
31.31-31.36
 ,instead of having two separate machines that have you know disk memory and the CPU
这里我们不使用两台单独的机器，这里的单独的机器是指每个上面都有磁盘、内存以及CPU
31:36- 31:37
, what if I had one machine 
如果我使用的是一台机器呢？
31:37 - 31:41
that just had the same amount of resources that it has put across two machines 
它上面拥有的资源等同于原来两台机器上的资源
31:41 -31:43
but now in you know a single unit .
即现在，这一台机器相当于之前两台机器上的资源
31:49 - 31:49
so question is
So，她的问题是
31.49-31.51
 what were the advantages of doing the distributed databases
使用分布式数据库的优势是什么
31:51 - 31:55
so one is if 
So，其中一个优势是
31:59 - 32:03
the you get diminishing returns as you scale up Hardware vertically 
当你对你的硬件进行垂直扩展时，你的收益会递减
32:03 - 32:08
so there's horizontal represent scalability as is adding new machines 
So，水平扩展的意思是添加新机器
32:08 - 32:12
vertical scale voting to take my one machine and adding more resources to make it more powerful 
垂直扩展的意思是，我往一台机器上添加更多的资源，以此来让它更为强大
32:14 - 32:17
going vertically is way more expensive usually 
通常情况下，这种垂直扩展的成本更加昂贵
32:18 - 32:20
and you get diminishing returns 
并且你所得到的收益会递减
32:20 - 32:23
and there's obviously an upper bound how big you can make a machine, right 
显然，对于你能搞出硬盘 内存多大，cpu多强的机器来说，这存在着上限
32:26 - 32:27
let me give one example
我给你们说个例子
32.27-32.29
 in the early days when I was in grad school 
在很久以前，当我还是研究生的时候
32.29-32.31
,we visited PayPal 
我们参观过Paypal
32:31 - 32:34
because PayPal was running Oracle 
因为Paypal使用的Oracle数据库
32:34 - 32:37
and they were freaking out that because every Christmas 
他们每个圣诞节都会是心惊肉跳的一天
32:37 - 32:41
they would hit the they were running Oracle on a single machine
他们在一台机器上运行Oracle数据库
32.41-32.43
 they bought the most expensive machine you could buy from IBM 
他们从IBM处购买了最贵的机器
32:43 -32:44
right and you had to buy two of them
你需要买两台这样的机器
32.44-32.46
, because you need a hot standby right 
因为你需要双机热备
32:46 - 32:51
so the every holiday season ,they were freaking out
So，每逢过节，他们就会紧张的要死
32:51 - 32:55
because that Oracle machine was hitting the in the limit what the hardware can do 
因为这些运行着Oracle数据库的机器会达到硬件所能达到的极限
32:55 - 32:57
and they couldn't buy a more expensive machine 
他们没法再去买一台更加昂贵的机器了
32:57 - 32:59
right so they couldn't scale any more vertically 
So，他们无法进行垂直扩展
32:59 - 33:02
so they were mainly moving portions of the database off 
So，他们主要做的事情就是移走数据库的一部分
33:02 - 33:08
that the humans are moving the portions databases off in like November to these separate machines on the side, just to get to the holidays 
工作人员会在十一月的时候移走数据库中的部分内容，并放到旁边单独的机器上，以应对这种购物节问题
33:08 - 33:10
and then they moved it all back
之后，他们会将这些数据再次移回来
33:10 - 33:11
so in that environment
So，在这种场景下
33.11-33.14
 if they had a distributed database system with cheaper machines 
如果他们使用更加便宜的机器来运行分布式数据库系统
33:15 - 33:16
then they'd say oh the holidays coming up 
接着，他们说：购物节要到了
33.16-33.18
,I'm going to buy or turn on a bunch of new machines 
我要去购买并启动些新机器了
33:19 - 33:23
and had the systems scale out that way handle my high demand
我们让系统进行水平扩展，以处理我的高流量需求
33.23-33.24
then when the demand goes down 
当流量变低的时候
33:24 - 33:28
I can start turning them off and coalesce into smaller machines
我就可以关闭这些机器，然后将这些数据合并到更小的机器中
33:45 - 33:46
your question is 
你的问题是
33.46-33.52
so these is the advantage of distributed database system scaling horizontal versus scaling vertically 
So，分布式数据库系统水平扩展和垂直扩展的优势是什么
33:52 - 33:56
is the advantage that you can scale out much more cheaply horizontally for 
水平扩展的成本是否更低？
33:58- 33.59
correct
没错
33.59-34.00
 but it is there's trade-offs right 
但这里面存在着取舍
34:00 - 34:04
like as we'll see as we talk about how we actually manage a distributed database system
当我们管理一个分布式数据库系统时
34.04-34.06
communication is now more expensive 
通信成本更加昂贵
34:06 - 34:09
I can definitely run faster if I'm on a single node 
如果我使用的是一个单节点系统，那么速度肯定超快
34:09 - 34:12
because I don't need to coordinate between other different nodes and send messages over the network 
因为我不需要在其他不同节点之间进行协调并通过网络发送消息
34:12 - 34:12
but as I said like 
但正如我说的
34.12-34.16
you can start to hit scale go to bottlenecks right 
你可以对系统进行扩展到极限
一个系统的垂直扩展是有极限的
34:21 - 34:27
the trend in database systems up into the 90s was always scale vertically 
在1990年代的时候，数据库系统的扩展方式始终是垂直扩展
34:27 - 34:29
the trend now is to scale horizontally
现在的趋势则是水平扩展
34.29-34.35
because it's considered you get better performance and 
因为这样做，我们能获得更好的性能
34:35 - 34:42
for getting the better performance you pay less you pay less is that always true 
付出更少的成本获得更高的性能，这种做法始终是正确的
34:43 - 34:45
yeah I think that's that's the convention wasn't that's always true yes 
我认为这是一贯想法，虽然不一定总是这样（知秋注：因为有时候我们就想追求简单，可能这个简单需要花很多金钱，而也可以花很少的钱也可以做到这个效果，但做起来复杂，维护成本很高，长期来看就很贵），请讲
34:54 - 34:55
his statement is like
他想说的是
34.55-34.57
isn't it better for disaster 
对于容灾来说，这样是不是会更好
34:58 - 35:03
because well again if you're running like a five million dollar machine from IBM 
因为如果你运行的是IBM那里买的价值五百万美金的机器
35:03 - 35:05
you're not plugging into the wall outlet
你不会将它往墙上的插座上插
35.05-35.10
right you have generators ,you have backup power, right 
你会使用发电机和备用电源
35:10 - 35:14
but I would say that the issues really would be the never get severed, right 
但我要说的是有些问题永远不会被解决
35:14 - 35:21
 if the the network to the Machine, even then even then you'll still have redundant Knicks going into it 
假设，有人踹断了机器的网线
35:21 - 35:23
but even then if you can't communicate the database 
如果你无法和数据库通信
35:23 - 35:26
but potentially on how you design your distributed database system 
取决于你设计分布式数据库系统的方式
35:26 - 35:30
you could have the database spread across different data centers 
你可能会将数据库分散到不同的数据中心
35:30 - 35:32
And then you can still be available 
那么，你的数据库依然是可用的
35:32 - 35:35
well it discuss more of this on Wednesday 
我会在周三的时候对此深入讨论
35:36 - 35:45
but  this is one of the trade-offs you get between the NoSQL guys versus the traditional or NewSQL or relational database systems 
但这是NoSQL、SQL或NewSQL以及关系型数据库之间的其中一个权衡点
35:45 - 35:48
the NoSQL guys were caring about availability 
NoSQL那群人考虑的是可用性
35:48 - 35:51
so no matter what they wanted a website to be online and available 
So，他们希望网站是在线且可用的
35:52 - 35:57
and so in exchange they would give up transactions to make that happen
So，作为交换，他们放弃了对事务的支持，以达到这个目标
35:57 - 35.59
because if you have to do new transactions
因为如果你需要执行新事务
35.59-36.02
then that the communication is more expensive 
那么，通信成本则会更加昂贵
36:02 - 36:05
you make sure that everybody is up in order to make changes 
你要确保所有人都是在线的，以此来执行修改
36:06 - 36:09
and they argue that was less than ideal 
他们认为这种是不理想的
36:11 - 36:14
for some applications I think that makes sense， for anything financial that doesn't make sense 
对于某些应用程序来说，我觉得是有意义的，对于金融方面来说，我觉得没有任何意义
36:15 - 36:17
I will cover the next class 
我会在下节课的时候介绍
36:19 - 36:19
okay 

36:23 - 36:26
so distributed database system are old
So，分布式数据库系统其实很早就出现了
36:27- 36:31
some of the first ones were built in the late 1970s 
第一批分布式数据库系统是在1970年代末构建出来的
36:31 - 36:34
muffin was created by one of my advisers Michael Stonebraker,
Muffin是由我的指导教授Michael Stonebreaker所创造
36.34-36.38
 the guy who built Postgras and ingress and vertical and VOLTDB 
他也构建了PostgreSQL、Ingres、Vertical以及VoltDB
36:38 - 36:43
he had a system called muffin that was a distributed version of of ingress 
他写的这个叫做Muffin的系统其实是Ingres的分布式版
36:43 - 36:48
SDD-1 was a I actually thought it was actually a real system 
实际上，我原本以为SDD-1是一个真正的系统
36.48-36.49
turns out it was just a prototype
事实证明，它只是一个原型
36.49-36.51
they actually never actually had anything running 
实际上，从来没有机器运行过这个系统
36:51 - 37:00
but there's a lot of seminal papers in the late 70s written by the great Phil Bernstein on how to build a distributed database and do transactions across them 
1970年代末的时候，Phil Bernstein写了大量的paper，内容是关于如何构建分布式数据库，以及如何在它们之上执行事务
37:00 - 37:03
a lot of a transaction theory that we talked about in this class 
在这门课上，我们讨论了大量的事务理论
37:04 - 37:04
all right 

37.04-37.08
all that early work was done by Phil 
所有早期工作都是由Phil所完成
37:08 - 37:10
System R* was a research project at IBM
System R*是IBM的研究型项目
37.10-37.15
 that was the distributed version of System R
它是System R的分布式版
37:15 - 37:16
that never became a product
它并未成为一个产品投入使用
37.16-37.18
 although there is a distributor version of DB2 today 
虽然，当下有一个分布式版的DB2
37:18 - 37:22
Gamma was an influential system out of Wisconsin by David DeWitt
Gamma是一个极具影响力的系统，它是由威斯康星大学的David DeWitt所编写
37.22-37.26
that was one of the first like high-performance distributed Database systems 
它是第一批高性能分布式数据库系统中的一个
37:26 - 37:32
and then NonStop SQL of all these is the only was the only commercial distributed database system
接着，NonStop SQL是唯一一个商用分布式数据库系统
37:32 - 37:36
and that was that was helped built or Jim Gray helped build this 
Jim Gray帮助构建了这个系统
37:36 - 37:42
Jim Gray was the guy, who was at IBM invented like two-phase locking, and a lot of the early stuff, that we talked about under system R
Jim Gary在IBM的时候发明了两阶段锁，以及我们很多在讨论System R时说过的早期东西
37:43 - 37:45
so NonStop was an interesting company 
So，NonStop是一个令人感兴趣的公司
37:46 - 37.49
they originally were selling these like super fault-tolerant machines
他们最初对外出售的就是这种具备很强容错能力的机器
37.49-37.51
like think of like redundant hardware
把它想象成：冗余的硬件
37.51-37.53
like space shuttle level redundancy
比如，航天飞机级的冗余
37.53-37.54
like you have 4 CPU is running
比如，你有4个CPU正在运行
37.54-37.55
and if one goes down
如果其中一个CPU出现了故障
37.55-37.57
 the other three you can keep on running 
其他3个CPU依然能执行
37:57 - 38:00
so they would sell a database system that would sort of build on this architecture 
So，他们会销售一种基于这种架构的数据库系统
38:01 - 38:02
um it's still round today
当下依然有人使用它
38.02-38.069
a lot of financial systems actually still use this 
很多金融系统实际依然使用这种数据库系统
38:06 - 38:08
and it's amazing how long it still runs
它能运行这么长时间真的令人惊叹
38.08-38.09
 I guess it's nonstop right
我猜这和它的名字一样，真的是永不停止

38:11 - 38:13
all right

38.12-38.13
 so uh, all right 
++++++++++++++++++++++++++++++++++++++=++++++++
38.13-38.18
so now we now that we understand the the what the architecture looks like 
So，现在我们已经理解这种架构是怎么样的了
38:19 - 38:21
a lot of you have these questions that like
你们可能会有很多这样的问题
38.21-38.22
 hey, how is this thing actually gonna work
比如：这种东西实际是如何工作的
38.22-38.24
how to actually find data
它实际是如何找到数据
38.24-38.26
, how do we actually make sure that everything is consistent
我们实际如何确保所有东西都是一致的
38:26 - 38:31
so all these things we need to be mindful of now when we build a distributed database system 
So，当我们构建一个分布式数据库系统时，所有这些东西都需要我们去考虑
38:31 - 38:31
and there's trade-offs
这里面存在着取舍
38.31-38.33
 because we do not be able to do everything 
因为我们没法做到面面俱到
38:33 - 38:37
so we're not gonna have a system to be guaranteed online all the time
So，我们无法保证我们的系统始终在线
38:37 - 38:42
and make sure that we always support transactions and and not lose any data or have inconsistent results 
也无法确保我们始终支持事务，并且不丢失任何数据或者产生不一致的结果
38:43 - 38:44
so as we go along
So，随着我们的讨论
38.44-38.46
we'll see what these trade-offs are
我们会看到这些取舍是什么
38.46-38.49
and why you're not gonna achieve everything
以及为什么我们没法做到面面俱到
38:49 - 38:51
the other big question we're gonna have is
我们会遇到的另一个重要问题是
38.51-38.54
 how we actually execute the queries on this distributed data 
我们如何对这些分布式数据执行查询
38:55 - 38:57
and so I showed two examples so far 
So，目前为止我展示了两个例子
38:57 - 38.59
I showed the example on shared disk
我展示了一个关于shared-disk的例子
38.59-39.06
where the compute nodes pull the data from the shared disk system ,into their local memory and compute the result 
计算节点会从shared-disk系统中拉取数据并放入它们的本地内存，以及计算结果
39:07 - 39:11
and then in the case of the share nothing system 
接着，在这个share-nothing系统中
39:11 - 39:15
we would send the query to where the data was located run that locally
我们会将查询请求发送给数据所在节点进行本地计算
39.15-39.17
and then get back the result
然后返回结果
39:17 - 39:21
so there's trade-off between how you actually want to better doing a push or a pull
So，实际上，我们在Push和Pull之间要做取舍

39:22 - 39:24
so the last thing to talk about too is
So，我们最后要讲的事情是
39.24-39.26
what does the architecture look like
这种架构是什么样的
39.26-39.30
in terms of what are the nodes doing in the cluster for the distributed database 
分布式数据库集群中这些节点做什么呢
39:30 - 39:33
and there's basis you know there's just two approaches
这里有两种方案
39.33-39.36
you either have a homogeneous cluster or a heterogeneous cluster 
你可以使用Homogeneous Cluster，也可以使用Heterogeneous Cluster
39:36 - 39:37
so in a homogeneous cluster
在Homogeneous Cluster中
39.37-39.45
every single node in the database cluster is can perform every single kind of task you'd ever have 
数据库集群中每个节点可以执行你想要的任何任务
39:45 - 39:46
so I mean like
So，我的意思是
39.46-39.48
you could send a query to any single node 
你可以将一个查询发送给任意节点
39.48-39.52
and that node will figure out how to get the result that you're looking for 
该节点会弄清楚你所寻找的结果是什么
39:53 - 39:56
and they're all gonna give me potentially background tasks and other things 
然后它们会进行一些后台任务和其他东西
39:57 - 40:00
so the advantage of this approach is that
So，这种方案的优势在于
40.00-40.05
it makes provisioning and failover potentially easier to handle and support
它使故障预防和故障转移变得更易处理和更好地支持


40:05 - 40:11
because now I just add new nodes and long as I'm you know like I can move data around safely 
因为现在我可以添加新节点，并且我可以安全地移动数据
40:12 - 40:13
I can add new nodes 
我可以添加新节点
40.13-40.16
and there's you know the system gets stronger it gets better 
这让系统的可靠性变得更强，且性能更好
40:17 - 40:22
upon to a point which we'll see you next class 
我们下节课的时候会再看到这个
40:23 - 40:26
another approach to do heterogeneous cluster
另一种则是heterogeneous cluster方案
40.26-40.35
where you can have specific nodes or members of the database system be responsible for separate tasks 
数据库系统中的某个特定节点用于处理单独的任务
你可以给数据库系统中的某个节点分配指定任务
40:35 - 40:40
and so now I can make a decision say if my systems running slower， I want to add new nodes 
So，我现在需要去做出这样的决定，即如果我的系统运行速度变慢，我想去添加新节点
40:41 - 40:45
I have to know what I should add a node for this type of node or this other class of node 
我需要知道我应该添加的是这种类型的节点，还是另一种类型的节点
40:45 - 40:48
right I have to make a decision at that level 
我需要在这个层面做出决定

40:49 - 40:53
so give me an example of what one of these architectures 
So，这里我给出了其中一种架构的例子
40:53 - 40:54
I always like to use MongoDB 
我一直很喜欢用MongoDB作为例子来进行讲解
40.54-40.58
because that's the most basic one to understand
因为它最基础，理解起来也方便
40:58 - 41:01
so MongoDB uses what is known as a heterogeneous cluster architecture
So，MongoDB使用的是Heterogeneous Cluster架构
41:02 - 41:04
so you have special-purpose nodes
So，你拥有一些特殊用途的节点
41.04-41.07
 that are responsible or doing specific tasks in the system
它们负责处理系统中那些特定任务

41:08 - 41:11
so the application wants to send a request or execute a query
So 当应用程序想发送一个请求来执行一个查询时
41.11-41.14
 ,it always goes to this router 
它始终会跑到这个router处
41:14 - 41:21
and so the router looks at the request and says you know I want to look at I want to get record ID = 101 
该请求表示：我想去获取id为101的记录
41:22 - 41:23
these guys are stateless
这些东西是无状态的
41.23-41.27
they don't know about what any of the data is on the actual shards 
它们并不清楚这些分片上的数据是什么

41:27 - 41:29
so goes to this config server node 
So，它会跑到Config Server节点

41:30 - 41:40
that it's responsible for sending is sending out back the information about ,where to find data on these different partitions or these shards here 
该节点负责来回发送一些信息，这些信息描述了我们该在哪些分区中找到数据
通过Config Server节点可以得到一些信息，用来描述我们该去哪些分区中找到想要的数据
41:40 - 41:42
so that's all this thing does
So，这就是它所做的事情
41.42-41.47
this thing is responsible it is like a global State for what the configuration of the system is 
它就像是一个全局状态，即该系统的配置是什么

41:47 - 41:51
so now the router used gets this routing table from the config server 
So，router会使用它从Config Server中拿到的路由表
41:51 - 41:55
and then it can send the request to the MongoDB or the shard server 
接着，它可以将该请求发送给MongoDB或者某个Shard Server
41:55 - 41:59
and then that sort actually execute the query and gets back the result
接着，该Shard Server会执行查询，并返回结果
42:01 - 42:02
so under this architecture again
So，在这种架构下
42.02-42.07
if I notice that oh my router infrastructure is my bottleneck
如果我注意到我的router基础设施是我的瓶颈
42:08 - 42:09
that I can scale this thing out
那么，我可以对它进行扩展
42.09-42.15
and add more new nodes without touching the config server or the or the the shards servers
在不与Config Server或Shard Server接触的情况下，我可以添加更多的新节点
42:16 - 42:16
yes
请问
42:20 - 42:30
this question is what is able to tell so like garbage question we talk about MVCC, or building indexes ,or moving data around, 
这个问题就是说我能否在移动数据时候去做MVCC或创建索引之类的事情（知秋注：该问题无视就好）



42.28-42.30
because I'm scaling up or scaling down 
因为我正在对系统进行扩展或者缩小


42:34 - 42:36
again like you can't send a query to this guy here
你无法向Config Server这里发送一个查询
42.36-42.40
he can only tell you what the configuration of the system looks like
它只能告诉你该系统的配置是怎样的
42:40 - 42:42
and this guy can't hold any data
Router节点不保存任何数据
42.42-42.45
you can only tell you how to send your you know where to send your query 
它只是告诉你，你该将你的查询发往哪里

42:48 - 42:55
so the other thing we sort of briefly touched upon is about this notion of data transparency in a distributed DBMS
So，在分布式DBMS中，我们之前简单接触过的另一个东西是data transparency（数据透明度）这个概念
42:55 - 43:08
and that's where we don't want ideally the application to know anything about, how the data is is split up and divided or replicated across the different nodes in our cluster 
理论上来讲，我们不想让应用程序知道这些数据是如何拆分或者复制到我们集群中不同的节点处
43:09- 43:12
so the same SQL query or whatever query language I'm using
So，不管我使用的是什么查询语言
43.12-43.14
and my application for my database system 
对于我的数据库系统来说
43:14 - 43:20
I if I'm running on one node that same query should still work and still produce the correct sync same result 
不管我在哪个节点上执行同一个查询，它们生成的都应该是正确且同步的同一结果
43:20 - 43:23
if now I'm scaled out on a thousand nodes 
如果我扩展到1000个节点
43:24 - 43:27
because otherwise if I have a query says like you know a select star statement 
否则，如果我有这样一个查询，它要执行select *这种语句
43:27 - 43:31
and then you have like some special thing that says you know we're node equals 1、2、3
然后，你有这么一个特定的事情，我们要去找目标数据等于1、2、3的节点
43:31 - 43:35
if  1、2、3 gets now split up across multiple machines or not  1、2、3 goes away 
如果 1、2、3被分散在多台机器上
43:35 - 43:39
I don't want to go back and rewrite all all my application code 
我不想回过头去重写我全部的应用程序代码
43:39 - 43:43
so we're gonna hide all the details from the application
So，我们会将所有的细节对应用程序隐藏起来
43.43-43.46
where the data is actually being stored
即这些数据实际被存储在哪
43:46 - 43:51
although we can push some information to the client level at a driver 
虽然我们可以在驱动处推送一些信息
43:52 - 43:54
it allowed to figure out what node he wants to go talk to you 
这使得我们能弄清楚它想和哪个节点进行通信
43:54 - 44:00
but our application code, you know the Joe Schmoe programmer should not know anything about how the data is split up ideally 
但从理想情况来讲，程序员不应该知道数据是如何进行拆分的
44:01 - 44:02
it's not always the case
虽然不一定总是这种情况
44.02-44.03
but this is what we want 
但这是我们想要的东西

44:05 - 44:07
so now to talk about how we're gonna split the data up
So，我们现在来讨论下我们该如何拆分数据
44.07-44.09
 we've already sort of touched on this a little bit 
我们之前已经接触过一点了
44:09 - 44:11
we're going to use partitioning 
我们会使用partitioning这种方式
44:12 - 44:14
we talk about this as well when we did 
之前我们讨论过这个
44:16 - 44:20
I think was the timestamp ordering protocols talked about this 
我觉得我们在讨论timestamp ordering协议的时候，讨论过这个
44:20 - 44:22
and we talked about this with parallel execution
我们在讨论并行执行的时候说过这个
44:22 - 44:23
the idea here is that 
这里的思路是
44.23-44.27
we're going to take our database and split it up into disjoint subsets 
我们会将我们的数据库拆分为多个不相交的子集
44:28 - 44:31
that were then gonna assign to different resources 
接着，我们会将数据分配给多个不同资源
44:32 - 44:34
if you're coming from the NoSQL world
如果你们有NoSQL的背景
44.34-44.36
 they're going called a sharding
它们也被叫做sharding
44:36 - 44:38
but partitions and shards are essentially the same thing 
但partition和shard本质上来讲是一回事
44:40 - 44:42
so now what's gonna happen 
So，现在所发生的事情是
44.42-44.43
as the DBMS is going to get a query 
当DBMS拿到一个查询时
44:43 - 44:49
and it's gonna look at what data the different parts of that query plan need to access
它会去查看查询计划并根据不同部分的需要，去访问对应的数据
44:49 - 44:56
and then it may potentially need to send fragments of the plan to different nodes to go have them execute that part of the query 
它可能需要将该查询计划的不同部分发往不同的节点，来让这些节点去执行这部分查询
44:56 - 45:00
and then send back the result that they generated 
接着，这些节点会返回它们生成的结果
45:00 - 45:05
and we can use that same exchange operator we talked about ,before under the iterator model,when we did a parallel queries 
当我们执行并行查询的时候，我们会使用iterator model下我们之前讨论过的exchange operator
45:05 - 45:09
that same exchange operator is how we can parallelize things in distributed environment
在分布式环境下，我们可以通过同一个exchange operator来让查询并行化
45:11 - 45:13
so let's talk about how we actually split our tables up 
So，我们来讨论下我们实际该如何拆分我们的表


45:15 - 45:19！！！！
so the most simplest way to do table partitioning is
So，拆分表的最简单方式是
45.19-45.24
 you just take a single table and you have every single node, you have each node store one of those tables 
你让每个节点保存其中一张表
45:24 - 45:26
so I have three tables A B and C
So，假设我有3张表，即A、B和C
45.26-45.28
, node one gets a
节点1保存的是A表
45.28-45.29
node two gets B
节点2保存的是B表
45.29-45.30
node 3 gets C 
节点3拿到的是C表
45:30 - 45:32
that's the easiest way to do partitioning 
这是进行分区的最简单方式
45:33 - 45:37
alright for this one obviously have to assume that the table can fit on a single node 
很明显，在这种情况下，我们必须假设每个节点都有足够容纳一张表的容量

45:37 - 45:40
but for that it's fine
但对于这种情况来说，这没什么问题

45:40 - 45:42
so I have two tables one and two 
So，这里我有两张表，即表1和表2

45:42 - 45:46
I just take all again all the tuples in table 1 goes to one partition
我将表1中的所有tuple都放入一个分区中
45.46-45.48
 all the tuples in table 2 go to another partition 
我将表2中的所有tuple放入另一个分区
45:49 - 45:55
so the ideal query in this environment is any query that obviously touches one table 
So，在这种环境下，理想的查询显然是一个查询只会设计一张表
45:56 - 46:00
because now I don't need to communicate through between these different nodes
因为我现在就无须和这些不同节点进行通信
46.00-46.03
I just send my query to this one node， it runs and I send back the result 
我只需将我的查询放入这个节点，让它去处理这个查询，接着将结果返回给我
46:05 - 46:11
again I'll get parallelism assuming that I my workload is easily divided across these two two tables 
假设我的workload能很容易地被分散到这两个表上，我就能获得并行性
46:11 - 46:13
but we obviously know that's not always the case that's not realistic 
但很明显，我们知道我们不一定总能遇到这种情况，这不现实
46:15 - 46:18
so the only very few systems will let you do this 
So，几乎很少系统能让你做到这点
46:18 - 46:20
I know MongoDB can
我知道MongoDB可以做到这一点
46.20-46.23
MongoDB you can say in their world they called a collection instead of a table 
在MongoDB中，它们将table称之为collection
46:23 - 46:29
you can tell MongoDB store a table on this one you know on a single node by itself 
你可以告诉MongoDB将这张表保存在这个节点上
46:30 - 46:34
but this isn't that common in other systems 
但在其他系统中，这并不常见
46:36 - 46:36
yes 
请讲
46:39 - 46:40
this question is 
他的问题是
46.40-46.41
what are these partitions
这些分区是什么
46.41-46.43
doesn't matter
这不重要
46:45 - 46:46
for simplicity
出于方便起见
46.46-46.47
 assume is shared-nothing
假设这里使用的是shared-nothing架构
46:48 -  46:49
actually yeh 
实际上是
46:49 - 46:50
assume is shared-nothing
假设这里使用的是shared-nothing架构
46.50-46.52
 in a shared disk architecture
在shared-disk架构下
46.52-46.56
you don't necessarily have fine-grain control like this 
你不一定能做到这种粒度良好的控制
46:58 - 47:02
you could you basically you could to say in like s3 
基本上来讲，在S3这种东西中
47:02 - 47:05
you just have different buckets for different tables 
你可以用不同的bucket来保存不同的表
47:06 - 47:10
but you don't have any information where it's actually being stored 
但你并没有任何关于这些被表被存储在哪的相关信息
47:10 - 47:11
so assume this is shared-nothing 
So，假设这里使用的是shared-nothing架构


47:15 - 47:16！！！！！
what is more common
最为常见的方案是什么呢？
47.16-47.19
 we get most people think about in a distributed DBMS is to do horizontal partitioning
在分布式DBMS中，大部分人会选择去使用水平分区
47:20 - 47:23
for us again we're assuming we're doing a row-store system 
假设我们使用的是行存储数据库系统
47:23 - 47:27
so for this one we're gonna split the table up, row by row
So，我们会逐行对我们的表进行拆分
47.27-47.32
,by looking at one or more columns as the partitioning key 
我们会将一个或多个列作为partitioning key
47:32 - 47:35
and examining the value of those partitioning keys
通过对这些partitioning key的值进行测试
47.35-47.37
and then deciding what partition to assign it to
接着，我们会选择该将它分配给哪个分区
47:40 - 47:44
so again in a shared disk system ,so I shared nothing system
So，在shared-nothing系统中
47.44-47.45
 you do physical partitioning 
你可以使用物理分区
47:45 - 47:50
because every nodes gonna have actually store locally on at the local disk,its partition 
因为每个节点实际上会将它的分区保存在本地磁盘
47:51 - 47.53
and then in a shared disk system
接着，在一个shared-disk系统中
47.53-47.55
, you would do a logical partitioning 
你使用的是逻辑分区
47.55-48.02
where you fit you assign a compute node to be allowed to access a particular partition 
你会分配一个计算节点以让它来访问特定分区
48:02 - 48:08
so that you know you don't have a copy of the same page across multiple multiple nodes
So，这样你就无须将同一个page复制到多个节点上了
48.08-48.10
 to reduce the amount of coordination you have to do 
以此来减少你需要做的协调工作量

48:10 - 48:12
so let's look simple example it like this
So，我们来看个简单例子
48.12-48.15
 let's say that we select this column as the partitioning key 
假设，我们选择这一列作为partitioning key
48:16 - 48:18
and we're gonna do hash partitioning
我们会去使用hash partitioning

48.18-48.24
which is just we're gonna scan through and look at the value for every single tuple for this particular column 
我们会去扫描并查看这些类对应的每个tuple
48:24 - 48:28
and there's gonna hash it mod by the number of partitions we have 
我们会使用我们所拥有的分区数量来对它进行hash取模


48:28 - 48:32！！！！！
and then that will tell us where we're to actually want we want to go send the data 
接着，它就会告诉我们，我们实际该往哪个分区发送数据
48:33 - 48:40
so now if a query shows up and it's like you know select star from table where partition key equals some value 
假设我们有这样一个查询Select * From table Where partitionKey= ?
48:40 - 48:43
we just take that value running through our same hash function
我们会将这些值传入同一个hash函数
48.43-48.45
and now we know exactly where our partition is
现在，我们就会知道我们的分区是哪个
48:47 - 48:49
so this is hash partitioning 
So，这就是hash partitioning
48.49-48.51
you also you can do range partitioning 
你也可以去做Range partitioning
48:51 - 48.54
where which I've shown before 
这个我之前已经向你们展示过了
48.54-48.55
you basically say
简单来讲
48.55-49.00
,you know these contiguous segments of the value space column, goes to this partition 
目标列这段连续的行数据会放在这个分区
49:00 - 49:03
then the next we know 100 keys go to this next partition 
接下来100个key所对应的数据会放入下一个分区
49:03 - 49:07
and then same thing the query shows up you look at the value they're trying to do a lookup on
我在这里执行和刚才相同的查询
49:07 - 49:13
and I you know I know where to route the data that I want or go route the query to find the data where I want,
我知道该将我要执行的查询路由到哪个分区来找到我想要的数据
49.13-49.14
yes
请问
49:22 - 49:22
yes 

49:34 - 49:35
her question is
她的问题是
49.35-49.40
 just rephrase your question
我重新组织下你的问题
49:40 - 49:44
selecting what partitioning key to use is actually an np-complete problem
选择使用哪个partitioning key其实是一个np-complete问题
49.44-49.45
because there are so many different combinations I could do 
因为我可以做很多不同的组合
49:46 - 49:47
how do I know what to do 
我该如何知道我该怎么做呢？
49:48 - 49:52
so this is something I actually have done research on 
So，其实我在这方面已经做了一些研究了
49:52 - 49:58
there's like a forty fifty year history of people developing different methods ,and algorithms to pick the partitioning key 
关于选择partitioning key这方面，人们开发了不同的方法和算法，这已经有四五十年的历史了
50:00 - 50:03
again my adviser's adviser wrote one in the 70s
我指导教授的指导教授在1970年代的时候写过一个
50.03-50.03
and he's dead
他现在已经去世了
50.03-50.06
 I wrote one, right 
我也写了一个
50:06 - 50:08
it's basically it's like a search optimization problem
简单来讲，它就像是一个查找优化问题
50.08-50:13
I look at my workload I see how I'm accessing my queries are accessing the table
通过查看我的workload，我能知道我该怎样让我的查询去访问这个表

50.13-50.17
and I'm seeing this thing you know partition key, something equals something over and over again 
我会去看这个partition key，它等于多少，然后通过相应的算法得到想要数据的目的地
50:17 - 50:19
then that's obviously the one I want to choose 
显然，这是我想去使用的东西
50:19 - 50:21
for OLTP applications
对于OLTP应用程序来说
50.21-50.24
oftentimes you can we'll talk about next class 
我们会在下节课的时候讨论
50:24 - 50:26
you can almost develop like a tree schema
你们可以去开发一种树形图
50.26-50.31
and identify like brain or passed down to the tree that you then split everything up 
通过将数据传递到树上，然后我们将这些数据拆分开来
50:31 - 50:32
so for example
So，例如
50.32-50.42
 like say Amazon divides up its database based on like state ,where the customer is located
Amazon基于他们用户所在的位置来对它的数据库进行拆分
50:42 - 50:44
so here's all the customers in Pennsylvania 
So，比如这里是宾夕法尼亚的所有用户
50:44 - 50:48
and then here's all the orders for the customers in Pennsylvania 
接着，这里是宾夕法尼亚用户的所有订单
50:48 - 50:50
here's all the items that they bought in Pennsylvania 
这是宾夕法尼亚那批人所购买的所有商品
50:51 - 50:56
so I can take all of the Pennsylvania customers ,and put them in one partition 
So，我可以将所有宾夕法尼亚的用户放在一个分区中
50:56 - 50:59
all the the Maryland ones going on another partition 
所有马里兰州的用户放到另一个分区中
50:59 - 51:03
so it's a lot of times it's sort of obvious what that key should be 
So，显然，在很多情况下，它的partitioning key就是根据用户所在州来选的
51:04 - 51:05
for OLTP
对于OLTP来说
51.05-51.06
 for OLTP more tricky
对于OLTP来说，这更为棘手
51.06-51.08
 you definitely have to look at the queries what the queries are 
你肯定会去查看这些查询是什么
51:08 - 51:12
because again you want to minimize mount of coordination or data you're sending between different partitions 
因为你想最小化你在不同分区间所做的协调工作量或者是发送的数据量
51:13 - 51:13
yes 
请问
51:24 - 51:25
question is 
他的问题是
51.25-51.27
if we have an index on the partitioning key
如果我们在partitioning key上制作了索引
51.27-51.30
will this have an impact on the design
这对设计是否存在影响
51.30-51.32
I mean the selection of the partitioning key 
我的意思是对partitioning key的选择上是否有影响
51:33 - 51:34
What do you mean my design 
你所谓的设计是什么
51:47 - 51:48
alright so this question is 
So，他的问题是
51.48-51.50
will get that 
我们稍后会讲
51:51 - 51:51
this question is
他的问题是


51.51-51.53
 this is my query my application sends us 
这个是我应用程序发给我们的查询
51:54 - 51:58
how do I know that where to go what partition has the data I'm looking for
我该如何找到保存着我要查找数据所在的那个分区
51:59 - 52:02
like how does it know that it uses hash function and send the query 
它该如何知道它要去使用hash函数找到这个分区，并往这个分区发送查询呢？
52:02 - 52:05
so if it's a heterogeneous system
So，如果这是一个heterogeneous系统
52.05-52.08
 you could have a front-end query router like a Mongo did
你可以使用一个像Mongo那样的前端查询路由
52:08 - 52:11
it say oh I know the sharding key is this thing here 
它表示，它知道这个sharding key是什么
52:12 - 52:14
so let me go pick that out of the query hash this value 
So，我会拿到这个查询，并对它进行hash
52.14-52.16
and then I say that's where I want to go 
然后，我就会告诉它该去哪个分区
52:16 - 52:20
if it's a shared nothing system with the homogeneous architecture 
如果它是一个使用homogeneous架构的shared-nothing系统


52:21 - 52:23
you could say I land on p1
假设，我落在了P1这里
52.23-52.24
 ,p1 says 
P1表示
52.24-52.27
oh you won't execute this query ,but I don't have this data, p3 has it 
你无法在这里执行这个查询，因为我没有相关数据，但P3有你要找的数据
52:27 - 52:28
so there's route your query for you
So，它就会帮你将这个查询路由到P3
52.28-52.32
 or it sends your query down here runs it and then sends back the result through p1 
或者，它会将这个查询发送到P3，然后让P3执行这个查询，接着让它通过P1返回结果
52:32 - 52:33
there's different ways to do this 
我们可以不同的方法来做到这一点
52:36 - 52:36
all right 

52.36-52.41
so I'm showing hash partitioning here 
So，这里我展示了hash partitioning
52:42 - 52:46
right we just take the hash value mod by the number of partitions I have 
我们根据我们拥有的分区数量来对partitioning key进行取模
52:46 - 52:48
and that tells me where I need to go
这就会告诉我们该去哪个分区
52.48-52.49
what's the problem of this  
这样做的问题是什么
52:57 - 52.59
this is collision
会产生hash碰撞？
52.59-53.00
ignoring collision
这里我们忽略掉hash碰撞这种问题
53.00-53.00
 assume we have a good hash value
这里我们假设我们有一个不错的hash值
53:05 - 53:06
so he says well 
So，他说的是
53:09 - 53:10
yeah so if you do hash partitioning
So，如果你使用的是hash partitioning
53.10-53.12
 if you do sequential scan
如果你进行循序扫描
53.12-53.15
 like if this is a range predicate set equality predicate
如果你进行范围判断，或者等价判断
53.15-53.16
 hash partition is a bad idea 
那么，hash partitioning就是一个糟糕的想法
53:16 - 53:18
because I can't hash a range just the same  of the hash table 
因为我无法对一个范围进行hash，并将这个范围中的数据都放入同一个hash table中

53:19 - 53:20 
but not something else 
还有其他问题吗？
53:26 - 53:29
so his question is if I update the partition key 
So，他的问题是，如果我更新partitioning key
53:30 - 53:33 ！！！！
right if I said a partitioning on this column， I partition this column
假设我根据这个列进行分区
53.33-53.34
 I got to move everything around 
我就需要去移动所有数据
53:34 - 53:36
yes but that doesn't happen that often 
确实如此，但这种情况并不会经常出现
53:37 - 53:41
right like like think about like your your your Amazon account ID 
思考下你Amazon的账户id
53:42 - 53:45
that they're not gonna say ,all right we're not partitioning on that 
Amazon并不会说，我们根据你的账户id进行分区
53.45-53.47
and what were partition other thing like your email address
它们会根据你的email地址进行分区
53.47-53.48
that rarely ever happens 
这基本不会发生什么变化
53:52 - 53:53
Bingo
Bingo
53.53-53.54
so he says 
So，他表示
53.54-53.57
if I had a 5th partition here 
如果这里我有第五个分区
53:57 - 54:00
I have that same problem I had when I was talking on hash tables
我就会遇上我们在讨论hash table时所遇到的相同问题
54.00-54.02
and I see why we have to talk about the single node stuff first 
为什么我们必须先讨论单节点的情况呢？
54:02 - 54:04
if I had a 5th partition here
如果这里我有第五个分区
54.04-54.06
 , now if I rehash all the values and modify five 
如果我重新使用5来对所有的值进行hash
54:07 - 54:08
they're not guaranteed to be the same partitions 
这就无法保证，这些数据依然是在同一个分区中
54:08 - 54:14
I may end up moving the entire database everyone might be swapping and moving to another location 
我可能会将整个数据库原本的分区交换并移动到另一个位置
我可能最终会移动整个数据库中的每一条数据，可能将在这个分区的这条数据交换并移动到另一个分区中
54:15 - 54:16
so that's bad
So，这就很糟糕
54:18 - 54:20
so we need a way to handle that 
So，我们需要某种方式来处理这个问题
54:21 - 54:23
who here has ever heard of consistent hashing
在座的有谁之前听说过一致性hash吗？
54.23-54.25
very few
看来没几个人听过
54.25-54.26 
good ok perfect
Ok，Perfect 

54:26 - 54:30
so consistent hashing was a technique developed in the early 2000s 
So，一致性hash是2000年代早期所开发的一种技术
54:30 - 54:38
and the way it basically has to do it allowed to do incremental updates and removals of partitions in your cluster, without having to move everything around 
简单来讲，它允许在不移动任何东西的情况下，你能对你集群中的分区进行增量更新和移除

54:39 - 54:41
so the way to think about this is 
So，思考它的方式是
54.41-54.045
that the hashing space is just a ring, 0 to 1 
这个hash空间其实就是一个环，从0到1

54:45 - 54:49
and so I'm gonna have say three partitions a B and C 
So，假设我有3个分区，即A、B和C
54:50 - 54:51
so the way to think about this is
So，思考它的方式是
54.51-54.52
 like if I hash now key  
假设我对key进行hash
54:53 - 54.55
and I don't modify the number of partitions
我无须去修改分区的数量
54.55-54.56
 I just hash it 
我只需对它进行hash即可
54.56-54.58
,and say you put it between 0 and 1
并将它放在0和1之间


54.58-55.00
said it land at this point in the ring 
假设，它落在这个环的这个位置

55:00 - 55:08
so then I travel forward going clockwise motion, until I find the node that has the first node that shows up 
So，我按照顺时针的顺序遍历这个环，直到我找到第一个拥有该数据的节点
55:08 - 55:10
and that's where I know my data it's gonna be located 
这样我就知道我的数据是在哪个节点上的了
55:11 - 55:12
so I hash it 
So，我对key进行hash
55.12-55.12
I get a value
我得到了一个值
55.12-55.14
 put it between 0 & 1 
并将它放在0和1之间
55:14 - 55:19
and I know that in between this you know from doing this and this is A so the data I want is on a 
我知道0和1之间存在着一个节点A，我想要的数据就是放在A之上的

55:21 - 55:22
right same thing over here I hash 2,
这里同样如此，我对key2进行hash
55.22-55.25
 I land here  so we're in the ring space, 
我落在了这个环的这个位置
55.25-55.26
and I jump here to go to C
我跳到了C这里

55:27 - 55:35
so again the way to the keyspace for all these guys is from the where on partition starts back into the next partition 
这些节点之间的空间（keyspace ）就是从上一个分区开始到下一个分区之间的空间（知秋注：比如，A没了，就hash结果就落到了B上，B没了，hash结果就落到c上，而hash的结果落在b与c之间的话，那就存在c上面，注意，是顺时针操作）
55:37 - 55:38
right that's fine
这没问题
55.38-55.40
 that's not so great
只是没那么好而已
55:40 - 55:41
what matters now is that 
这里重要的地方在于
55.41-55.42
when I add new nodes 
当我添加新节点时
55:43 - 55:48
again say my distributed DBMS can't keep up with the traffic I'm trying to support 
当我的分布式DBMS无法处理那些流量时，我想让它去处理
55:48 - 55:50
so I want to add new machine it's and scale out 
So，我想去添加新机器，以此来对它进行扩展

55:50 - 55:52
so let's say I had a new partition here D 
So，假设这里我有一个新分区D
55:53 - 55:57
so if I was doing the static hashing technique that I showed in the last slide 
So，如果我使用前一张幻灯片中的static hashing技术
55:57 - 56:00
then I add now a  fourth partition
接着，现在我要添加第4个分区
56.00-56.04
and I got a  rehash and mod by 4 now everybody 
那我就需要使用4来对所有数据条目进行重新hash
56:04 - 56:06
and we had to move potentially move all of data around 
我们就可能需要移动所有的数据
56:06 - 56:10
but the way because system hashing works is that, I add my guy into the ring here 
但使用这种一致性hash的话，我将服务器添加在环中的这个节点处

56:10 - 56:13
and now the only thing I need to transfer is 
我唯一需要转移的东西是
56.13- 56:24
whatever C used to have where d is located， so it's just this part here of all the batteries that are in this partition that would be covered by this part of the Ring 
B与D这个区域里的hash结果所对应的数据条目，需要从c转移到d上

56:25 - 56:26
I send them down 
我把它们发送到下面这个d分区
56:27 - 56:30
and everybody else in my cluster stays where they are at 
我集群中的其他分区中的数据依然还在它们原来的位置

56:32 - 56:34
so I can add new part you know getting new partitions
So，我可以添加新的分区
56.34-56.39
 and they just update the ring and add a new space in 
它们只需去更新这个环，并往这个环中添加新的空间
56:39 - 56:40
and likewise 
同样
56.40-56.41
if I take a partition away
如果我移除一个分区
56.41-56.44
then anything here just goes up to where C was 
那么原本在D分区中的数据就会转移到C分区上面
56:47 - 56:50
so it's really interesting about this technique as well is the way to do replication
我们对这种技术感兴趣的另一个地方在于replication这块
56:51 - 56:52
okay and we'll cover more this next class 
Ok，我们会在下节课的时候，对此介绍更多内容

56:53 - 56:55
but let's say I want to do a replication factor of 3 
但假设，这里的replication factor的值是3
56:55 - 56:59
so for every single tuple, I insert to my database 
So，对于我往数据库中插入的每个tuple来说
56:59 - 57:04
I wanted to be replicated on three different nodes or three different partitions 
我想让这个tuple复制到3个不同的节点或者分区上
57:04 - 57:06
so that way if one of them goes down
So，如果其中一个分区挂掉了
57.06-57.08
I have two others available for me 
对我来说，那么还有2个分区可用
57:09 - 57:12
that can serve as a backup  and my database system go down 
当我的数据库挂掉了，它们可以作为备份使用



57:12 - 57:15
so now say I'm replicating a
So，假设我对A进行复制
57.15-57.18
and I'm gonna replicate it on three nodes 
我会将它复制到3个节点上
57:18 - 57:20
so I have on an a councils one then two and three 
So，我会在A、F和B这三个节点上保存着A分区那份数据
57:21 - 57:25
so any write to a any key that was an a is also going to be on F and B 
So，对任何A分区中的key所对应数据所做的修改，也会应用到F分区和B分区上

57:26 - 57:30
so now when my query shows up same thing I hash to this point in the ring
So，当我要执行查询的时候，我会将它hash到环中的这个地方

57:31 - 57:34
and I can get it from either A F or B
我可以从A或F或B分区中获得我要的数据
57:36 - 57:42
and I'm there it's guaranteed to be there, assume you're doing transactions we'll talk about next class 
假设，你在执行事务，它保证数据是在这些地方的一致性，这个我们会在下节课的时候讨论
57:42 - 57:49
so this now actually gets into the consistency issue that we sort of glossed over, and we talked about transactions before and talked acid 
实际上，这就会涉及我们之前讨论事务和ACID时所略微提到的一致性问题
57:49 - 57:51
right if I do a write on A
So，如果我对A分区进行写入操作
57.51-57.56
how do I know that it's been propagated to F and B 
我该如何知道这个写操作已经被传播到F和B呢？
57:56 - 58:00
well I you have to wait until they all acknowledge that they got the write
Well，我们必须等，直到我们收到它们已经拿到这个写操作的通知
58:00 - 58:01
which could be bad 
这可能有点糟糕
58.01-58.02
because one of these guys could go down 
因为其中一个分区可能会挂掉
58.02-58.06
when I'm waiting for the acknowledgement ,and I'm stalling 
当我正在等待这些通知的时候，我就会停在那里
58:06 - 58:07
or I say I don't wait
或者，假设我不想进行等待
5807-58.08
, but now I have this issue 
但现在我遇上了这个问题
58.08*58.10
where I may do a write on A 
我可能要对A分区进行写操作
58:11 - 58:13
and then immediately try to read that thing on B 
接着，立刻去试着读取B分区中的数据
58.13-58.16
and I might not C what I expect to C
这可能并不是一致的，但我希望它是一致的
58:16 - 58:19
again so this will we'll cover this more in next class 
So，我们会在下节课的时候对此进行更多介绍
58.19-58.20
, but this is the consistency 
但这是一致性方面的问题
58:20 - 58:22
this is the C in acid
这就是ACID中的C，即一致性
58.22-58.24
 that I said we were gonna gloss over for single-node databases 
我之前说过，我们会在单节点数据库中对此（知秋注：一致性）有所接触
58:24 - 58:26
but matters in distributed databases 
但在分布式数据库中，它会变得更为重要
58:27 - 58:29
so consistent hashing is a really cool technique
So，一致性hash是一个很Cool的技术
58.29-58.31
and it's actually used in some distribute databases 
实际上，在某些分布式数据库中就使用了它


58:32 - 58:33
so the three most famous ones
So，这里有三个最为知名的例子
58.33-58.36
 are MemcacheD
其中一个就是MemcacheD
58.36-58.37
 which is that caching service
它是一个缓存服务
58:38 - 58:40
Cassandra and dynamodb
接着就是Cassandra和DynamoDB
58.40-58.45
like dynamodb I think was that had the first paper discussed an architecture using this 
我认为DynamoDB这篇paper是最先讨论使用一致性hash这种架构的paper
58:45 - 58:53
and then at Facebook the one of the cofounders of cassandra ,he saw the DynamoDB paper that was a good idea，started building Cassandra at Facebook
然后，Facebook里面其中一个Cassandra的联合创始人读了这篇DynamoDB的paper，他觉得这是一个好想法，并着手将这个想法用在了构建Facebook的Cassandra上
58:53 - 58:54
facebook says
facebook表示
58.54-58.56
we actually don't need this anymore
我们实际上不需要这个
58.56-58.57
and they decided not to use Cassandra 
他们决定不使用Cassandra
58:57 - 59:00
so then they just open sourced it ,and put it out there 
So，接着，他们就将Cassandra给开源了
59:00 - 59:05
and then that people picked it up and started making Cassandra actually be you know a quality system 
然后，人们就将它捡起来，然后让Cassandra变成一个有质量的系统
59:06 - 59:09
so these probably three most famous systems that use this consistent hashing technique
So，这可能是使用一致性hash技术里面最为知名的3个数据库系统


59:11 - 59:19
we have brieflytalked  about what the distinction is between logical partitioning and physical partitioning 
我们已经简单讨论过逻辑分区和物理分区之间的区别了
59:19 - 59:21
so again this the idea is the same
So，这里的思路是相同的
59.21-59.27
 that you have this hash function or range function ,that allows you to divide up the database to disjoint subsets
你可以通过这种hash函数或者range函数来将数据库拆分为不相交的子集
59:27 - 59:29
but under the shared-disk system
但在shared-disk系统中
59.29-59.31
you have to do logical partitioning 
你需要做逻辑分区
59:31 - 59:37
because you don't have control over how the data is actually being written to the shared disk thing 
因为你不需要去控制数据是如何写入到共享磁盘的
59:37 - 59:38
right Amazon controls this
Amazon会负责控制这个
59.38-59.39
you don't
你不需要去控制这个

59:40 - 59:42
so the basic ways where it works is that
So，它的基本工作方式是
59.42-59.48
you assign some portion of the database to these different compute nodes
你会将数据库中的某一部分按某种算法分配给这些不同的计算节点

59:49 - 59:51
so that again the application server knows that
应用程序服务器知道
59.51-59.52
 if I want execute to query
如果我想去执行一个查询
59.52-59.56
here's the machine to go get you know go to run it
这个机器会去执行这个查询

59:57 - 01:00:00
right likewise from down here he's responsible for 3
同样，下面这个节点负责处理id等于3这个记录



01:00:01 - 01:00:04
shared-nothing systems are running when you do physical partitioning 
在shared-nothing系统下，你使用的是物理分区

01:00:05 - 01:00:13
again this is where you have the each node is assigned a portion of the data that's managed by partition
每个节点会管理着数据库中属于自己这一份的分区数据

01:00:13 - 01:00:18
so again same thing I know how to get the data that I'm looking for from these different nodes 
So，同样，我知道该如何从这些不同节点处获取数据
01:00:21 - 01:00:22
alright so we have like ten minutes left
So，我们还剩10分钟左右
1.00.22-1.00.23
so let's finish up
So，让我们赶快结束吧
1.00.23-1.00.25
and then that'll set us up for Wednesday's class
接着让我们为周三的课做好点准备


01:00:27 - 01:00:29
so when we want to start actually transactions
So，当我们想开始执行事务的时候
1.00.29-1.00.30
 this is when things get hard 
这也是这些东西变得很难的时候
01:00:31 - 01:00:33
and this one things get expensive
这些东西的成本就会变得更高
01:00:33 - 01:00:36
this is why I see her question is, her question before was
这也与她之前提的问题有关
1.00.36-1.00.39
Oh doesn't always make sense maybe try to scale vertically
垂直扩展并不是一直很有用
1.00.39-1.00.42
why would you ever want to scale horizontally 
为什么我们想使用水平扩展呢？
01:00:42 - 01:00:45
there are gonna be just as I said if there's diminishing returns 
正如我说的，使用垂直扩展所给我们带来的收益会递减
01:00:45 - 01:00:46
if you scale vertically
如果你进行垂直扩展
1.00.46-1.00.47
 the hardware can't actually get any better 
硬件性能实际也不会变得更好
01:00:47 - 01:00:51
because you can't buy a machine that gets in you know there's immediately faster 
因为你无法买到那种可以让你系统立刻变得更快的机器
01:00:51 - 01:00:53
it's also assumed your software can actually scale
这里我们假设你的软件也能进行扩展
1.00.53-1.00.59
and it's not gonna be plagued by concurrency bottlenecks and other things 
并且它并不会因为并发瓶颈和其他事情而困扰
01:00:59 - 01:01:01
if you're not scaled horizontally
如果你不进行水平扩展
1.01.01-1.01.04
then you're also gonna have diminishing returns and performance gains 
那么你也会遇上收益递减，虽然性能会有所提升
01:01:04 - 01:01:08
because now you're gonna end up with what are called distributed txn
因为现在你将会遇上分布式事务的情况
01:01:08 - 01:01:12
so if I have something that has to update data on a single-node 
So，如果我要对单节点上的数据进行更新
01:01:12 - 01:01:13
we know how to do that 
我们知道该怎么做
1.01.13-1.01.14
we've card an entire semester about this 
我们这整个学期都在讨论这个
01:01:14 - 01:01:23
and that's gonna be the best-case scenario where my transaction ,that I needed mean to touch data it's all in a single-node 
最好的情况是，我事务需要接触的数据都放在一个节点上

1.01.21-1.01.23
I can run that with ever without having to correlate with anybody else 
在不与其他节点接触的情况下，我就可以进行更新
01:01:24 - 01:01:26
if I need to touch data across multiple nodes
如果我需要接触的数据跨多个节点
01:01:26 - 01:01:31
then now I need a way to make sure that if I make a write here and I make a write here
那么，我现在需要通过某种方式去确保，如果我在这里执行了一个写操作，另一个地方我又执行了一个写操作
01:01:31 - 01:01:32
when my transaction says commit 
当我的事务要提交的时候
1.01.32-1.01.34
that it actually does commit 
它实际上就会进行提交
01:01:35 - 01:01:39
because I only make sure that all my change their atomic and durable just as just as I was a single-node system 
因为我只确保我所有的修改都具备原子性并且被持久化，这就如同我在单节点系统中所做的那样
01:01:40 - 01:01:41
and that's gonna get expensive
这样成本就会变得很高
1.01.41-1.01.42
because how do I make sure that
我该如何确保这个呢？
1.01.42-1.01.44
if I say I commit
如果我说我要提交事务
1.01.44-1.01.46
 then everyone actually truly commits
那么实际上所有人都应该进行提交


01:01:49 - 01:01:54
so the way we can do this is through a transaction coordinator 
So，我们可以通过事务协调器来做到这点
01:01:55 - 01:01:58
so you sort of think of this is like a traffic cop for the entire system 
So，你可以将它想象为整个系统中的交警
01:01:58 - 01:02:05
it allows a way to determine who's allowed to do what ,and when it goes time to commit 
它以某种方式来决定允许谁去做哪些事情，以及什么时候去提交事务
01:02:05 - 01:02:08
that everyone agrees that we're actually going to go ahead and commit 
当所有人都赞同的话，我们就会去进行提交
01:02:09 - 01:02:12
so the two different approaches are do centralized decentralized 
So，这里有两种不同的方案，即中心化和去中心化
01:02:12 - 01:02:13
a centralized one is
中心化方案指的是
1.02.13-1.02.15
where everyone goes to some centralized location
所有人都会跑到某个中心
1.02.15-1.02.19
 that has a complete view of everything going on inside the system 
这个地方能看到系统中所发生的一切
01:02:19 - 01:02:21
and then it makes decisions about whether you're allowed to commit 
它会去判断是否允许你进行提交
01:02:22 - 01:02:23
and it is decentralized approach 
接着，就是去中心化方案
1.02.23-1.02.29
where the nodes try to organize themselves, and make a decision about, yes we this this transaction made these changes 
这些节点会去组织它们自己做出判断，比如这个事务是否能做这些修改
01:02:29 - 01:02:30
and we're allowed to commit 
我们是否能提交该事务
01:02:30 - 01:02:35
and we can notify whoever else is involved in the transaction that they've committed successfully 
我们可以通知涉及该事务的其他人，他们已经成功提交了这个事务

01:02:36 - 01:02:43
so the very first version of one of these transaction coordinators was this thing called a TP monitors from the 1970s to 1980s
So，这些事务协调器最开始是出现在1970年代到1980年代，当时其中一种事务协调器叫做TP monitor
01:02:43 - 01:02:45
nowadays I think if you look at the Wikipedia article,
现在，我相信如果你们去看下维基百科上的文章
1.02.45-1.02.48
 TP stands for a transaction processing monitor 
TP monitor的意思是事务处理监视器
01:02:48 - 01:02:50
back in the 70s
在1970年代的时候
1.02.50-1.02.53
they call these things telecom processing monitors 
他们将这个叫做电信处理监视器
01:02:53 - 01:02:57
because these things are built for like the early you know the phone companies that come back in the day 
因为这些东西当时是为了那些电话公司所构建的
01:02:57 - 01:02.59
because they were the ones that had most of the traffic
因为他们当时掌握了大部分的流量
1.02.59-1.03.02
 I mean you know most of the data 
我的意思是大部分数据
01:03:02 - 01:03:07
so the way to think about this TP monitor is that
So，我们思考这种TP monitor的方式是
1.03.07-1.03.09
 it's the standalone piece of software 
它是软件中独立的一部分
01:03:09 - 01:03:17
that everybody has to talk to in order to figure out whether they're  do certain operations on a distributed database 
所有人都必须和它进行通信，以此来弄清楚它们是否要在分布式数据库中执行某个操作
01:03:17 - 01:03:21
so the database system itself could be stored across different nodes 
So，数据库系统自身可能被存储在不同节点上
01:03:21 - 01:03:25
and they don't really know that they're actually involved in the distributed transaction or distributed database 
它们并不清楚它们实际参与了某个分布式事务或者分布式数据库
01:03:25 - 01:03:30
if you just take MySQL whatever single-node system you want run that separately 
这就好比你使用了MySQL之类的单机数据库系统，单独执行这些事务一样
01:03:30 - 01:03:31
and then up above you have this TP monitored 
接着，在此之上，你会拥有一个TP monitor
1.03.31-1.03.33
allow you to figure out ,whether you're allowed to do certain things 
它会让你去弄清楚你是否能做某些事情

01:03:35 - 01:03:36
so it looks like this right 
So，它看起来就像是这样
1.03.36-1.03.37
so we have application server
So，我们有一个应用程序服务器
1.03.37-1.03.38
, we have four partitions 
我们有4个分区
01:03:39 - 01:03:43
so say your transaction touch these three partitions 
So，假设你的事务要涉及这3个分区

01:03:43 - 01:03:50
so we're gonna begin our transaction by going to coordinator, and say hey we want to modify some data at these partitions 
So，我们会开始执行我们的事务，我们会先跑到协调器这里，并说：我们想修改这些分区中的数据

01:03:50 - 01:03:53
we need to acquire the locks for them are we allowed to do that 
我们会去获取它们所对应的lock，以获取我们对它们进行修改的权限
01:03:54 - 01:03:55
and then the coordinator says
协调器表示
1.03.55-1.03.57
 well I know it's where else is running the system
Well，我知道它在系统中的哪个地方
1.03.57-1.03.58
because everyone has to go through me 
因为所有人都必须经过我才行
01:03:59 - 01:04:00
yes well I see these locks are available
事务协调器表示：Well，我看了下，这些lock都是可用的
1.04.00-1.04.01
so I'm gonna assign them to you
So，我会将这些lock分配给你

01:04:02 - 01:04:05
And then tell you that you've acquired them 
接着，事务协调器会告诉你，你已经获取到这些lock了
01:04:05 - 01:04:13
and then now the application server can go to the different partitions ,do whatever it is that wants to do to make the changes it wants to make 
那么，应用程序服务器可以跑到这些分区去做它想做的那些修改
01:04:13 - 01:04:15
and then when it wants to go to ahead and commit
接着，当它想去提交事务的时候
1.04.15-1.04.18
 it goes to the coordinator ,and says hey I want to commit I made these changes at these partitions
它会跑到事务协调器这边，并说：hey，我想提交该事务，我在这些分区做了这些修改
01:04:19 - 01:04:21
 am I allowed to do this 
我是否能提交该事务呢？

01:04:21 - 01:04:25
and the coordinator is responsible for going and communicating with these guys down here and say
事务协调器会负责与下面这些家伙们（知秋注：事务在各个分区服务器执行的相关方）进行通信，并说
1.04.25-1.04.28
hey I think you know about this transaction 
hey，我觉得你们知道这个事务
01:04:28 - 01:04:29
because it told me it was gonna touch you
因为它告诉我，它和你们是一起的
1.04.29-1.04.31
 did it actually do anything 
它实际是否做了一些事情呢？
01:04:31 - 01:04:37
and then they come back and say, yes you know these changes were what happened ,and they're okay or safe to commit 
这些分区就会表示：Yes，确实有这些修改，它们是Ok的，你可以安全地提交这个事务
这些分区会响应信息，即这些修改都已就绪（包括我们自己的），可以安全地提交这个事务了

01:04:38 - 01:04:43
and then once we everybody agrees once the coordinator recognized that everyone agrees that we can go and commit 
一旦事务协调器意识到所有人都同意我们进行提交，那我们就可以进行提交
01:04:43 - 01:04:44
we can send back acknowledge
我们就可以去返回提交成功的通知
1.04.44-1.04.45
question 
有任何问题吗？
01:04:49 - 01:04:49
So question is
So，他的问题是
1.04.49-1.04.52
 I know what scenario would it be not safe to commit 
在什么情况下，提交事务是不安全的
01:04:52 - 01:04:57
so let's say I violate a you know integrity constraint here ,
So，假设这里我违反了完整性约束
1.04.57-1.04.59
my transaction aborts 
我的事务就中止了
01:04:59 - 01:05:02
right I try to insert a duplicate key 
这里，我试着插入一个重复的key
01:05:02 - 01:05:03
the coordinator doesn't know what you did
事务协调器并不清楚你做了什么
1.05.03-1.05.05
 it says hey I want to acquire the locks on these things 
它表示：hey，我想去获取这些东西对应的lock
01:05:06 - 01:05:08！！！！！！！！
and I want to commit in a distributed fashion 
我想在分布式环境下提交这个事务
01:05:09 - 01:05:12
you have to go ask them whether they were allowed to do that 
你需要去询问它们（是否可以提交事务），是否允许这么做
01:05:14 - 1.05.17
for simplicity  his question are we like the whole partition simplicity, yes 
他的问题是我们该怎样做才能更简单


01:05:20 - 01:05:24
right I think it's like the XA if there's a protocol that allows you do more fine-grained locking
如果这里有个协议能让你使用粒度更细的lock
1.05.24-1.05.27
just stupid partitions makes it simple 
你就可以减少工作量，整个也会变得简单

01:05:31 - 01:05:31
Okay

1.05.31-1.05.43
so again there's a bunch of a lot of the enterprise ,software vendors, sell you something that that is a TP monitor
So，这里有很多企业或者软件厂商会向你销售TP monitor之类的东西
01:05:43 - 01:05:44
Oracle has this thing called tuxedo 
Oracle有一个叫做Tuxedo的东西
01:05:44 - 01:05:46
IBM sells this thing called transrac
IBM卖的那个产品叫做Transarc
1.05.46-1.05.51
which actually was a senior startup like the guy that did the afs ,stuff in the 80s 
实际上，1980年代某位做了Afs的老兄创办了一家初创企业
01:05:51 - 01:05:54
they did a startup called tranzarc, I got bought by IBM and IBM still sells us
他们创立了一个叫做Transarc的初创企业，之后被IBM收购，IBM现在依然向我们销售这个产品
01:05:55 - 01:05:59
there's a project you can't really read the logo ,it's called Apache Omid, it was built by Yahoo, 
这里还有个项目，你们可能看不懂它的logo是什么意思，它叫做Apache Omid，它是由雅虎构建的
01:05:59 - 01:06:04
it's basically a TP monitor for HBase, or NewSQL system
简单来说，它是一个供HBase或NewSQL系统使用的TP monitor
01:06:04 - 01:06:06
that's actually used by a couple other systems today 
实际上，当下有两个系统在使用它
01:06:06 - 01:06:10
so you can build a distributed database without worrying about transactions
So，在无须关心事务的情况下，你可以去构建一个分布式数据库
1.06.10-1.06.13
 could you just rely on these guys to figure things out for you
你可以依赖这些产品去帮你们弄清楚这些事情
01:06:13 - 01:06:16
and you just do all the single-node stuff that you're normally
这就和你们在单节点数据库下平时所做的那些操作一样

01:06:19 - 01:06:23
but pi more common is to use a centralized coordinator as a middleware where 
更常见的是使用中心式协调器作​​为一个中间件（middleware ）
01:06:23 - 01:06:27
you had this piece of software that sits between the application server and the database partitions 
它是位于应用程序服务器（application server）和数据库分区之间的软件 

01:06:28 - 01:06:36
all queries go through this middleware, and the middleware is responsibly figuring out, oh  this query  touch this data this partition 
所有的查询请求都会通过这个middleware，这个middleware会弄清楚，这个查询所涉及的数据分别在这些个分区上

01:06:37 - 01:06:42
so it looks at its it's it's you know it's its global lock table or information about partitions are there 
so 它看起来就是一个全局lock表或者说它存放了关于这些分区的信息
01:06:42 - 01:06:45
and it routes the queries as needed for you 
通过它可以将你的查询路由到对应的地方
01:06:45 - 01:06:51
so you look like you're talking to a single single-node database system through the middleware 
so 通过middleware 你看起来好像是在和一个单节点数据库系统进行通信
01:06:51 - 01:06:54
but in the backend its distributed and broken across these different partitions 
但在这背后，它在分布式存在的，横跨这些不同的分区
01:06:55 - 01:06:58
so when the commit request shows up by the application server 
so 当application server发出提交请求，

01:06:58 - 01:07:03
the middleware does the same thing as the TP monitor does, it communicate to these guys and say hey are we allowed to commit 
middleware所做的事情和 TP monitor一样，它与这些分区服务器沟通，并说嘿，大伙允许它提交么？
01:07:04 - 01:07:08
and only when everyone agrees do you then send back the acknowledgment 
只有当每一个人都同意的时候，middleware 会返回一个ack确认信息
01:07:09 - 01:07:14
so this one this this approach is actually very very common, like Facebook is probably most famous one 
这种方式实际上非常普遍，例如Facebook可能是这种方案使用者中最著名的一个
01:07:14 - 01:07:16
Facebook runs the world's largest MySQL cluster
Facebook运行着世界上最大的MySQL集群
01:07:17 - 01:07:20
and they have a middleware assistant to do all this routing for you 
他们有一个middleware 为我们进行所有路由
01:07:21 - 01:07:24
Google used to do this for for MySQL in the ads
Google在它的广告业务中为它的MySQL集群使用了这个方案
01:07:24 - 01:07:26
there's a planet-scale that came out of YouTube 
YouTube 同样也应用了
01:07:27 - 01:07:33
but this approach is actually very very common, you take us you know Postgres MySQL whatever you know your favorite single-node database system is 
这种方案实际上非常普遍，无论你喜欢哪种单节点数据库系统，比如Postgres MySQL，都可以使用这种方案
01:07:34 -01:07:36
and you build this little wrapper layer in front of it
你可以在这个数据库之前构建一层这个东西
01:07:36 - 01:07:38
eBay did this with Oracle 
eBay 在使用的Oracle之上做了这一层 
01:07:40 - 01:07:41
it's very common 
它很通用

01:07:43 - 01:07:47
the other approach is the last versions to decentralized coordination
另一种方法是通过数据的最新版本来进行去中心化协调
01:07:47 - 01:07:51
where you don't have a coordinator, you don't have a centralized view of what's going on in the system 
你不需要有这么一个协调器（coordinator），你无须通过一个中心化的视角来看系统正在进行的事务

01:07:52 - 01:08:00
the application Server communicates with some home partition or base partition ,some master node that's been responsible for this given transaction 
application Server会和分区中的某些服务器通信（也就是我们的分区其实是主备形式的），主节点用来对给定事务进行响应
01:08:01 - 01:08:04
all their notes will be master nodes, if you're assume you're a homogeneous architecture 
能和application Server进行通信的都是主节点，如果假设你是在homogeneous architecture下

01:08:05 - 01:08:11
so you send all the query requests either to directly to the master node or to individual partitions, it doesn't matter
so 你可以将所有查询请求直接发送到主节点或单个分区，这没关系
01:08:11 - 01:08:18
but it's when you want to go commit ,you go to the master node, and say hey I made these changes I want to go ahead of commitment transaction 
 但当你想要去做提交，你就必须去主节点，并说，我做了这些更改，我想进行提交事务操作

01:08:19 - 01:08:23
and then it's responsible for communicating with the other partitions and deciding whether you're allowed to commit 
然后，主节点负责与其他分区进行通信并确定是否允许提交
01:08:23 - 01:08:25
and if yes then you send back the acknowledgment
如果yes，那它就会返回一个ack


01:08:28 - 01:08:34
all right so the thing that I call Stover is that part of a how do we figure out whether it's safe to commit
so 现在我们要考虑的事情就是，我们该怎么做，才能弄清楚它是否可以安全的提交
01:08:35 - 01:08:35
question 
请讲
01:08:39 - 01:08:41
this question is how do you take the locks 
他的问题是我们该怎么拿到锁

01:08:42 - 01:08:48
so it would be say again assume I'm doing a lot project locking the whole partition
假设，我在整个分区上做了大量的lock 

01:08:48 - 01:08:50
so when the query shows up ，right 
当你进行这里所示的查询时
01:08:50 - 01:08:53
you  try to acquire the lock at that point 
你要在这个点（非主节点）来尝试获取这个lock
01:08:57 - 01:09:04
so the master node would only know information potentially about, what partitions you touched, doesn't know what you did at them 
主节点只会知道一些有关你接触过的分区的潜在信息，但不知道你对它们做了什么
01:09:04 - 01:09:10
right and it's response to the application responsible are saying ,hey I couldn't get the lock of this partition I have to abort my transaction 
它是对所负责的应用程序响应说，嘿，我无法获得此分区的锁，我必须中止事务
01:09:10 - 01:09:12
so you go back to this guy hey say I aborted
你给这家伙回复说，我被中止了
01:09:12 - 01:09:17
alternatively, you send all the requests that this guy ,and he's responsible for farming out to the different machines 
或者，你可以将所有请求发送给P1，它将其询问这些不同的机器
01:09:20 - 01:09:23
at the master node, if you touch the data at the master node sure yes 
如果你发送数据修改请求在主节点上，那没问题

01:09:29 - 01:09:35
okay so we'll cover this in more detail next class 
ok，关于这些更多的细节我们将在下一节课涉及
01:09:35 - 01:09:41
it's an impress upon you and then you'll think about it and see on Wednesday why you know how hard it actually is 
看来这给你留下了深刻的印象，你回去思考一下，在周三的时候，你会看到为什么这么难
01:09:42 - 01:09:44
say we're doing two-phase locking my last example 
我们会做两阶段锁
01:09:44 - 01:09:50
and say that my node or over the white or never one node is in Pittsburgh,one node is in San Francisco
我这里有两个节点，一个节点在匹兹堡，一个在旧金山

01:09:50 - 01:09:55
so at the same time I have two applications trying to update the database right 
在同一时间，我有两个应用程序试图更新数据库

01:09:55 - 01:10:00
and the very beginning I get a lock on my node here for A, this guy gets the lock on B 
在最初，我获取到了节点A的锁，这家伙获取到了B上的锁

01:10:00 - 01:10:05
but now I want to update this guy wants to update B, the other guy wants to update A 
但现在，这个家伙想更新B，另一个家伙想更新A

01:10:06 - 01:10:11
so now I got to go over the network and send a lock request to get the other lock on the other thing 
现在我必须通过网络发送锁请求，以获取另一个节点的锁
01:10:11 - 01:10:15
the other guys doing the same thing, I'm obviously ending up with the deadlock here 
另外一方也做着同样的事情，很明显，我最终死锁了
01:10:15 - 01:10:19
so how do I actually figure out who's actually should be allowed to commit 
so 我该怎么做才能搞清楚到底应该允许谁提交
01:10:20 - 01:10:24
this again if I'm doing a decentralized architecture if I don't have that TP monitor 
 如果我是基于去中心化架构（decentralized architecture），即我没有TP monitor 
01:10:24 - 01:10:30
but even if I do, I mean have not the fine-grained information about what exactly it's doing on each node 
我的意思是我没有关于每个节点上的确切在做什么的详细信息
01:10:30 - 01:10:33
because you can't always know what the queries gonna do before you actually run it
因为你在实际运行查询之前并不总是知道查询将要做什么

01:10:34 - 01:10:38
someone needs to figure out I have this weight to a graph of a cycle I need to kill somebody
我需要有个权重来弄清楚该如kill掉哪一个来解除这个死循环

01:10:40 - 01:10:46
and then so  this guy says ,oh I'm gonna back off, I have a deadlock ,if I'm doing prevention I kill myself
这个家伙说，哦，我要退一步海阔天空，要不就会有一个死锁，如果我做提前准备的话，我会kill 掉我自己
01:10:46 - 01:10:47
this guy could be doing the same thing
这家伙可能做了同样的事情


01:10:49 - 01:10:52
so this is what we're gonna talk about on Wednesday 
So，这就是我们会在周三要讨论的东西
01:10:52 - 01:10:54
how do you actually do to distributed concurrency control 
即我们实际该如何做到分布式并发控制
01:10:54 - 01:10:58
how do you figure out you take two phase locking、 timestamp ordering  and run it in distributed environment
你如何在分布式环境下使用两阶段锁或timestamp ordering之类的东西
01:10:58 - 01:11:04
 , we don't have a complete global view of everything's going on ,the side the submitting a and a given time 
我们无须去设定一个全局的视野来获知所发生的所有事情，并以此来应对事务提交
01:11:05 - 01:11:11
we're also gonna spend time on, when my transaction says go ahead and commit, how do I guarantee that I that I commit everywhere 
当我的事务要去提交的时候，我该如何保证每个地方这个事务都进行了提交呢？
01:11:13 - 01:11:15
because what happens if a node goes down while I'm trying to commit
因为如果当我试着提交的时候，某个节点挂掉了
1.11.15-1.11.16
what should I do
我应该做什么呢？
01:11:17 - 01:11:19
that's actually super hard to get right
这实际上难以做到正确
01:11:20 - 01:11:22
so if you're ensure these kind of things
So，如果你能保证这些东西
1.11.22-1.11.27
there's this great is this great website called the Jepsen project 
这里有一个很棒的网站，它叫做Jepsen Project
1.11.27-1.11.28
by this guy called Kyle Kingsbury
它是由一个叫做Kyle Kingsbury的人创建
01:11:28 - 01:11:37
so he's basically he built this torture chamber for distributed databases, Britain enclosure which is a bit gnarly
基本上可以这么说，他为分布式数据库构建了严格的测试环境
01:11:37 - 01:11:43
but he basically has this test suite where he can take your distributing database run through these weird edge cases 
简单来讲，他使用测试工具来对这些分布式数据库进行测试，比如使用一些奇怪的边界案例来对它们进行测试
01:11:43 - 01:11:52
and identify that it's not always correct, and has problems on a guaranteeing reliability availability or correctness of transactions 
以确定它哪里是不正确的，找出在保证可靠性可用性或事务的正确性方面存在的问题
01:11:52 - 01:11:54
so right now he he has a consult company 
So，现在他创办了一家咨询公司
1.11.54-1.11.56
people pay him money to go actually run this 
人们会付他钱来测试这些东西
01:11:56 - 01:12:01
so build his website he has these write-ups, which are super super detailed
So，他将这些报告放到了网站上，并且写的超级详细
1.12.01-1.12.04
and take a long time to read to understand ,what he's actually talking about 
你们需要花很长时间去理解他实际讨论的是什么
01:12:04 - 01:12:06
but he talks about how these different DBMS he's tried this against 
但他讨论了他是如何测试这些不同DBMS的
01:12:07 - 01:12:09
they claim that there may be transactions correctly
这些DBMS声称正确执行了这些事务
1.12.09-1.12.13
they claim that they can always support high availability or good performance 
并且声称他们始终可以支持高可用或者具备良好的性能
01:12:13 - 01:12:15
and his thing shows that that they don't 
但这位老兄写的文章表示这些DBMS并没有做到他们所说的那些事
01:12:16 - 01:12:19
so they paying him money to go run his thing on their database system
So，这些DBMS厂商会付给他钱来让他测试他们的数据库系统
1.12.19-1.12.19
and then if they pass
如果他们通过了这些测试
1.12.19-1.12.21
they can announce that they're certified 
他们就能宣布他们被认证了
01:12:21 - 01:12:25
there's one day to his company was aerospike which is a distributed key-value store 
曾经有一天有家叫做aerospike数据库的公司，这个aerospike是一个分布式的key-value数据库
01:12:25 - 01:12:29
they used to claim on their website they had you know they had strong consistency guarantees 
他们曾经声称他们提供强一致性保证
01:12:30 - 01:12:32
he ran his thing against theirs crushed it showed how it wasn't 
这位老兄用他的测试工具证明他们所说的并不成立
01:12:32 - 01:12:36
and they had to go back and change all the marketing crap to remove it, because t humiliated it 
他们就必须从他们的营销用语中移除这个认证，因为太打脸了
01:12:37 - 01:12:38
says websites awesome
这个网站很棒
1.12.38-
 his Twitter feed not so much 
他的推特则没有那么多内容
01:12:40 - 01:12:42
you'll see why to go look at it uh
你们去看下就知道原因了
01:12:42 - 01:12:44
it's not my thing 
这不关我事
01:12:44 - 01:12:46
but he's a really sharp dude
但他是个极端的家伙
1.12.46-1.12.47
, I think there's a really good website 
我觉得这是一个很棒的网站


01:12:48 - 01:12:52
alright next class Distributed OLTP systems, replication, cap theorem 
下节课我们会去讨论分布式OLTP系统，replication以及CAP定理
01:12:52 - 01:12:53
and then real-world examples
以及现实中的例子
01:12:53 - 01:12.58
again we'll go through start worrying about how we're actually gonna run transactions in a distributed environment
我们会去关心如何在分布式环境下执行事务
1.12.58-1.12.59
will tell them about NoSQL systems,
我们会去讲下NoSQL系统相关的事情
1.12.59-1.13.01
and see why they don't want to do transactions 
并看看他们为什么不想执行事务
01:13:01 - 01:13:04
because it's gonna affect performance and availability 
因为这会影响性能和可用性
01:13:04 - 01:13:07
okay all right guys awesome see you on Wednesday
Ok，孩儿们，周三再会
