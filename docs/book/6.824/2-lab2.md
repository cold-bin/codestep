# Lab 2: Raft 总结

1. 阅读实验手册：https://pdos.csail.mit.edu/6.824/labs/lab-raft.html
2. 阅读：[Lecture 06 - Raft1](https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1)
3. 阅读：[Raft一致性算法论文](https://github.com/maemual/raft-zh_cn/blob/master/raft-zh_cn.md)
4. 

## 一些总结

分布式存储系统的难点是什么？

问题：如何高性能的利用数百台机器资源进而完成大量的工作？

首先进行**分片**，也就是将数据分割到大量的服务器上。因为有很多机器，所以可能会发生故障，所以需要**容错**。解决容错的方法是维护**副本**，但是副本又导致了一致性的问题。为了保证一致性就需要降低性能，反之提供性能就降低了一致性。



1. Raft 论文总结

一致性算法实现了将一组机器当成一个整体的功能，即使其中某些机器出现故障这个整体依旧能够正常运行。

最初 Paxos 算法统治者一致性算法这一领域，但是这个算法难以理解，需要做很大的修改才能在实际中应用。

而 Raft 算法设计时参考了这一点，比 Paxos 算法更容易理解并且能更清楚的知道它为什么能工作。

Raft 将算法分为了领导人选举，日志复制和安全三个模块并减少了状态机的状态（相对于 Paxos，Raft 减少了非确定性和服务器互相处于非一致性的方式）。

Raft 的特性：

1. 强领导人：和其他一致性算法相比，Raft 使用一种更强的领导能力形式。比如，日志条目只从领导人发送给其他的服务器。这种方式简化了对复制日志的管理并且使得 Raft 算法更加易于理解。
2. 领导选举：Raft 算法使用一个随机计时器来选举领导人。这种方式只是在任何一致性算法都必须实现的心跳机制上增加了一点机制。在解决冲突的时候会更加简单快捷。
3. 成员关系调整：Raft 使用一种共同一致的方法来处理集群成员变换的问题，在这种方法下，处于调整过程中的两种不同的配置集群中大多数机器会有重叠，这就使得集群在成员变换的时候依然可以继续工作。

什么是复制状态机？

Paxos 有两个明显的缺点。第一个缺点是 Paxos 算法特别的难以理解，不透明。Paxos算法的第二个问题就是它没有提供一个足够好的用来构建一个现实系统的基础。

Paxos 算法在理论上被证明是正确可行的，但是现实的系统和 Paxos 差别是如此的大，以至于这些证明没有什么太大的价值。


Google Chubby:在Paxos算法描述和实现现实系统中间有着巨大的鸿沟。最终的系统建立在一种没有经过证明的算法之上。

Raft : 可理解性，直观认识，减少状态的数量来简化需要考虑的状态空间


Log 是 Leader 用来对操作排序的一种手段。所有副本以相同的顺序执行相同的操作。

Follower 收到操作后并不执行而是存在本地，然后将操作发送给 Leader ，直到收到 Leader 的 commit 号之后才会执行。

Leader 需要在本地复制一份客户端请求的副本，一旦 Follower 由于网络原因或者其他原因短时间离线了或者丢了一些消息，Leader 能够向 Follower 重传丢失的 Log 消息。

所有节点保存 log 的另一个原因是帮助服务器重启恢复。服务器重启后需要读取 log 内容，所有此前 Raft 节点需要将 log 写入磁盘中。但是此时该节点并不知道当前 log 的执行位置，所以不会对 Log 做任何操作。

如果 Leader 处理速度远大于 Follower ，那么会导致 Follower 堆积大量 log 进而导致内存被填满。解决方案是 Leader 和 Follower 之间进行通信，Leader 告诉 Follower 目前执行到了哪一步进而调节速度。

Leader 出现故障后进行会重启，然后选举 Leader 。选举出来的 Leader 首先会确认当前 log 执行到哪一步了，然后确认一个过半服务器都认可的 log 执行点。

接口：

Start函数：客户端向 KV 层发送一个请求，KV 层将请求转发给 Raft 层，而 Raft 层则将请求存放在 log 中的某处，当 log 被 commited 时会告诉 kv 层。

applyCh 的 channel： 随着时间的推移，Raft 层会通知 kv 层，告诉此前被 commited 的 log 。此处被并非是最近的一次 log 可能中间夹杂着几百条。

一般来说不同副本的 log 的末尾是不相同的。例如 Leader 向 Follower 发送日志到一半故障了，有些 Follower 收到了而有些 Follower 没收到所以日志不同。但对于 Raft 来说，Raft会最终强制不同副本的Log保持一致，虽然短暂的不一致但是最终都将一致。

* 为什么Raft系统会有个Leader，为什么我们需要一个Leader？

不用 Leader 也是可以的，例如 Paxos 。但是在不出故障的前提下有 Leader 会使整个系统更加高效，例如一个请求通过一轮消息就可以获得过半的服务器认可，但是无 Leader 首先需要一轮消息来确认一个临时 Leader 然后通过第二轮消息来确认请求。所以一个 Leader 提升了两倍的性能。此外一个 Leader 可以更好的理解 Raft 系统的工作流程。

在整个系统中可能有多个 Leader 通过任期号 term number 来区分，每一个任期内只有一个 Leader ，也就是要么没有要么只有一个 Leader 。

* 如何选举 Leader ？

有一个定时器，在这段时间内如果一直没有收到当前 Leader 的消息会认为 Leader 已经下线进而触发选举。

* 选举流程？

增加任期号，开启一个新的周期。当前服务器向剩余 N - 1 个服务器发送请求投票的 RPC ，N 是服务器总数。Raft 中规定自己会投给自己。

在一个任期内每一个节点只会对一个候选人投一次票，所以不会出现两个候选人获得过半的选票。过半的原则导致了只能有一个胜出的候选人。如果超过一半的服务器故障那么永远也无法选出一个 Leader 。

一旦服务器赢得选举会立刻向其他节点发送一条AppendEntries消息给其他所有的服务器。Raft 规定只有 Leader 才能发送 AppendEntries ，进而隐晦的表明自己就是 Leader 。

* 什么时候进行选举？

Leader 出现故障一定会有选举，但是 Leader 没有故障也有可能存在选举。例如网络很慢导致丢失了几个心跳进而导致几个选举定时器超时开始了新的选举，但是此时 Leader 还在健康运行并认为自己的还是 Leader 。

> 例如，当出现网络分区时，旧Leader始终在一个小的分区中运行，而较大的分区会进行新的选举，最终成功选出一个新的Leader。这一切，旧的Leader完全不知道。所以我们也需要关心，在不知道有新的选举时，旧的Leader会有什么样的行为？

在两个网络分区中，一个分区有着过半的服务器，另一个分区内服务器没有过半。而在没有过半的分区中存在一个旧 Leader ，但是过半的分区中选出了新的 Leader ，此时旧 Leader 会出现两个问题：

1. 客户端向旧 Leader 发送请求，但是旧 Leader 因为在没有过半的分区内所以凑不够一定数量的投票进而导致无法 commite 也就是永远无法响应客户端。
2. 有可能旧 Leader 向一部分 Follow 发送完 AppendEntries 就故障了。

* 选举定时器

Leader 会定时的向 Follow 发送 AppendEntries 消息，Follow 收到消息后会重置所有 Raft 节点的选举定时器。

* 选举失败

如果大面积网络故障，超过半数的节点无法参与选举那么会导致选举失败，此时什么事情都不会发生。

候选人同时参加竞选也会导致选举失败，也就是所有的分割选票(Split Vote)。例如网络中的节点同时超时节点变为候选人，而所有节点都投给了自己无法选出 Leader ，如果下次还是同时发生那么状态就会持续下去。

* 分割选票

Raft 无法避免分割选票，但是可以降低发生的概率。将选举定时器随机选择一个超时时间即可。

例如 Leader 故障后，Follow 在同一时刻重置了计时器，而此时定时器选取了不同的超时时间。因为超时时间存在先后所以提前到达超时时间的节点会率先发起投票如果获得超半数的选票那么该节点就可成为新的 Leader 。

* 为了分割选票对超时时间做的处理

其中超时时间不能小于 Leader 的心跳间隔，因为一旦小于这个间隔，存在没有正常收到心跳但是却触发了选举。

超时时间的上限影响了系统能多快从故障中恢复。因为这段时间内旧 Leader 故障系统实际上是瘫痪的，而客户端的请求则被丢弃，所以上限越长也就导致恢复时间越长。

此外两个节点之间的超时时间差也要足够长，确保第一个开始选举的点能够完成一轮选取，也就是需要大于发生一条 RPC  往返所需的时间。

在 lab2 中需要在几秒内搞定如果不行就会报错，实际上这个限制并不严格。

每次节点重置自己的选举定时器时都会随机重置一个时间，而非第一次重置一次后续一直使用，因为这也是有可能导致存在分割选票的情况。

* 旧 Leader 故障后，新 Leader 如何确保整理不一致的 log ？

正常情况下 Leader 告诉 Follow 发生的 log 的内容，而 Follow 全盘接收并添加到本地的 log 中。

https://mit-public-courses-cn-translatio.gitbook.io/mit6-824/lecture-06-raft1/6.9-ke-neng-de-yi-chang-qing-kuang