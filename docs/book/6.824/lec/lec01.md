# lec1 

这节课主要讲性能和容错。

## 什么是分布式系统？

分布式系统是通过网络使得一群计算机相互通信进而完成一连串的任务。

## 为什么要使用分布式？

如果能用一台机器搞定就不要用多台机器。也就是能不用分布式就不用分布式，因为分布式不简单。

使用大量机器协作通过并行可以获得**高性能**。例如 CPU，内存，硬盘都能够并行运行。

此外分布式系统可以实现**容错**。例如两台机器做完全相同的事情，但是其中一台发生故障后可以马上却换到另一台机器上而不用从头开始，进而提高了效率。

物理空间上的**扩展**，两地转账，虽然处于不同的地域物理上是分散的，但是在这个系统内可以实现交互。

**隔离**：不信任某些代码，这些代码不会立即出现异常，可以将计算拆分，分散到不同的机器上计算，然通过网络协议进行通信从而实现隔离。

## 实现分布式系统的困难？

因为分布式由多部份组成，并且同时执行。这在并发上会有很大的问题，例如超时机制，熔断机制。这也是分布式系统难得原因。

另一个问题是，分布式系统中多个组件之间进行通信可能会产生意想不到的故障。例如电源故障，网络某些部分损坏，某些部分停止工作，其他继续工作。

构建分布式系统是为了获得高性能，但是会遇到很多难题，需要精心设计整个系统。

目前分布式已经在实际中投入使用的。例如某些大型网站。并且因为大型网络的兴起推动导致分布式系统变得很重。

虽然此前已经解决了很多的问题，但是目前依旧有很多的问题等待解决。

## 四个 lab 的介绍

Lab1 是让你去阅读论文，并实现你自己的简单 MapReduce 版本。

lab2 实现一个 raft 算法。Raft 通过复制实现了容错，当某个服务器出现故障后会选择其他备机进行主从切换.

lad3 使用自己实现的 Raft 算法实现具备容错功能的 key-value server,可以被复制和容错.

lab4 实现分片，分片是指将数据拆分为多个服务器之间的数据分区，以达到并行加速。将有可复制能力的主备 key-value 服务器克隆到多个独立的组中。然后你将你之前的key-value 存储系统中的数据分割并分别存储到这些独立的组中，以便通过运行多个这种可复制组来并行提高速度（每个组只存储对应自己的部分数据，所有组合起来是原来的一整份数据）

并且你还将负责在不同服务器之间来回移动各种数据块同时保证不会出现数据丢失（知秋注：数据分片到各个group中，group中的服务器内又会有主从复制）

lab 根据通过测试的次数来打分.本地的测试用例是全的，没有隐藏的测试。

debug 是很耗时间的，因为是分布式系统其中存在了大量的并发和通信，这就会导致在debug时会变得异常困难。

## 三种基础架构系统

存储，因为存储是一个定义明确且有用的对象，所以这是最为关注的一块。如何构建某种具有复制容错的高性能分布式存储实现。

计算，如何实现计算系统，例如 MapReduce 就是一种计算系统。

通信，分布式是构建在通信的基础之上。

对于存储和计算，目标是能够在分布式场景下发现可抽象的部分并设计成接口进而简化使用，最终在此基础上简化构建应用放入流程。也就是要从中隐含的分布式性质中构建出抽象来，构建出隐藏系统分布式性质的抽象。实际上这是难以完全实现的。

其实就是是一个超高性能容错的分布式系统，但是却提供和非分布式存储和计算同样的接口。

## 如何思考这些抽象？

首先是 RPC，目标是掩盖了通过不可靠网络进行通信的事实。

线程的使用，充分利用多核从而实现并发，线程简化了程序员的并发操作。在实现的时候需要考虑并发控制，例如锁。

## 性能

性能，分布式系统的目标是可扩展的提速，通过扩展进行加速。

什么是可扩展性？

例如一台机器能够解决的问题买两台机器能够节省一半时间，或者两台计算干的活是一台机器的两倍。也就是两倍的计算机或资使得具有两倍的性能或吞吐量。

通过增加机器的数量进而达到提高性能的目的，机器用钱就可以买到，不仅可以容灾还可以提高性能。但是和单机场景下花钱优化出更快的代码相比，后者显然要付出更昂贵的代价。

最终目的是希望用一千台机器来获得一百倍的吞吐量，但是必须要精心设计才能获得。

## 通过增加服务器实现性能加速的例子

例如某台计算机运行着数据库和 web 服务突然之间访问量加大，此时该怎么办？

一般来说是买更多的 web 服务器实现分流，一部分用户访问服务器一，另一部访问服务器二。也就是可以通过增加 web 服务器来实现并行加速的方法。

但并不能无线扩展。因为 web 服务器要访问同一个数据库，成百台 web 服务器访问数据库会使得数据库成为性能瓶颈，此时再增加 web 服务器就没用了。也就是无法通过无限制的增加电脑数量来提高性能，达到某个点就不起作用了。

此时可以拆分多个数据库，但是工作量很大。通常大型网站很容易耗尽单个服务器数据库或存储服务器的性能。

此时花很小的代价是不显示的，需要重新设计来实现可扩展性。

## 容错

单机系统稳定运行很多年是很正常的，但是使用成千上百条机器构建一个系统所有机器都稳定运行确是很难的。假设有一千台电脑通常意味着每天可能有三台电脑出现故障，一千台电脑所组成的系统中几乎每时每刻都会有东西损坏，例如崩溃，运行缓慢或者网络崩溃等问题。随着机器数量的增加，出现问题的概率也就变大了。

因为故障总是会出现，所以构建分布式系统时需要考虑整个系统能够处理这些故障。进而上层的程序员就不用考虑这些问题了。

## 容错：可用性

此外关于容错还有一种常见的思想是可用性，当系统遇到故障时能够正常运行并且提供完好的服务，但是当系统遇到很大的故障那么系统将不可用。例如有两份一样的数据，其中一个副本服务器挂掉了，因为副本的存在还能正常工作。当然也有可能全部故障，此时则无法保证可用性。

## 容错：自身可恢复性

自身可恢复性指系统出问题后可以被修复，修复前整个系统暂停，而修复后系统能够正常运行。和可用性相比，在故障修复前系统不会做任何事情。通常可恢复系统需要将最新的数据保存在硬盘上，恢复和再从硬盘中加载进来。

期待的可用系统是，遇到太多故障那么系统会暂停，修复足够多的问题后系统能够正常工作。其中有**非易失性存储**和**复制**两样东西很重要。

## 非易失性存储

非易失性存储是指断电后数据依旧不会丢失的设备，例如磁盘，SSD 等。而易失性存储反之，一旦断电数据就没了，例如内存。

使用非易失性存储可以将系统状态的检查点或者日志保存在硬盘，闪存，或者SSD之类的东西中。恢复后从磁盘中读取信息到达最新的状态并从该状态继续运行。缺点是更新成本很高，需要整个系统使用大量的技巧来避免过多的写入非易失性存储。

## 复制导致的一致性问题

因为和磁盘交互速度是很慢的，所以需要复制来提高性能。而复制又会带来一致性的问题。

通过复制可以获得容错能力，但是随着时间的推移复制之间会产生不一致的问题。

例如构建一个分布式 KV 存储系统，只支持 PUT，Get 两种操作，其中 PUT(key, value) 实现了将 key value 存入数据库中，系统内维护可一张 key 对应相应 value 的表，而 get(key) 则可以获得 key 对应的 value 。客户端发送一个 key 如果存在相应的 value 将会返回。

在非分布式系统中往往只有一个服务器和一张表，通常 put，get 操作不会出现歧义。

但在分布式系统中，通过复制和缓存从而获得了性能和容错，也就是存在多个副本，键值存在不同的版本。如果客户端发出一个 put 操作用于更新数据，但是存在多个副本，put 操作会将所有副本都进行更新。

假设更新副本时服务器出现了故障，出现了部分服务器的数据更新了，部分没有更新。

![20220402215248](https://cdn.jsdelivr.net/gh/weijiew/pic/images/20220402215248.png)

也就是说本次更新其中一些副本失败，一些副本更新成功，导致不同服务器上的数据不一致。

假设现在要通过 get 操作来进行数据读取，需要读取 key 相应的 value ，但是此时 value 不一致。

如果构建一个容错系统，有条规则，即你要始终先访问最上层的这个服务器，失败后再访问备用服务器。

但是因为旧数据的存在，未来某一天依旧有可能访问到旧数据。

如何在 put 和 get 的实现规则中保证一致性？

## 强一致性

强一致性：通过 get 获得最近 put 的数据。有时候弱一致性也非常有用，弱一致性无法保证 get 获得最新 put 所写的值，因为服务器中该值存在多个版本。

强一致性需要做大量细节的处理，需要消耗大量的通信才能实现。

## 强一致性实现

对于一个有些副本被更新但是有些副本没有被更新的情况可以采取用户读取所有副本。但是浪费很大因为读一个值需要浪费很多时间，可以进来尽量避免读取整个新的值，但是这样的话如果副本很远就相当于建立了一个弱系统。

使用副本来实现容错会带来很多问题，并且副本不能放在一个机房。假两个副本在同一个支架上，并且有人被电源线绊倒那么两个副本的数据都将丢失。所以副本尽量分开到不同的城市中。

## 弱一致性

强一致性需要消耗大量的通信，但是通过弱一致性可以获得高性能。

## MapReduce 简介

MapReduce 解决了以TB为单位运行大量计算的问题。例如创建网络所有内容的索引或分析链接整个网络的结构以便识别最重要的页面。或者你所知道的最权威的页面，对整个网络这段时间产生的数十亿兆字节的数据构建索引，基本上相当于对整个数据进行排序。单机来执行是非常耗时的，可能要数周，数年，数月来实现。

对 Google 来说购买大量计算机很值，所以能否通过增加机器来获得无穷的计算。

此时需要一个软件框架，对于普通程序员来说不需要考虑将工作分散到数千台计算机中的细节和怎样组织数据的细节，框架能够自动的处理各种故障。这个框架使得使得非专业人员不用知道分布式的任何知识也能够轻松开发和运行大型分布式计算系统。这就是 MapReduce 的全部意义所在。

将输入以某种方式分割为一大堆不同的文件或数据块

## MapReduce 执行流程

首先输入待处理的文件，然后 MapReduce 框架会将每个输入文件传入Map函数进行处理，Map 之间的并行的。Map 处理输入产生相应的输出，输出是一个 list 。Map函数生成了一个list的Key/Value键值对来作为函数的输出。以词频统计为例，输出的 key 是单词，value 则是该单词出现的次数。Key/Value对的集合作为中间输出。

接下来是 Reduce 阶段，从所有的 map 中遍历所有相同的 key 然后将其作为参数传给 Reduce 函数。在词频统计中，Reduce 函数只需要统计每项的个数。

![20220402230748](https://cdn.jsdelivr.net/gh/weijiew/pic/images/20220402230748.png)

整个计算流程被称为Job，任何一次Map/Reduce的调用都被称为Task 。因此，我们就有了完整的Job，它是由一系列MapTask和一系列ReduceTask所组成

## 如何实现 MapReduce 函数？

Map函数要做的就是拆分，其中 Key 是一个文件名，V 代表传入 Map 函数的输入文件的内容，其中包含了所有的文本。

![20220402231736](https://cdn.jsdelivr.net/gh/weijiew/pic/images/20220402231736.png)

首先将V的内容拆分为单词，然后对于每个 word 都会发出（emit）两个参数。也就是生成相应的键值对，例如 <"word", 1>

map 方法无须知道任何关于分布式或有多少台计算机或我们需要在网络上移动数据这个事实或一些其他分布式的系统的细节。

接下来是 Reduce 实现的功能，MapReduce 框架都会为每个给定key调用Reduce函数以接收它的所有实例。

Reduce 函数会根据这个key所关联的所有map对应的值得到一个vector数组，这个Key就是那个单词，value 只需要统计 vectore 的长度即可。

![20220402232327](https://cdn.jsdelivr.net/gh/weijiew/pic/images/20220402232327.png)

## GFS 

中间文件是存放在 GFS 中，GFS实际上与运行MapReduce的worker是在同一个服务器集上。在与运行MapReduce的worker服务器完全相同的一组worker上运行。

当你知道你读取文件是在GFS(即一个文件系统)的时候，GFS会自动地将你存储在它之上的任何大文件分割为很多64MBit大小的数据块，并将它们分散存储在许多服务器上

如果你爬取了10TB的网页内容，然后，你只需要将它们写入GFS即可，即使它是一个大文件，GFS也会自动将它拆分成无数个64kb大小的数据块，将它们均匀分布在谷歌所有可用的GFS服务器上。

此后要运行一个MapReduce job，它将整个已爬过的网站作为输入，数据已经根据某种方式拆分存储到了所有服务器中。

假设有一千台机器，启动 map worker 后将会有一千台 map worker 。每次就能读取一千个输入数据，并且这些worker能够从一千台GFS文件服务器中并行读取数据。此时吞吐量已经很大的了。

## MapReduce 的优化

GFS 和执行 MapReduce jobs 是如何交互的。随着谷歌对该系统的改进，答案已经发生的变化，一般来说大文件存储在大型网络文件系统中，需要通过通信才能恢复原来的数据。MapReduce 工作进程必须经常通过网络与正确的GFS服务器进行通信。这就是MapReduce在该论文中实际的工作方式。这样做会导致大量的网络通信，但是依然需要消耗大量的时间来移动数据。因此在2004年他们发表文章的时候，他们的MapReduce系统的最大瓶颈就是网络吞吐量。

因为和 CPU 以及内存的处理速度相比，网络通信是最慢的，所以设计的时候应当尽量避免使用网络传输。后来使用了一系列技巧来尽可能的避免通过网络传输。例如将GFS服务器和MapReduce Worker运行在同一组计算机集群中。也就是一千台机器上运行着GFS服务，并且将MapReduce也运行在这一千台机器上

例如当需要运行map的机器要读输入文件1时，需要对该文件执行 map 操作。master 从 GFS 上找出实际保存该输入文件1所在的服务器。然后，它会在该服务器上使用 MapReduce 软件来执行该输入文件所对应的 map 操作。所以此处的优化本质上是计算靠近存储，此前是文件通过网络通信发给 worker ，而更改后是 worker 在存文件的服务器上进行计算，避免了网络通信。这样节省了大量时间，否则将不得不等待在网络上移动输入数据。

map 任务会将输出存储当前正在执行任务的服务器的磁盘中。因此再次存储 map 的输出不需要立马进行网络通信，因为输出结果是保存在硬盘里的。在MapReduce 中将给定key对应的所有这些值组合到一起后，将它们传给某些机器上的Reduce以供调用。

我们会想要获取所有这些数据，并通过网络将它们发送到一台机器上。这种shuffle 的过程键最初是按行移动存储的，并且与运行map同在一个机器上，

我们需要将这些按列存储在一台机器上以供Reduce使用，将行存储转换为列存储的过程，在论文中将之称为 shuffle 。确实需要将整个网络中的所有数据从生成它的map服务器转移到需要它的reduce服务器。这个过程也是MapReduce中非常消耗性能的一部分

将 Reduce 的输出也存储在运行Reduce worker的GFS服务器上，但是因为GFS不仅为提高性能而拆分数据同时为了实现容错也保留了两三个副本。这意味着无论如何，你都需要通过网络将数据的一个副本写入到另一台不同的服务器。因此，这里就有大量的网络通信，这种网络通信方式在2004年严重限制了MapReduce的吞吐量。在2020年，因为这种网络布局对于人们想要在数据中心中所做很多事情来说，是一个限制因素，现代数据中心网络在根节点处的速度要远比这个快。如今，你所可能见到的数据中心网络会有许多根节点，而不是只有一个要处理所有东西的根交换机。你可能有很多根交换机，每一个机架交换机都和这些具有容灾能力的根交换机有连接，并且流量在这些根交换机之间分配，因此，现代数据中心网络具备了更大的网络吞吐量。由于这种现代化，我认为谷歌已经停止使用MapReduce有些年头了。但是在他们停止使用它之前，现代的MapReduce实际上不再尝试在存储数据的同一台计算机上运行map 任务，他们乐于从任何地方来获取数据，因为他们认为现在的网络非常快。