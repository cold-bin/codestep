09-01  Chain Replication with Apportioned Queries
0-0.03

today I want to do two things

今天我想做两件事情

0.03-0.05

I want to finish the discussion of zookeeper

我想结束关于ZooKeeper这方面的讨论

0.05-0.08

and then talk about CRAQ

然后，再来讨论CRAQ

0.08-0.16

the particular things that I'm most interested in talking about zookeeper are the design of its API 

在讨论ZooKeeper时，我最感兴趣的一点就是它的API设计

0.16-0.26

that allows the  zookeeper to be a general-purpose service that really bites off significant tasks that distributed systems need

通过它的API可以使ZooKeeper成为一个通用型服务，它确实解决了分布式系统中所需要的一些重要任务

0.26-0.29

 so why is this you know why is that a good API design

So，它的API设计为什么这么好呢？

这就是它被称为“一个好的API设计”的原因

0.29-0.34

 and then the really more specific topic of mini transactions

然后还要来说下mini-transaction这方面的具体内容

0.34-0.37

 turns out this is a worthwhile idea to know

事实证明，这个想法值得去了解


0.37-0.46

so they got API and mini-transaction

So，我们要去讨论API以及mini-transaction这方面的东西

0.46-0.51

and I'm just just a recall zookeepers based on raft.

回忆下，ZooKeeper其实是基于raft来做的

0.51-0.57

 and so we can think of it as being and indeed it is fault tolerant and does the right thing with respect to partitions

So，我们就可以把它当做是raft来看待，它确实具备容错能力，在应对网络分裂这种问题的时候，能做出正确的响应

0.57-1.06

it has this sort of performance enhancement  by which reads can be processed at any replica

通过将读请求交给任意replica来处理，这让它在性能上有所提升

1.06-1.08

and therefore the reads can be stale 

由于，通过这种方式读到的数据可能是过时的数据

1.08-1.14

so we just have to keep this in mind as we're analyzing various uses of the zookeeper interface

So，当我们在分析ZooKeeper接口的不同用途时，我们要始终注意这点

1.14-1.15

 on the other hand

另一方面

1.15-1.25

zookeeper does guarantee that every replicas process the stream of writes in order one at a time with all replicas executing the writes in the same order

ZooKeeper保证每个replica处理写请求流的顺序都是一致的

ZooKeeper会保证，在某个时间内所有的replica执行的一系列写请求的顺序都是一样的

1.25-1.30

so that the replicas advance sort of in their states of all than  exactly the same way

So，这些replica中的所有state的更新顺序也都是完全相同的



1.30-1.41

and that all of the operation reads and writes produced by a generated by a single client or processed by the system also in order 

所有由单个client所生成的读写操作或者是由系统处理的读写操作都是有序的



1.41-1.42

both in the order that the client issued them in

client发起这些操作也是有序的

1.42-1.55

and successive operations from a given client always see the same state or later in the writes stream as the previous read operation right any operation from that client

来自相同client发起的任何操作都是有序的，都会以写请求流的形式那样，总是能看到一样的或更新后的state

给定客户端的后续操作始终会在写入操作流之后读到相同的状态（知秋注：即在读一个全新状态前可以先发起一个写操作，再读，读的就是写操作返回的那个状态版本，此时，哪怕是在replica上读，也是一样的数据）

1.55-1.58

 okay



1.58-2.03

so before I dive into what the API looks like 

So，在我深入讨论API之前

2.03-2.11

and why it's useful it's worth just thinking about what kinds of problems zookeeper is aiming to solve or could be expected to solve

我先要来思考下ZooKeeper旨在解决什么样的问题，或者说我们期待它能解决什么样的问题

2.11-2.13

so for me

So，对于我来说



02.13 - 02.26

a totally central example of motivation of why you would want to use ZooKeeper

这里我来举一个比较自我的例子，即为什么我想去使用ZooKeeper

到底是什么在驱动我想要去使用ZooKeeper

02.26-2.37

this is it as an implementation of the test and set service that vmware ft required， in order for either server to take over when the other one failed

它像是一种vmware ft所需要的test-and-set服务的实现，即当某台服务器挂掉了，其他服务器可以去接手它的工作

2.37-2.42

it was a bit of a mystery in the vmware paper what is this test-and-set service

到底什么是test-and-set服务，在vmware那篇paper中，有一点点小神秘

2.42-2.48

 how is it may you know is it fault tolerant ？does it itself tolerate partitions？

它具备容错能力吗？它自身能容忍网络分裂吗？

2.48-3.01

but zookeeper actually gives us the tools to write a fault tolerant test and set service of exactly the kind that vmware ft needed

但ZooKeeper实际上给我们提供了工具，我们通过这些工具可以写出具备容错能力的test-and-set服务，这也正是vmware ft所正需要的

3.01-3.05

 that is fault tolerant and does do the right thing under partitions

我们所写出的这个test-and-set服务是具备容错能力，并且在遇上网络割裂的情况下，能够正确应对



03.05 - 03.07

that's sort of a central kind of thing that zookeepers doing

这也是ZooKeeper所做的主要工作

3.07-3.11

 there's also a bunch of other ways that turns out people use it

事实证明，人们也将ZooKeeper用在其它方面

3.11-3.13

ZooKeeper was very successful

ZooKeeper是一个很成功的东西

3.13-3.14

people use it for a lot of stuff 

人们将它用在很多地方




3.14-3.21

one kind of thing people use is just to publish just configuration information for other servers to use 

其中一种用法就是，人们将配置信息推送给其他服务器来使用

其中一种用法就是，人们可以通过它来发布信息给其他服务器使用

3.21-3.22

like for example

比如



3.22-3.27

the IP address of the current master for some set of workers

当前master或者某些woker组的IP地址

可以将一组worker中作为当前Master的那个IP地址存放在zookeeper中

3.27-3.32

 this is just configuration information

这就是一个配置信息

3.32-3.38

another classic use of zookeepers to elect a master

ZooKeeper的另一种经典用途就是选出一个master

3.38-3.40

 you know if we want to have a when the old master fails 

当老的master挂掉了

3.40-3.45

we need to have everyone agree on who the new master is  and only elect one master 

我们需要让其他服务器推举出一个新的master，而且我们只能选出一个mastrer


3.45-3.47

even if there's partitions

即使遇上网络割裂

3.47-3.52

 you can elect a master using zookeeper primitives

你也可以使用ZooKeeper原语来选出一个master



3.52-4.03

if the master for small amounts of stated anyway  if whatever master you elect ，needs to keep some state

对于数量不大的state来说，不管你选了哪个服务器用来当做master，它都得去维护这些state

4.03-4.04

 it needs to keep it up-to-date 

它需要让这些state保持最新

它(master)需要去保持state是最新的

04.04- 04.12

like maybe you know informations such as who the primary is for a given chunk of data like you'd want in GFS 

它里面会有一些信息，比如，在GFS中，对于这个给定的chunk数据来说，哪个是它的primary

这就像，你知道，就好比你想从GFS的primary 中拿到一个chunk，这个信息应该是最新的

4.12-4.15

the master can store its state in zookeeper 

master可以将它的state存放在ZooKeeper中

4.15-4.17

it knows Zookeepers not going to lose it

它知道ZooKeeper不会将这个状态给弄丢

4.17-4.18

 if the master crashes

如果master崩溃

4.18-4.20

 and we elect a new master to replace it 

我们就会选出一个新的master来替换它

4.20-4.23

that new master can just read the old master state right out of zookeeper

新的master可以从ZooKeeper中读取旧master的状态

4.23-4.26

 and rely on it actually being there

并依靠这个状态完成替换



04.26-4.32 ！！！！！！！

other things you might imagine maybe you know MapReduce like systems 

你可能会联想到些其他东西，比如MapReduce这样的系统

4.32-4.37 （需要再过一遍视频）

workers could register themselves by creating little files  and zookeeper and again

worker们可以通过创建一些小文件来将它们注册到ZooKeeper中



04.38 - 04.40

with systems like MapReduce

对于MapReduce这样的系统来说



4.40-4.46

you can imagine the master telling the workers what to do by writing things in zookeeper 

你可以想象下，master会通过在ZooKeeper中写些东西来告诉worker该做什么

4.46-4.48

like writing lists of work in zookeeper 

比如，在ZooKeeper中写下要做的工作清单

4.48-4.55

and then worker sort of take those work items one by one out of zookeeper and delete them as they complete them

然后worker会从ZooKeeper中将这些工作一件件的拿出来，当它们完成工作时，就会将这些工作从清单上移除


04.54 - 04.57

 but people use zookeeper for all these things 

但总而言之，人们会在这些事情上使用ZooKeeper

但总而言之，人们会使用ZooKeeper来做这些事情

4.57-4.58

question

请说出你的问题



05.11 - 05.15

exactly yeah so the question is oh 

So，他的问题是



5.15-5.17

how people use zookeeper 

人们该如何使用ZooKeeper



5.17-5.18

and in generally

一般来讲



05.18 - 05.19

yeah you you would if you're running some big data center 

如果你经营了某个大数据中心



05.19 - 05.23

and you run all kinds of stuff in your data center

并且你在你的数据中心里运行着各种各样的东西



5.23-5.27

 you know web servers， storage systems， MapReduce who knows what

比如：web服务器，存储系统，MapReduce之类的东西，

5.27-5.30

 you might fire up a zookeeper one zookeeper cluster

你可能会去启动一个ZooKeeper集群



05.30 - 05.31

because this general purpose can be used for lots of things

因为出于通用的目的，它可以用来干很多事情



05.31 - 05.36

 so you know five or seven zookeeper replicas

它里面可能有5个或7个ZooKeeper replica



05.36 - 05.38

 and then as you deploy various services

当你在部署各种服务的时候

5.38-5.43

you would design the services to store some of the critical state in your one zookeeper cluster

你会想去需要设计些服务，以此将某些关键的状态存储在一个ZooKeeper集群中

你会去设计这种服务，即将服务的某些关键的state存储在你的一个ZooKeeper集群中

5.43-5.49

 alright 



5.49-5.53

the API zookeeper looks like a filesystem some levels 

某种程度上来讲，ZooKeeper的API看起来就像是一个文件系统

5.53-5.56

it's got a directory hierarchy

它有一个目录层次结构


5.56-5.57

 you know there's a root directory

这里有一个根目录

5.57-6.02

 and then maybe you could maybe each application has its own sub directory 

接着，可能每个应用程序都有它自己的子目录


6.02-6.06

so maybe the application one keeps its files here in this directory

So，比如，应用程序1将它的文件放在这个目录中

6.06-.08

app two keeps its files in this directory 

APP2将它的文件放在这个目录中



06.08 - 06.13

and you know these directories have files and directories underneath them

这些目录里会保存一些文件，在它们里面还有其他目录



06.13 - 06.15

one reason for this is just 

对此，其中一个解释是



6.15-6.22 （5.21 内容）

because Zookeeper is like just mentioned is a design to be shared between many possibly unrelated activities 

因为ZooKeeper被设计用来在多个不相干的活动中共享

ZooKeeper就像刚才提到的那样，是一个可在许多可能不相关的活动之间共享使用的设计（知秋注：可以是毫不相关的系统）

6.22-6.28

we just need a naming system to be able to keep the information from these activities distinct

我们需要一个命名系统，它能够将关于这些活动的信息区分开来

6.28-6.31

 so they don't get confused and read each other's data by mistake

So，这样应用程序就不会感到困惑，也不会错误地读取到其他人的数据了


06.31-6.33

within each application

在每个应用程序中

6.33-6.41 *****

 it turns out that a lot of convenient ways of using zookeeper involve creating multiple files

事实证明，使用Zookeeper的许多便捷方法都涉及到创建多个文件

6.41-6.46

 let's see a couple examples like this in a few minutes 

稍后，我们来看两个例子

6.46-6.48

okay so it looks like a filesystem

Ok，So，ZooKeeper看起来就像是一个文件系统

6.48-6.50

 this is you know not very deep

你知道的，这并不是什么深奥的东西

6.50-6.56

 it doesn't it's not actually you know you can't really use it like a file system in the sense of mounting it and running LS and cat and all those things

从磁盘挂载的角度来看，你并不能把它真的当做一个文件系统来使用，比如使用ls和cat这类的命令

6.56-7.00

 it's just that internally it names objects with these path names

它只是会在内部使用这些路径名来命名对象


7.00-7.03

 so you know one this x y&z here

比如这里我们标记x，y和z

7.03-7.05

 few different files

还有些不同的文件

7.05-7.07

 you know when you talk to me you send an RPC 

当你和我进行通信，你向我发送了一个rpc

7.07-7.08

- zookeeper saying

ZooKeeper表示


07.08 - 07.15

you know please read this data，you would name the data you want maybe App2 slash X 

请读取这个数据，你会将它命名为/APP2/X



07.15-7.18

there's just a sort of hierarchical naming scheme

这就是一种层级命名方案


7.18-7.24

 these files and directories are called Z nodes

这些文件和目录被称为znode

7.24-7.32

and it turns out it's there's three types you have to know about that helps you keep or solve various problems for us

事实证明，有三种类型你必须要了解，它们能帮我们解决各种问题


7.32-7.33

there's just regular Z nodes

这只是些普通的znode

首先要说的是常规的znode

7.33-7.34

where if you create one

如果你创建一个这样的znode

7.34-7.38

 it's permanent until you delete it

在你将它删除前，它会一直存在

7.38-7.41

 there's a ephemeral Z nodes

当然我们也有临时的znode

7.41-7.43

where if a client creates an ephemeral Znode

如果一个client创建了一个临时的znode

7.43-7.49

 zookeeper will delete that ephemeral Znode if it believes that the client has died

如果ZooKeeper觉得这个client已经挂掉了，它就会删掉这个临时的znode

7.49-7.51

it's actually tied to client sessions

实际上，这和client session绑定

7.51-7.55

 so clients have to sort of send a little heartbeat in every once a while into the zookeeper

So，client必须每隔一段时间发送heartbeat给ZooKeeper

7.56-7.58

zookeeper say oh I'm still alive I'm still alive

它会对ZooKeeper说，我现在还活着呢


7.58-8.01

 so zookeeper won't delete their ephemeral files

So，ZooKeeper就不会删掉它们的临时文件


8.01-8.10

 and the last characteristic files may have is sequential 

最后，我们还可能会有些特征文件，它们都是有顺序的

8.10-8.12

and that means when you ask to create a file with a given name 

这意味着，当你要使用一个给定的名字来创建一个文件时

8.12-8.18

what you actually end up creating is a file with that name but with a number appended to the name

实际上当你最终创建出来一个文件的时候，它的名字是将一串数字追加在该文件的名字后面

8.18 - 8.25

 and zookeeper guarantees never to repeat a number if multiple clients try to create sequential files at the same time 

zookeeper保证同一时间如果有多个client端同时创建sequential（有序）文件，zookeeper生成的这些文件名的序号永远不会相同

它永远不会生成包含相同序号的文件



08.25 - 08.33

and also to always use montt increasing numbers for the for the sequence numbers that appends to filenames 

而且会一直使用递增数字来作为序列号追加到文件名上



8.34-8.37

and we'll see all of these things come up in examples

我们会在之后的例子中看到所有这些东西




8.37-8.39

at one level

在某一层面上来讲

8.39-8.46

the operations the RPC interface that zookeeper exposes is sort of what you might expect for 

zookeeper暴露的RPC接口就是我们期望的那样



08.47 - 09.03

 your files was to create RPC where you give it a name really a full path name ，some initial data and some combination of these flags

你要给create方法传入一个完整的路径名，一些初始数据，以及一些flag组合


9.08-9.10

 and interesting semantics of create is that

create是比较有趣的语义

create的语义令我们比较感兴趣

9.10-9.11

 it's exclusive 

它是独占的

9.11-9.14

that is when I send a create into zookeeper ask it to create a file 

当我发送一个create操作给ZooKeeper，让它去创建一个文件

9.14-9.17

so ZooKeeper your responds with a yes or no

So，ZooKeeper使用yes或者no来响应

9.17-9.21

 if that file didn't exist and I'm the first client who wants to create it 

如果这个文件并不存在，并且我是第一个想去创建它的client

9.21-9.23

zookeeper says yes and creates the file

那么ZooKeeper就表示yes，然后创建这个文件

9.23-9.24

the file already exists 

如果这个文件已经存在

9.24-.26

zookeeper says no and returns an error

ZooKeeper就表示No，并且返回一个错误

9.26-9.30

 so clients know it's exclusive create

So，client就会知道这是一个独占型的create

9.30-9.35 

and clients know whether they were the one client if multiple clients are trying to create 

the same file which we'll see in locking samples

 如果多个client端尝试创建相同的文件，client会知道他们是否是成功创建文件的天选之子，这个我们会在之后关于locking的案例中看到




9.36-9.42

the clients will know whether they were the one who actually managed to create the file

client们就会知道它们是不是那个实际管理创建这个文件的client

client们会知道这个创建的文件到底是否是它自己实际可管理的


9.46-9.49

there's also delete 

这里也有delete操作



9.54-9.55

and one thing I didn't mention is 

我有一点没有提到的是

有一个事情我并没有提到，

9.55-9.59

every znode has a version as a current version number that advances as its modified 

每个znode都会有一个版本号，当它被修改的时候，它就会有一个当前版本号

每个znode都有一个版本号，该znode的当前版本号随着znode的修改而提高

9.59 - 10.06

and delete along with some other update operations

delete往往伴随着一些其他的更新操作

10.06 - 10.13 ！！！！！

you can send an a version number saying only do this operation if the files current version number is the version that was specified 

你可以发送一个版本号，并表示如果该文件当前版本号和我发送的这个指定版本号匹配，那么对该文件只进行这种操作


10.13 - 10.22

and that'll turn out to be helpful， if you're worried about in situations where multiple clients might be trying to do the same operation at the same time 

事实证明，如果你担心同一时间有多个client试着对这个文件做相同的操作，那么这会对你解决这个问题很有帮助


10.21 - 10.25

so you can path and version saying only delete 

So，你可以传入一个路径和版本号，并表示只做删除操作


10.26 - 10.30

there's an exists call

这里有一个EXISTS调用函数

10.30-10.33

 oh does this path name Znode exist

它会去根据传入的路径名，来检查这个znode是否存在



10.33 - 10.36

an interesting extra argument is that

这里还有一个令我们感兴趣的参数



10.36 - 10.41

you can ask to watch for changes to whatever path name you specified 

你可以去要求对你指定的路径名设置一个watch事件

你可以设定watch来监控对你指定的znode的变化




10.42 - 10.46

you can say does this path name exist  and whether or not exists it exists now

watch可以告诉你这个znode是否存在以及现在是否存在



10.46 - 10.49

 if you set this watch if you pass in true for this watch flag

如果你把传入的这个watch flag设置为true

10.49 - 10.57

 zookeeper guarantees to notify the client if anything changes about that path name  like it's created or deleted or modified

ZooKeeper会保证，如果这个路径名发生了任何改变（比如，创建，删除，修改之类的操作），ZooKeeper会将这些发生的事情通知client





10.57 - 11.00

 and furthermore

此外



11.00 - 11.11

the check for whether the file exists and the setting of the watch point of the watching information in the inside zookeeper are atomic

在ZooKeeper内部检查该文件是否存在，并设置观察信息的观察点，这两个操作都是原子性操作

11.11 - 11.16

 so nothing can happen between the point at which the point in the write stream

因此在write stream中的点之间什么也不会发生

so 在两个write 操作间watch不会有任何动作

11.16 - 11.25

 which zookeeper looks to see whether the path exists and the point in the write stream at which zookeeper inserts the watch into its table 

即在znode完成改变之前，watch不会给你任何反馈



11.25 - 11.31

and then that's like very important for for correctness

对于正确性来说，这点非常重要


11.31 - 11.40

we also get data then you get a path and again the watch flag 

我们也会有一个GETDATA函数，我们要往里面传入路径和watch flag

11.40-11.45

and now the watch just applies to the contents of that file

然后，通过设置watch，我们就可以关注到该文件是否被修改


11.45 - 11.57

there's set data again path the new data and this conditional version

这里还有一个SETDATA函数，我们要往里面传入路径，新的数据以及这个特定版本号



11.55 - 11.59

 that if you pass an version

如果你传入了一个版本号

11.59-12.04

 then zookeeper only actually does the write if the current version number of the file is equal to the number you passed in

ZooKeeper只有当该文件的当前版本号和你传入的版本号一致时，才会执行写入操作

1204-12.11

 okay so let's see how we use this 

Ok，So，我们来看下该如何使用这个吧

12.11-12.15

the first maybe almost this first very simple example is

首先，我们来看下一个非常简单的例子

12.15-12.17

supposing we have a file in zookeeper

假设我们在ZooKeeper中有一个文件



12.17 - 12.20

and we want to store a number in that file 

我们想在这个文件中保存一个数字



12.20 - 12.21

and we want to be able to increment that number 

我们想要能够去增加这个数字的大小



12.21 - 12.31

so we're keeping maybe a statistics count and whenever a client you know I know gets a request from a web user or something it's going to increment that count in zookeeper 

所以我们要维护统计计数，来对应来自web端用户或者其他client想要增加这个Zookeeper文件中的计数的请求

so 我们要维护这个数字，应对无论什么时候，一个客户端得到一个从user来的web请求，想要去增加存放在zookeeper中的这个数字count的值

12.31-12.38

and more than one client may want to increment the count

可能有多个client想去增加这个count值

12.38-12.39

that's the critical thing

这很关键


12.39 - 12.43

so an example

So，来看个例子



12.43-12.48

 so one thing to sort of get out of the way 

So，为了解决这一点

12.48-12.58

is whether we actually need some specialized interface in order to support client coordination as opposed to just data 

我们是否需要某些专门的接口来支持client端之间的协调，而不是仅仅支持数据



12.58-13.00

this looks like a file system

这看起来就像是一个文件系统

13.00-13.07

 could we just provide the ordinary readwrite kind of file system stuff that dated that typical storage systems provide 

我们能否提供像标准存储系统所提供的那种普通的读写文件系统

13.07-13.08

so for example

So，比如说

13.08-13.14

 some of you have started and you'll all start soon Ladd 3 in which you build a key value store

你们有些已经开始或准备开始做lab3，即构建一个key/value存储服务


13.14-13.25

 where the two operations are the only operations are put key value and get key current value

这里面就两个操作，PUT(Key, Value)以及Get(key)拿到当前value

13.25-13.26

and so one question is 

So，其中一个问题是

13.26-13.29

can we do you know all these things that we might want to do with zookeeper

我们可以通过zookeeper来做两个操作么

13.29-13.33

can we just do them with lab 3 with a key with a key value put get interface 

我们可以通过lab3来实现这两个操作么


13.34 - 13.38

so supposing for my I want to implement this count thing 

假设我想要去实现这个计数需求



293

13.38 - 13.43

maybe I could implement the count with just lab threes key value interface

也许我正好可以通过lab3的key/value接口来实现这个计数




13.43 - 13.50

so you might increment the count by saying x equals get you know whatever key were using

增加count你可以这么做：取出K对应的值放到X变量中




13.50 - 13.58

 and then put that key an X plus 1

然后把X+1的结果更新到K中




13.59 - 14.08

why why is this a bad answer yes yes oh

为什么这么做有问题，你回答一下



14.08 - 14.12

it's not atomic that is absolutely the root of the problem here 

是的，它不是原子操作，这就是问题所在



14.15 - 14.18

and you know the abstract way of putting it

在这个抽象的处理过程中



14.19 - 14.23

 but one way of looking at it is that of two clients both want to increment the counter at the same time 

在同一时刻，有两个client都想增加计数



14.23 - 14.28

they're both gonna read they're both gonna use get to read the old value and get you know ten

它们都将使用读取到的值，假设为10好了



14.28 - 14.31

those gonna add one to ten and get 11

接下来10加1等于11



14.31 - 14.33

and I was gonna call put with 11 

然后我要更新k等于11



14.33 - 14.40

so now we've increased the counter by one but two clients were doing it so surely we should have ended up increasing it by two

所以现在我们已将计数器增加了1，但是两个client做这件事，结果肯定是要把计数器增加2的（知秋注：但这两个连续的操作不具备原子性，假如同一时间大家都时从master get获取的同一个值，但是调用put方法时，并没有得到预期的效果，最好的结果就是如果有版本号的设定，那第二个client操作会失败）



14.42 - 14.46

 so that's why the lab three cannot be used for even this simple example

这就是即使这个例子简单，也不能用lab3来做的原因



14.47 - 14.57

furthermore in the sort of zookeeper world where gets can return stale data is not lab 3 or gets are not allowed to return stale data 

此外，在zookeeper中，get可以返回旧数据，而不像lab3  get不可以返回旧数据



14.57 - 15.03

but in zookeeper reads can be stale and so if you read a stale version of the current counter and add one to it

在zookeepr中是可以读取到旧数据的，所以如果你读了当前计数器的旧版本数据，然后加1





15.03 - 15.05

you're now writing the wrong value 

你就会写入一个错误的值



15.05 - 15.13

you know if data values 11 but you're get returns a stale value of 10 you add 1 to that and put 11 that's a mistake 

例如如果当前数据值是11，但是你读取了一个旧版本的值是10，你加1并更新它，结果就不对了



15.14 - 15.15

because we really should have been putting 12 

因为我们知道应该更新为12的



15.15 - 15.23

so zookeeper has this additional problem that we have to worry about that that gets don't return the latest data

所以我们就必须要关注下get不能返回最新数据这个额外问题，



15.25 - 15.32

ok so how would you do this in zookeeper

那么在zookeeper中，你应该怎么处理呢



15.32 - 15.36

here's how I would do this in zookeeper

下面是我的处理方式



15.40 - 15.47

it turns out you need to do you need to wrap this code Siemens segments in a loop because it's not guaranteed to succeed the first time

事实证明你需要把代码片段包裹在一个循环中，因为这段代码它不能保证在第一次就能处理成功




15.48 - 15.54

 so we're just gonna say while true

所以我们先写个while true



15.56-16.00

we're gonna call get data to get the current value of the counter and the current version

这里我们会调用GETDATA来获取当前的count值，以及当前版本号


16.00-16.09

 so we're gonna say X V equals I'm get data 

所以，这里就是X,V接收GETDATA的返回值



16.09-16.12

and we need to say final name I don't care what the file name is we just say  f 

然后我们需要传入一个文件名，这里我不在意它是什么，所以我就用f进行表示



16.12-16.17

that nice now we get the well we get a value and a version number 

于是，现在我们拿到了一个值和一个版本号

16.18-16.20

possibly not fresh， possibly stale

可能它并不是最新的数据，可能它是过时的



16.20 - 16.21

but maybe fresh

但也可能是最新数据




16.21-16.30

 and then we're gonna use a conditional put a conditional setdata

然后我们可以在这里设置条件来进行put操作

然后我们要以put操作的返回值作为条件来设定if语句



16.45 - 16.48

and if set data is a set data operation return true

如果这个SETDATA操作返回的是true



16.48 - 16.52

 meaning it actually did set the value we're gonna break

这就意味着，它确实将值设定完成，那我们就会break退出循环

16.52-16.55

 otherwise just go back to the top of the loop

否则，它就会跳回这个循环的开头处


16.55 - 16.59

otherwise 

否则就是这样的



16.59 - 17.06

so what's going on here is that we read some value and some version number maybe still maybe fresh out of the replicas  

这段伪代码运行的过程是这样的：我们会读取一些值和一些版本号，这也许还是刚从replicas中读取到的



17.06 - 17.08

the set data we send actually did the zookeeper leader

SETDATA是我们实际上是发送给zookeeper leader的



17.09 - 17.10

because all writes go to the leader 

因为所有的写请求都会发给leader



17.10 - 17.19

and what this means is only set the value to X plus one if the version with the real version the latest version is still is V

SETDATA的用意就是，如果该数据当前最新的版本号仍然是V这个值的话，那么我们才会把value设置为X+1



17.19 - 17.27

so if we read fresh data and nothing else is going on in the system like no other clients are trying to increment this

所以如果我们读取的是新数据，也没有其他clients正在增加计数操作





17.27 - 17.34

 then we'll read the latest version latest value we'll add one to the latest value specify the latest version

那么我们将这两个读取到的最新的版本和值传入进来，并把这个版本对应的值加1



364

17.34 - 17.43

and our setdata will be accepted by the leader and we'll get back a positive reply to our request after it's committed and we'll break because we're done

leader会接受SETDATA请求，value更新后，我们会得到一个返回值true，并退出循环，整个处理流程结束



17.44 - 17.48

 if we got stale data here or this was fresh data

如果我们得到的是旧数据，或者得到的数据暂时是新的



17.50 - 17.58

but by the time our set data got to the leader some other clients set data and some other client is trying to increment their set data got there before us

但是，我们发送SETDATA到leader的时候，其他客户端已经设置了最新数据，而且还有其他客户端在我们之前已送达至leader并试图增加计数

 



17.58 - 18.02

our version number will no longer be fresh in either those cases this set data will fail 

在这一个场景中，我们的version号就不再是新的，SETDATA调用失败



18.03 - 18.05

and we'll get an error response back

我们会得到一个error 响应



18.05 - 18.08

 it won't break out of the loop

这段代码就不会退出循环



18.08 - 18.12

 and we'll go back and try again and hopefully we'll succeed this time 

我们就会回到循环最初并重新开始，希望我们会下次成功



18.15 - 18.25

yes  

什么问题



18.25 - 18.29

yes, so the question is could this it's a while loop or we guaranteed is ever going to finish

他的问题是这里有while循环，我们能否保证代码会执行结束，退出循环



18.29 - 18.33

and no no we're not really guaranteed that we're gonna finish 

答案是我们真不一定能保证代码会执行结束



18.33 - 18.48

in practice you know so for example if our replicas were reading from is cut off from the leader and permanently gives us stale data then you know maybe this is not gonna work out

在练习中，举个例子，如果我们的副本已经与leader切断了，副本的数据永远是旧数据，那么可能这段代码就在put的道路上一去不复返了



18.48 - 18.51

 but you know but in real life well

但是在真实场景中



18.51 - 18.58

in real life the you know leaders pushing all the replicas towards having identical data to the leader 

在真实场景中，leader会把它自身的数据推送到所有的副本中，以保证数据是完全相同的



18.58 - 19.03

so you know if we just got stale data here probably when we go back you know maybe we should sleep for 10 milliseconds or something at this point 

如果我们取得的是旧数据，我们可以在put处理失败时先休眠10毫秒或做其他操作以后，再回到上面的处理



19.04 - 19.08

but when we go back here eventually we're gonna see the latest data

最终，我们会重新取得最新的值和版本号



19.08 - 19.18

 the situation under which this might genuinely be pretty bad news is if there's a very high continuous load of increments from clients 

在这种处理逻辑下，可能真正令我们害怕的是，我们要处理来自客户端超大规模的持续性的计数负载



19.18 - 19.23

you know if we have a thousand clients all trying to do increments

如果我们有一千个客户端要去做增加计数操作



19.23 - 19.28

 the risk is that maybe none of them will succeed 

风险就是可能没有客户端能成功地执行结束



19.28 - 19.33

or something I think one of them will succeed because I think one of the most succeed 

或者我认为其中的一个会成功，因为我认为他们当中会有幸运儿的



19.35 - 19.40

because you know the the first one that gets its set data into the leader will succeed 

因为第一个取得并更新leader中数据的client，往往会成功



19.40 - 19.43

and the rest will all fail because their version numbers are all too low 

剩下的client都会失败，他们的版本号全都太低了



19.43 - 19.47

and then the next 999 will put and get data's in and one of them will succeed 

接下来，下一轮999个client中，也会有一个幸运儿成功的



19.48 - 19.57

so it all have a sort of N squared complexity to get through all of the all other clients which is very damaging but it will finish eventually

so 这段代码在所有的客户端都成功执行话，它的复杂度是N方复杂度（即N的阶乘），这性能是非常糟糕的，但它最终会完成的。



19.57 - 20.01

and so if you thought you were gonna have a lot of clients you would use a different strategy  here 

所以如果你认为你会有很多clients要进行这种情况的处理，你应该在这里使用不同的策略。



20.01 - 20.02

this is good for low-load  situations

这段代码只适用于低负载情况

========================================================

20.05-20.05

 yes

请问









八十四  阅举报
09-02
20.17-2018

if they fit in memory，it's no problem

如果它们是放在内存中，那么没问题

20.18-20.19

 if they don't fit memory

如果它们没放在内存中

20.19-20.20

it's a disaster

那么就是一场灾难



20.21 - 20.31

 so yeah when you're using ZooKeeper you have to keep in mind that it's yeah it's great for 100 megabytes of stuff and probablterrible for 100 gigabytes of stuff 

So，你必须要谨记的是：当你使用ZooKeeper时，存储100M数据是没问题的，100G的话那就不太妙了



20.31 - 20.38

so that's why people think of it as storing configuration information rather than their wild old data of your big website

所以，这就是人们用它存储配置信息而不是存储大网站的原始旧数据的原因



20.38

yes



20.52 - 20.56

 I mean it's sort of watch into this sequence

观察这个序列么

我的意思是在这个逻辑流程中会有一些（对数据版本状态）观察（知秋注：最终得以修改数据）



20.59 - 21.07

yet that could be so if we want if we wanted to fix this to work under high-load

这是可行的， 如果我们想要满足高负载的场景就需要修改这些处理逻辑



21.08 - 21.13

 then

那么




21.13 - 21.16

you would certainly want to sleep at this point where I'm not well

你可能应该修改sleep的相关的代码



21.17 - 21.20

the way I would fix this my instinct I'm fixing this

凭我的直觉，我会修改sleep相关的代码




21.21 - 21.25

 would be to insert a sleep here and furthermore double the amount of it sort of randomized sleep

这里会有个sleep的调用，此外我会随机加倍sleep的时间



21.30 - 21.33

whose span of randomness doubles each time we fail

哪个client的SETDATA设置失败，我们就让哪个client多多sleep，稍后再SETDATA



21.33 - 21.39

 and that's a sort of tried and true strategies exponential back-off is a

这样就会使 tried and true 策略的执行次数指数级下降



21.40 - 21.42

it's actually similar to raft leader election

实际上这么做和raft协议的leader选举类似



21.42 - 21.48

 it's a reasonable strategy for adapting to an unknown number of concurrent clients 

这就是一个自适应未知并发量场景的，一个合理的策略（我厉害吧）



21.47 - 21.54

so okay tell me what's right

好吧，你想说啥




22.03 - 22.08 ！！！！！！！

 okay so we're getting data and then watching its true

取得数据，然后加个监视器



22.17 - 22.22

so yes so if somebody else modifies the data before you call set data maybe you'll get a watch notification

如果有人在你设置数据前已经修改了数据，你也许会收到一个watch通知

So，如果其他人在你调用SETDATA之前修改了数据，你也许会收到一个watch通知

22.28 - 22.30

um the problem is the timing is not working in your favor

问题是，执行命令行的时机不是你所期望的那样




22.30 - 22.40

 like the amount of time between when I received the data here and when I send off the message to the leader with this new set data is zero

例如，从我取得数据到发送给leader更新命令（如setdata 0）之间的时间




22.40 - 22.42

that's how much time will pass here

即这期间需要花费的时间有多久



22.42 - 22.51

roughly and if some other client is sent in increment at about this time

简要的说，如果这期间有别的客户端这次已经发送更新命令了





22.51 - 22.53

 it's actually quite a long time between when that client sends in the increment and when it works its way through the leader and is sent out to the followers and actually executed the followers and the followers look it up in their watch table and send me a notification

那实际上所消耗的时间就要花费很久。别的客户端发送更新命令，通过leader来完成该命令。leader再发送followers命令，followers也执行完成。然后followers查看它们的watch table后找到了我这个client，并给我发送一个通知



23.03 - 23.06

so I think

所以，我认为



23.17 - 23.24

it won't give you any read result

ZooKeeper不会返回给你任何读请求结果的



23.25 - 23.33

or if you read at a point if you're gonna read at a point that's after where the modification occurred， that should raise the watch

或者如果你读请求的这个点在其他修改请求发生之后，那么应该触发watch事件了



23.33 - 23.37

you'll get the notification of the watch before you get the read response 

在你取得读响应之前，你会先得到watch的通知



23.37 - 23.42

but in any case I think nothing like this could save us 

但是我认为，在任何场景下，没有方法能像我提出的那样能帮助我们解决问题

23.42-23.48

because what's gonna happen is all thousand clients are gonna do the same thing whatever it is

因为这里会有数以千计的客户端要做同样的事情





23.47 - 23.50

right they're all gonna do get and set a watch and whatever they're all gonna get the notification at the same time they're all gonna make the same decision

它们都要去获取数据，并设定watch、同一时间取得通知、做出同样的决定



23.54 - 24.00

about well they're all not gonna get to watch because none of them has done the put data yet 

不对，它们不会得到watch通知，因为它们当中谁都没有成功更新数据



24.01 - 24.01

right

对吧



24.01 - 24.05

so the worst case is all the clients are starting at the same point 

最糟糕的例子就是，所有的客户端同时发出命令



24.05 - 24.11

they all do a get they all get version one， they all set a watch point they don't get a notification because no change has occurred

它们都取得了版本号1，它们都要设置watch点，它们不会获得通知的，因为值没有发生改变

它们都去调用了GETDATA来获取版本1的数据，它们都设置了watch点，但它们并不会收到任何通知，因为此时它们所监控的数据并没有发生改变



24.11 - 24.17

 they all send a set data RPC to the leader all thousand of them

然后，它们全都发送了SETDATA RPC请求给leader，这些请求数以千计的



24.17 - 24.26

the first one changes the data and now the other 999 and get a notification when it's too late because they've already sent the set data

第一个client改变了数据，那么其他999个client会得到通知，但是消息来的太晚了，它们已经发送了SETDATA RPC请求





24.26 - 24.31

 so it's possible that watch could help us here

所以在这个场景下，watch事件可能会对我们有所帮助（知秋注：只是可能有用，但其实没啥用，因为已经发送了set请求）



24.33 - 24.38

 but sort of straightforward version of watch 

但是只是简单版本的watch



24.38 - 24.43

I have a feeling if you wanted the the mail we'll talk about this in a few minutes

如果你还想要继续讨论的话，稍后我们可以通过邮件来交流



24.43 - 24.49

but the anon heard the second locking example

但是稍后你们会听到你们之前从未听到过的关于locking的第二个例子



24.49 - 24.51

 absolutely solves this kind of problem

它绝对能解决这类问题




24.51 - 24.54

 so we could adapt to the second locking example from the paper to try to cause the increments to happen one at a time， if there's a huge number of clients who want to do it 

如果遇上有大量客户端想要同时设定数据的场景，我们可以通过paper中的第二个 locking的例子来适配这个场景，以此来尝试让增量更新操作同一时间只发生一次



25.01 - 25.04

other questions about this example 

还有其他问题吗





25.06 - 25.12

okay this is an example of a way many people call a mini-transaction

这个例子，就是一些人所提到的mini-transaction



25.11 - 25.12

 all right



25.13 - 25.17

it's transactional in a sense that wow there's you know a lot of funny stuff happening here

它是某种意义上的事务性的，这里发生很多有趣的事情



25.17 - 25.26

the effect is that once it all succeeds we have achieved an atomic read-modify-write of the counter

效果就是一旦处理全部成功，那么我们就实现了一个具备原子性的读-改-写能力的计数器



25.26 - 25.26

right




25.26 - 25.35

the difficulty here is that it's not atomic

难点就是，这里的读和写不是原子性的



25.35 - 25.38

 the reading the write, the read the modifying the write are not atomic 

读改写都不是原子性的



25.37 - 25.44

the thing that we have pulled off here is that this sequence once it finishes is atomic

我们已经实现的事情是，当这段代码一旦完成，这段代码执行的序列就是原子性的



25.44 - 25.47

 right 



25.47 - 25.50

we actually man and once we have to be on the pass through this that we succeeded

实际上，一旦我们这段程序执行通过，我们就达成要求了


25.50 - 25.56

we managed to read，increment and write without anything else intervening

我们设法没有其他任何干预地读取增量和写入数值

在没有其他任何干预的情况下，我们会设法通过GETDATA来读取数据，然后我们会对数据进行写入（对值加一，设定版本号）

25.56 - 25.59

we managed to do these two steps atomically

我们设法原子性的做这两步操作

我们设法让这两步操作变成原子性操作

26.01 - 26.04

and you know this is not

你知道这个不是



26.05 - 26.09

because this isn't a full database transaction like real databases allow fully general transactions

因为这个不是一个像真实数据库系统那样完整的数据库事务，支持完整的普通事务



26.09 - 26.17

where you can say start transaction and then read or write anything you like maybe thousands of different data items whatever who knows what and then say end transaction

就像你可以开始事务，然后根据你的需要对任何信息进行读或者写，所读写的这些信息也许是数以千计不同的数据item或别的什么内容，然后结束事务



26.17 - 26.21

and the database will cleverly commit the whole thing as an atomic transaction 

数据库会很聪明的以原子事务来提交整个内容



26.21 - 26.23

so real transactions can be very complicated

所以真正的事务会非常复杂



26.23 - 26.35

ZooKeeper supports this extremely simplified version of you know when you're sort of one we can do it atomic sort of operations on one piece of data

ZooKeeper支持这种极简版的事务，比如：我们可以对一块数据做原子操作



26.35 - 26.38

 but it's enough to get increment and some other things

但是对于这里进行加一并处理一些其他事情来说，ZooKeeper提供的就足够了




26.39 - 26.49

so these are for that reason since they're not general，but they do provide atomicity these are often called mini transactions

这就是它不是常规事务，但仍能提供原子性操作而被称为为mini-transactions的原因了

So，基于这个原因，它们并不是我们通常所见的事务，但它们确实为我们提供了原子性操作，它被叫做mini-transactions

26.50 - 26.54

 and it turns out this pattern can be made to work with various other things too like

事实证明，这个模式也可以与其他各种需求一起工作，例如



26.54 - 26.59

if we wanted to do the test and set that vmware ft requires

如果我们想要实现vmware ft需要的test-and-set服务




27.00 - 27.02

it can be implemented with very much this setup 

它可以通过这个设定来实现




27.02 - 27.08

you know maybe the old value if it's zero then we try to set it to one but give this version number 

也许这个旧值是0，那么我们试着将值设定1，并传入这个版本号



27.08 - 27.12

you know nobody else intervened and we were the one who actually managed to set it to one

在没有其他操作介入的前提下， 我们就会设法把值设定为1



27.12 - 27.16

because the version number hadn't changed but i'm leader got our request and we win

因为，除了leader得到我们请求以外，这个版本号是没有改变的，所以我们就操作成功了

因为这个简单锁的存在，在leader在得到我们请求并获得执行权（即获取到锁）执行之前，这个版本号是没有改变，所以在获取到锁执行后，我们就操作成功了


27.16 - 27.22

 somebody else changes to one after we read it then the leader will tell us that we lost

但是在我们读取值以后，其他操作成功改变了这个值，那么leader就会告诉我们本次setdata操作失败（知秋注：get到的数据的version已经落后最新的版本了）



27.23 - 27.27

so you can do test and set with this pattern also

所以，可以通过这个模式来做test-and-set操作



27.27 - 27.30

 and you should remember this is the strategy

并且，你应当记住这种策略



27.34 - 27.35

okay 





27.36 - 27.42

alright next example I want to talk about is these locks 

我想谈论的下个例子是关于这些锁的



27.42 - 27.48

and I'm talking  about this because it's in the paper not because I strongly believe that this kind of lock is useful

我谈论这个例子是因为相关内容论文中有叙述，而不是因为我坚信这种锁很有用




27.48 - 27.58

but they have they have an example in which a choir acquire has a couple steps 

这里有个例子，acquire有两个步骤


27.59 - 28.16

one we try to create we have a lock file and we try to create the lock file now again some file with a ephemeral set to true

步骤1，我们会尝试创建一个临时的lock文件（ephemeral 设定为True）



28.17 - 28.22

and so if that succeeds then or not we've acquired the lock 

如果成功的话， 我们就会获得这个锁




28.22 - 28.34

the second step that doesn't succeed ，then we want to wait for whoever did acquire the lock

步骤2，如果获取锁没有成功，我们就等待某个获得该lock文件的客户端

步骤2，如果创建失败，也就是该文件已经有创建，那我们就等待某个拿到锁的客户端释放锁

28.34 - 28.35

what if this isn't true

即如果返回值不是true

28.35-28.37

 that means the lock file already exists 

就意味着这个lock文件已经存在

28.35-28.40

I mean somebody else has acquired the lock and so we want to wait for them to release the lock 

某些人已经持有这个锁，所以我们就要等待它们把锁释放



28.40 - 28.43

and they're gonna release the lock by deleting this file so we're gonna watch

它们通过删除文件把锁释放，那么我们就得拿到这个watch通知



28.43 - 28.44

 yes

什么问题




28.51-28.54

oh yes, you're right, sorry about that

小伙儿你很不错，我给你讲知识，你跟我谈拼写

没错，你是对的，这里的t我写成了l

28.56 - 29.11

alright so we're gonna watch we're gonna gonna call exists and watching is true

我们会调用exists函数来观察这个文件是否存在



29.11 - 29.15

now it turns out that um okay and and

现在，事实证明




29.15 - 29.21

and if the file still exists right which we expect it to because after all they didn't exist presumably would have returned here

如果文件存在就是我们所期待的那样，因为毕竟如果文件不存在，我们就会在步骤一处成功创建并返回了




29.21 - 29.30

so if it exists we want to wait for the notification we're waiting for this watch notification call this three 

如果文件存在， 我们就会等待步骤2的watch通知，这步作为步骤3



29.29 - 29.39

and a step four goto one 

步骤4，跳转到步骤1



29.39 - 29.41

so the usual deal is you know we call create

通常的做法是我们先创建lock文件



29.41 - 29.44

you know maybe we win

也许我们成功创建lock文件了




29.44 - 29.51

if it fails we wait for whoever owns a lock to release it we get the watch notification when the file is deleted at that point

如果创建锁lock文件失败，我们就要等待某个持有锁的client把锁释放，当lock文件删除（锁释放）的时候， 我们会在这个点得到watch通知




29.51 - 29.55

 this wait finishes and we go back to one and try to recreate the file hopefully we will get the file this time

当wait执行完毕，我们就会回到步骤1尝试重新创建文件，希望我们这次能取得lock文件





29.59 - 30.06

okay so we should ask ourselves questions about possible interleavings of other clients activities with our four steps

那么我们应该思考下这个问题，其他client有可能在这四个步骤的代码中交错执行么



30.06 - 30.09

so one we know for sure we know of already 

步骤1我们已经可以肯定

So，我们已经确信的一点是

30.09-30.11

if another client calls create at the same time

如果其他client在同一时刻调用create函数



30.11 - 30.20

 then the ZooKeeper leader is going to process those two create rpcs one at a time in some order

ZooKeeper leader会以某种顺序来处理这两个create rpc请求，即一次执行一个create rpc请求



30.20 - 30.25

so either my create would be executed first or the other clients create will be executed first 

所以要么先执行我的create请求，要么先执行其他客户端的请求



30.26 - 30.33

minds executed first i'm going to get a true back in return and acquire the lock and the other client is guaranteed to get a false return

如果我先执行，我会得到返回值true并获得这个锁。我们保证其他client会得到一个false返回值



30.33 - 30.38

and if there are rpcs processed first they'll get the true return and i'm guaranteed to get the false return

如果其他clients create rpc先执行，它们会得到true返回值，我保证会得到false的返回值



30.38 - 30.39

and in either case the file will be created

不管在哪种情况下，lock文件都会被创建的



30.40 - 30.49

so we're okay if we have simultaneous, executions of one

如果同时执行create， 也是没问题的。只有一个create能成功



30.50 - 30.51

another question is 

另一个问题是

30.51-30.57

well you know if I if create doesn't succeed for me and I'm gonna call exists

如果我的create没有成功执行，我会调用exists函数




30.57 - 31.05

what happens if the lock is released actually between the create and the exists

然而在exists调用之前，lock被释放了，释放的时间刚好卡在create与exists调用之间，这段程序会发生什么呢




31.09 - 31.18

so this is the reason why I rap I have an knife "f" around me  the around the exists is because it actually might be released before I call exists

这就是我围绕exists参数f提问的原因了。因为实际上它可能会在我调用exists函数之前被释放掉

这就是为什么在exists函数中我放入一个f的原因了，因为实际上它可能会在我调用exists函数之前被释放掉

31.19 - 31.22

because it could have been acquired quite a long time ago by some other client 

因为lock文件可能已经被某些client持有了很长一段时间了




31.22 - 31.24

and then if the file doesn't exist at this point 

如果这个文件在这个时间点不存在



31.25 - 31.30

then this will fail and I'll just go directly back to this go to one and try again

那么步骤2就会失败，那么我就要直接返回到步骤1，重新执行了



31.32 - 31.34

similarly and actually more interesting

类似的，实际上会发生的有意思的问题是



31.35 - 31.50 （ 这个问题就比较有趣了）

is what happens if the whoever holds it now releases it just as I call exist or as the replica I'm talking to is in the middle of processing my exists requests

某些持有锁的客户端刚刚释放掉锁，我的exist函数就马上调用了。或者，作为replica，在我exists请求执行过程中，如果目标锁文件被释放掉，会发生什么情况呢



31.50 - 31.54

and the answer to that is that the whatever replica I'm looking at you know

答案是，无论我查看的是哪个replica



31.57 - 32.03

it's log or guaranteed that writes occur in some order right

它的日志会保证以某种顺序来正确的写操作




32.04 - 32.09

so the replica I'm talking to it's it's log its proceeding in some way 

我要说的replica以某种方式来执行

So，replica会去查看它的日志，并以某种方式来执行


32.10 - 32.19

and my exists call is guaranteed to be executed between two log entries in the right stream

exists的调用保证在正确的流中，两个log条目之间执行（知秋注：exists保证会在两个写操作之间调用）




32.19 - 32.22

 right this is a this is a read-only request 

这是一个只读请求（知秋注：可能会请求到replica上去）




32.22 - 32.27

and you know the problem is that somebody's delete request is being processed at about this time

问题是在这个时间点，某个client的delete请求正在处理中




32.27 - 32.36

 so somewhere in the log is going either is going to be the delete request from the other client 

因为来自其他client的delete请求，会反映在log的某个位置




32.36 - 32.49

and the replica and you know this is my mind the replica that I'm talking to ZooKeeper replicas I'm talking to his log my watch my exists RPC is either processed completely processed here

与我通信的ZooKeeper replicas，我的exists rpc和watch事件都会在它log的这个位置上完全执行成功





32.50 - 32.54

in which case the replica sees oh the file still exists

在某个场景下，replica会看到lock文件依然存在




32.54 - 33.00

and the replica inserts the watch information into its watch table at this point

在这个时间点，replica会插入watch信息到它的watch表中



33.00 - 33.02

 and only then executes the delete

然后才执行删除操作



33.02 - 33.04

so when the delete comes in

因此当delete请求进来时



33.04-3308

 were guaranteed that my watch request is in the replicas watch table and it will send me a notification

确保我的watch请求在replica的watch表中，并且该replica会给我（client）发送通知




33.09 - 33.18

 right or my exist requests is executed here at a point after the delete happen the file doesn't exist

即在delete执行完毕后，该lock文件就不存在了，此时执行我的exist请求

或者说我们的exist 请求在这里执行，即在delete发生之后，文件已经不存在的情况下执行了我们的exist请求


33.18 - 33.24

 and so now the call returns true and know well actually a watch table entry is entered but we don't care 

这是调用exists，记录了一个watch table的条目，但是我们就不在意后续操作了

于是exists方法给我们返回了true，watch表上记录了一条条目，但我们并不在意

现在，这个watch回调会返回一个true，其实会去访问watch表上记录的一条entry信息，但我们无须在意

33.24 - 33.25

right 



33.27 - 33.38

so it's quite important that the writes are sequenced and that reads happen at definite points between writes

所以，确定多个写请求的序列以及读请求发生在多个写请求之间的位置，这是十分重要的。

So，我们得确定这些写请求是有序的，并且这些读请求所执行的位置是在写请求之间，这相当重要



33.38 - 33.54

yes 

请问




33.54 - 33.56 ！！！！！！！

well okay so yes so this is where the exists is executed the file doesn't exist at this point

exists请求执行的时候，此时lock文件并不存在




33.58 - 34.02

 exists returns false we don't wait we go to one 

exists 返回false，我们不用执行第三步进行等待， 直接进入第四步，即转到步骤一



34.02 - 34.05

we create the file and return 

我们创建lock文件并返回



34.07 - 34.12

we did install a watch here that watch will be triggered it doesn't really matter because we're not really waiting for it 

我们设置了一个watch在这，所以说，watch事件会被触发，但这真的不重要，因为我们不一定需要等待锁释放



34.12 - 34.16

but the watch will be triggered by this created

当lock文件被创建时，watch事件会被触发



34.16-34.23

学生提问中



34.23 - 34.26

we're not waiting for it but yeah okay

我们不会等待锁释放



34.26 - 34.28

so the file doesn't exist we go to one

因为文件不存在，我们会跳转到步骤一



34.28 - 34.33

somebody else has created the file we try to create the file that fails 

可能某些客户端已经创建了文件，那么我们尝试创建文件会失败




34.33 - 34.37

we install another watch and this watch that we're not waiting for

我们设置了另一个watch事件，它不是步骤三要等待的事件



34.38 - 34.40

 so this way does not a wait for anything to happen

步骤三不是要等待某些事件的发生



34.41 - 34.43

although it doesn't really matter in the moment 

尽管这个时候它没什么意义



34.43 - 34.49

it's not harmful to to to break out of this loop early it's just wasteful

早些退出循环不会造成危害，就是有些浪费计算与I/O资源



34.51 - 34.57

anyway we've all the history this code leaves watches sort of in the system

无论如何，我们会在系统中把这些代码操作的所有历史记录保存下来



34.57 - 35.03

 and I don't really know what does my new watch on the same file override my old watch I'm not actually sure

我实际上真的不知道，对于同一个文件，新watch事件是否会覆盖原有的watch事件



35.08 - 35.16

okay I'm finally this example and the previous example suffle suffer from the herd effect

这个例子和上一个例子都是受到了惊群效应的影响



35.14 - 35.23

 we also herd effect we talked about I mean what we were talking about when we were worrying about oh but if clients I'll try to increment this at the same time

我们所谈论的惊群效应，是指我们担心当所有的客户端在同一时刻尝试发起增量修改请求时，所产生的影响



35.23 - 35.29

gosh that's going to have N squared complexity as far as how long it takes to get to all thousand clients 

直到数以千计的clients完成操作时，程序的时间复杂度是O(N^2)



35.29 - 35.32

this lock scheme also suffers from the herd effect in that 

这个锁机制也受到了惊群效应的影响



35.33 - 35.36

if there are a thousand clients trying to get the lock

如果有1000个clients尝试获得锁



35.36 - 35.45

 then the amount of time that's required to sort of grant the lock to each one of the thousand clients is proportional to a thousand squared

那么1000个客户端中，每一个客户端获得锁所花费的时间，与1000的平方成正比



35.45 - 35.54

 because after every release all of the remaining clients get triggered by this watch all of the remaining clients go back up here and send in a create

因为每次释放锁，所有剩下的clients都会收到watch通知，所有剩下的clients都会返回到步骤一发送create请求



35.54 - 36.00

and so the total number create our rpcs generated is basically a thousand squared

所以，我们创建请求的的总数基本上是1000^2（知秋注：其实是1000 999 998 997...1 等差数列求和，1000*500，按复杂度来算去掉抽取出二分之一，为1000^2）




36.00 - 36.14

so this suffers from this herd the whole herd of waiting clients  is beating on ZooKeeper 

所以这就是由大量等待clients造成的惊群效应（一石激起千层浪）

=======================================================================

36.15 - 36.21

another name for this is that it's a non scalable lock or yeah 

另一种叫法就是non-scalable锁（不可扩展锁）



36.21 - 36.29

okay and so the paper is a real deal and we'll see it more and in other systems  

So，这论文就是一个真正的解决方案，我们后面会看到它更多的内容，也会在其他系统中看到它的身影



36.30 - 36.35

and soon enough serious end of problems the paper actually talks about how to solve it using ZooKeeper

马上，我们就解决这棘手的问题，论文实际上讲述了如何使用ZooKeeper解决这个问题





36.35 - 36.46

and the interesting thing is that ZooKeeper it's actually expressive enough to be able to build a more complex lock scheme

有趣的是，ZooKeeper它实际有足够的表现力，能构建更复杂的锁方案





36.46 - 36.50

that doesn't suffer from this herd effect  that even of a thousand clients are waiting

即使有上千个clients在等待，通过这个方案也能免于惊群效应的影响





36.50 - 36.58

 the cost of one client giving up a lock and another acquiring it is order 1 instead of order n

对于一个client释放锁，另一个client获取锁时所付出的成本来说，它的复杂度应该是O(1)而不是O(n)

36.59 - 37.09

and this is the because it's a little bit complex this is the pseudocode in the paper in section 2.4 it's on page 6 if you want to follow along

这个有一些复杂，如果你想要参照的话，相关的伪代码在论文的第六页2.4章节




37.09 - 37.22

so this is lock without herd 

So，这就是互斥锁（lock with herd）



37.23 - 37.26

and so this time there is not a single lock file

So，此时，就不是一个单独的lock文件了



37.27 - 37.28

there's no, I'm sorry yes

请问



37.37 - 37.40

 it is just a name that allows us to all talk about the same lock

它只是一个名称，允许我们所有人都是在谈论同一把锁



37.42 - 37.50

so it's just a name know 

仅仅是个名称罢了



37.50 - 37.55

now I've acquired the lock and I can do I can whatever the lock was protecting

我获得了锁，然后在锁的保护下，我能够做些我想做的



37.55 - 38.00

you know maybe only one of us at a time should be allowed to give a lecture in this lecture hall

例如，在这个讲演厅，可能一次只允许我们其中一人来做讲演




38.00 - 38.06

 if you want to give a lecture in this lecture hall you first have to acquire the lock called 34100

如果你想在这个演讲厅做演讲，你首先要取得一个锁，叫做34100



38.07 - 38.13

the that turns out it's yes it's a Znode and ZooKeeper but it like nobody cares about its contents

事实证明这个锁就是个znode，没人会关心它的内容



38.14 - 38.16

we just need it to be able to agree on a name for the lock

我们只是需要这个znode能够接受lock的命名而已



38.21 - 38.24

that's the sense in which that's file this it looks like a file system but it's really a naming system

这就是file的意义，它看起来像文件系统，但实际上是个命名系统



38.27 - 38.28

 alright




38.28 - 38.34

so step one is we create a sequential file

步骤1，我们创建一个有序文件



38.37 - 38.39

and so yeah we give it a prefix name 

我们给它一个前缀名



38.39 - 38.47

but what it actually creates is you know if this is the 27th file sequential file created with with prefix F

但如果实际上创建的是以F为前缀的第27个文件



38.48 - 38.53

you know maybe we get F 27 or something and and

你知道的，也许我们获得了F27这个文件或者别的什么



38.54 - 39.05

in the sequenced in the sequence of writes that ZooKeeper is it's working through successive creates get ascending guaranteed ascending never descending

在写请求的顺序中，ZooKeeper会保证它以连续的升序创建文件，绝不会降序



39.05 - 39.10

always ascending sequence numbers when you create a sequential file 

当你要创建一个顺序文件，zookeepr一直会连续升序的创建序号



39.15 - 39.18

there was an operation I left off， from the list it turns out you can get a list of files

我忘说了一个操作，那就是list，通过它，我们能得到一个文件列表


39.22 - 39.20

you can get a list of files underneath you give the name of znode that's actually a directory with files in it you can get a list of all the files that are currently in that directory 

你传入指定的znode，你能得到一个文件列表，内容是znode目录下当前所包含的所有文件




39.33 - 39.42

so we're gonna list the files let's start with that you know maybe list f star we get some list back

So，我们往LIST命令中传入f*，这样我们就可以得到的以f开头的文件



39.42 - 39.49

 we create a file with the system allocated us a number here we can look at that number

我们通过系统分配给我们的一个数字创建了一个文件，我们能看到这个数字





39.49 - 39.55

 if there's no lower numbered file in this list then we win and we get the lock

如果文件列表中没有更小数字文件，那么代表我们成功了，获得了这个锁



39.55 - 40.01

so if our sequential file is the lowest number file with that name prefix we win

如果我们所创建的顺序文件是最小数字的文件，我们就赢得了锁






40.01 - 40.13

so no lower number we've acquired the lock and we can return

所以没有更小的数字，我们就获得了锁并返回



40.14 - 40.20

 if there is one then again what we want to wait for 

如果这里需要我们再一次的等待，我们要等待什么呢



40.21 - 40.30

then what's going on is that these sequentially numbered files are setting up the order in which the lock is going to be granted to the different clients

这里要发生的就是，这些按顺序编号的文件，就是不同clients获得锁的顺序



40.30 - 40.43

so if we're not the winner of the lock what we need to do is wait for the previously numbered with the client who created the previously numbered file to release to acquire and then release the lock

因此如果我们不是lock的获得者，如果我们前面有很多文件，我们需要做的就是等待前一个命名文件的创建client获得锁，并等待它释放锁



40.43 - 40.45

and we're going to release the lock

如果我们打算释放锁



40.45 - 40.52

the convention for releasing the locking in this system is for to remove the file, to remove your sequential file

在系统中，释放锁的约定就是删除你创建的顺序文件



40.52 - 40.57

so we want to wait for the previously numbered sequential file to be deleted 

所以我们要等待上一个顺序命名的文件被删除掉



40.57 - 41.00

and then it's our turn and we get the lock 

那么就轮到我们获得锁了





41.01 - 41.02

so we need to call exists

So，我们需要去调用exists





六十一  阅举报
09-03


41.01 - 41.09

so we need to call exists so we're gonna say if the call exists mostly to set a watch point

因此我们需要调用exists函数，我们要表明调用exists主要是为了设定watch点

So，基本上，如果我们调用exists，我们都会去设置一个watch事件


41.09 - 41.21

so it's you know next lower number file and we want to have a watch 

So，我们要传入下一个更小编号的文件（next lower number file），我们要需要对它进行监控


41.22 - 41.25

get that file still exist we're gonna wait 

如果这个文件依然存在的话，我们会继续等待



41.25 - 41.28

and then so that's step 5

然后这个处理（即wait）是步骤5



41.28 - 41.33

and then finally we're gonna go back to we're not going to create the file again

然后最终我们会返回到，我们不会再次返回到创建文件的地方（即第一步）

 


41.33 - 41.41

because it already exists we're gonna go back to listing the yeah the files 

因为文件已经存在了，我们要返回到列出文件这里，即步骤2



41.41 - 41.50 （废话？）

so this is acquired releases just I delete if I acquire the lock I delete my the file I created complete with my number

所以这就是，获取，释放操作，删除，如果获取到锁，在我要执行的业务逻辑完成后，我会删除用我的数字创建的文件



41.50 - 41.53

yes

请问



41.54 - 41.59

why do you need to list the files again

为什么需要重新列出文件



42.02 - 42.03

that's a good question 

这是个好问题




42.03 - 42.10

so the question is we got the list of files we know the next lower number file

答案就是如果我们得到文件列表，就能知道下一个较低编号的文件了



42.11 - 42.13

there's a guarantee of the sequential file creation is 

我们会对有序文件的创建提供一种保证

42.13-42.16

that once file 27 is created 

一旦f27被创建出来

42.16-42.0

no file with a lower number will ever subsequently be created

那么随后就不会有更低编号的文件被创建（比如f26）



42.20 - 42.22

so we now know nothing else could sneak in here 

因此这里就不会有其他隐蔽的动作发生



42.22 - 42.31

so how could the next lower number file you know why why do we need to list again why don't we just go back to waiting for that same lower numbered file 

下一个低编号文件是什么状态，为什么我们需要重新列出文件，为什么不需要返回去等待相同的低编号文件



42.34 - 42.37

anybody guess the answer

有人猜到这个答案了么





42.43 - 42.55

I mean the the the way this code works the answer to the question is whoever was the next lowered person might have either acquired him at least the lock before we noticed 

我的意思是这段代码的运行方式就是这个问题的答案，不管下一个较低序号的是谁，可能在我们注意到之前，他已经获得锁了（因为当前client只关注小于它的下一个文件）



42.55 - 42.59

or have died and this went 

或者已经消亡了



43.00-43.01

and these are transient files

这些都是临时文件


43.01-43.08

 sorry or whatever they're called ephemeral file

它们被叫做临时文件

43.08-43.16

even if we're 27th in line

即使我们创建的是第27个文件f27

43.16-43.19

 number 26 may have died before getting the lock

在我们拿到锁前，f26可能就挂掉了

43.19-43.23

if number 26 dies， the system automatically deletes their ephemeral files 

如果第26个挂掉了，那么系统就会自动删除它们的临时文件

43.23-43.26

and so if that happened

So，如果这种情况发生的话

43.26-43.28

 now we need to wait for number 25 

那么，现在我们就得去等待25

43.28-43.30

that is the next

它就是下一个数字较低编号文件（next lower number file）

43.30-43.37

 you know it if all files you know 2 through 27 and and we're 27 if they're all they are and they're all waiting there's a lock

如果所有文件是从2到27，我们是27。它们都在等待获取锁

43.37-43.40

if the one before is dies before getting the lock

如果某个client在得到锁之前，前面的那个文件挂掉了

43.40-43.43

 now we need to wait for the next next lower number file 

现在，我们就得去等待下下个数字较低的编号文件

43.43-43.46

because the next lower one is has gone away

这是因为下个数字较低的编号文件已经不见了

43.46-43.54

so that's why we have to go back and relist the files in case  our predecessor in the list of waiting clients turned out to died

So，这就是为什么如果处于等待列表中的前一个client挂掉了，我们就得回过头去，并重新列出所有文件的原因了

43.54-43.59

 yes

请讲



44.03-44.06

if there's no lower numbered file，then you have acquired the lock

如果此处没有较低编号的文件，那么你就已经获取到了锁

44.06-44.09

absolutely

确实如此

44.09-44.10

yes

请问

44.10-44.16

how does this not suffer from the herd effect 

它是如何避免受到惊群效应的影响

44.16-44.21

suppose we have a thousand clients waiting 

假设我们有1000个正在等待获取锁的client

44.21-44.27

and currently client made through the first five hundred client five hundred holds the lock

比如，当前第500个client持有着锁

44.27-44。31

and every client waiting 

每个client都在等待获取锁


44.31-44.34

every client is sitting here waiting for an event

每个client都处于第四步的阶段，并等待一个事件的发生

44.34-44.43

but only the client that created file 501 is waiting for the releasing of file 500

但创建了第501个文件的那个client在等待第500个文件被释放


44.43-44.45

 so everybody's waiting for the next lower number

So，每个client都在等待下一个数字较低文件编号

44.45-44.48

 so five hundred is waiting for 499

So，第500个client正在等待第499号文件

44.48-44.50

four hundred nine nine is

第499个client正在等待第498个文件

44.50-44.53

but everybody everybody's waiting for just one file

每个client都在等待这一个文件

44.53 - 44.54

when I release the lock

当我释放锁时

44.54-44.59

there's only one other client the next higher numbered client  that's waiting for my file 

只有一个客户端，下一个创建较高编号文件的客户端在等待我文件的删除

只有那个具有下一个数字较高的编号的client在等待我的文件被删除

44.59 - 45.00

so when I release the lock

So，当我释放这个锁的时候

45.00-45.02

 one client gets a notification

一个client就会收到这个通知



45.02 - 45.07

one client goes back and lists the files

这个client就会回过头来列出文件



45.07 - 45.11

one client and one client now has the lock

那么现在，这个client就拿到了锁



45.11 - 45.14

 so the sort of expense you know 

所以这段处理策略的开销



45.14 - 45.22

no matter how many clients that are the expense of one of each release and acquire is a constant number of RPCs

不用担心有多少客户端，每一次释放锁和获得锁的开销是常数级的RPC调用数


45.22 - 45.31

where's the expense of a release and acquire here is that every single waiting client is notified 

在这里发布和获取的开销是，每个等待的客户端都会收到通知

这里释放和获取锁的开销就是得去通知每个等待的client




45.31 - 45.36

and every single one of them sends a write request and the create request into zookeeper

每个client都会发送一个写请求和create请求到Zookeeper中





45.53 - 45.54

oh you're free to get a cup of coffee

你可以免费喝杯咖啡

Oh，回头我请你喝杯买免费咖啡

45.54 - 46.01

yeah I mean this is you know what the programming interface looks like is not our business 

我的意思是，程序接口长什么样不关我们的事儿



46.01 - 46.07

there's two options for what this actually means for us what the program looks like

就程序的表象而言，这实际上意味着两个选项



46.07 - 46.13

one is there's some thread that's actually in asynchronous wait

其中之一就是一些线程实际上是在异步等待



46.13 - 46.14

it's made a function call saying please acquire this lock 

这里要进行一次函数调用来获取这个锁

这里要进行一次函数调用来告诉它，请获取这个锁


46.14-46.19

and the function hold doesn't return until the locks finally acquired  or the notification comes back

这是一个获得锁的函数调用，直到最终释放锁后，这个函数才会返回。否则会重复接收通知

直到最后拿到锁或者返回通知的时候，该函数才会持有锁

这个函数只有在锁最终被client获取到时，才会返回或返回个通知

46.20-46.25

of much more sophisticated interface would being one in which you fire off requests a zookeeper  and don't wait 

你代码中会有很多更复杂的接口（interface），你会在其中对zookeeper发出请求，没有等待（知秋注：使用了异步）

46.25 - 46.30

and then separately there's some way of seeing well as ZooKeeper said anything recently

然后在另一条线程通过某种方式来看ZooKeeper 最近的状态（知秋注：也就是zookeeper发出的通知会改变本地定义的某个字段状态）

46.30 - 46.37

 or I have some goroutine whose job it is just wait for the next whatever it is from zookeeper

或者我通过某个Goroutine来等待来自Zookeeper中的下一个通知

46.37-46.42

 in the same sense that you might read the apply Channel and just all kinds of interesting stuff comes up on the apply channel 

也就是说，你可能在读并从channel接收 你感兴趣的各种信息


46.42-46.45

so that's a more likely way to structure this 

So，这是一种更有可能的组织方式



46.45-46.50

but yeah you're totally either through threading or some sort of event-driven thing

但你完全可以通过多线程或者事件驱动的方式来做到

46.50-46.51

you can do something else while you're waiting  

当你处于等待的时候，你可以去做些事情

4652 - 4653

yes

请问



47.03 - 47.13

yes or if the person before me has neither died nor released

是的，如果在我之前的client既没有死亡也没有被释放锁



47.15 - 47.18

it's a file before me exists 

如果在我之前的那个文件是存在的

47.18-47.23

that means either that client is still alive and still waiting for the lock

这意味着，该client依然存活，并且依然在等待获取锁



47.23 - 47.28

 or still alive and holds the lock we don't really know

或者该clien依然t是存活的并且持有着锁，我们并不清楚是哪一种情况

47.35 - 47.39

it does it as long as that client 500 still alive 

只要客户端500依然或者，它就会执行操作

只要client 500依然活着

47.39 - 47.42

if if this exists fails

如果exists失败

47.42-47.46

 that means one of two things either my predecessor held the lock and is released it and deleted their file

意味着一个是我的前任持有锁，而且释放了锁并删除了文件

这意味着，其中一点是，我的前任持有锁，但之后将锁给释放了，并且它们的文件被Zookeeper删除了

47.47 - 47.50

or my predecessor didn't hold the lock  they exited 

或者，如果我的前任并没有持有锁，它们退出了（即前任出毛病挂掉了）



47.50-47.52

and zookeeper deleted their file

Zookeeper就会删除它们的文件



47.52 - 47.55

because it was an ephemeral file 

因为它是一个临时文件


47.55 - 48.01（观察文件被删除，退出wait状态）

so there's two reasons to come out of this wait

这两个原因来由wait产生的

基于这两个原因，我这个client端才可以退出wait状态（知秋注：怎么退出呢，如果是我的话，就在背后设定一条线程去维护一张watch表，当收到通知时，给等待的线程发出一个唤醒信号）



48.01 - 48.03

or for they exist to return false 

否则client调用exist返回false，就老老实实等待了



48.03 - 48.08

and that's why we have to like we check everything 

这就是为什么我们得检查所有东西的原因了



48.08 - 48.13

you know you really don't know what the situation is after the exists completes

你知道的，当exists执行完毕的时候，我们真的不知道要遇上的是什么情况

48.16-48.18

yeah

请问

48.30 - 48.34

that might that yeah maybe maybe  that could need to work that sounds reasonable

你说的这种应该可行，听起来很合理



48.34 - 48.38

and it preserves the sort of scalable nature of this 

它保留了可扩展性



48.39 - 48.44

and that each require release only involves a few clients two clients

每次获取锁和释放锁，只涉及到了两个客户端



48.48 - 48.58

alright this pattern to me  actually first saw this pattern a totally different context and scalable locks for threading systems Iike go

这个模式对我来讲，实际上也是第一次看到，它为像go语言中那样的线程系统提供了完全不同的上下文和scalable lock


48.59 - 49.05

this end in for most of the world this is called a scalable lock

在大多数场景下，这叫做scalable lock



49.10 - 49.18

I find it one of those interesting constructions I've ever seen now 

到目前为止，它是我所发现的那些我很感兴趣的构造之一



49.18 - 49.22

and so like I'm impressed that zookeeper is able to express it 

我之前强调过，Zookeeper能够做到这点

49.22 - 49.24

and it's a valuable construct

它是一个有价值的构造



49.25 - 49.27

having said that

话说回来

49.27-49.33

 I'm a little bit at sea about why zookeeper about why the paper talks about locks at all

我还是有点摸不着头脑，为什么zookeeper的论文里讲的全是锁

49.33 - 49.40

because these locks are not like threading locks and go

因为这些锁并不像是Go中的线程锁那样



49.41 - 49.44

because in threading there's no notion of threads failing

因为在线程中，没有线程失败的概念

49.44-49.48

 at least if you don't want them there to be there's no notions of threads just sort of randomly dying and go

至少如果你不希望它们存在，那么线程中也就没有这个概念，那就只能随机等待直至死亡或者抢到锁继续执行（知秋注：拿到锁的线程释放锁的同时给线程发出解除等待信号，如果未做特殊设计，等待线程往往需要进行同一把锁的争抢）



49.48 - 49.52

 and so really the only thing you're getting out of a mutex

因此，这是唯一能让你真正摆脱mutex的做法



49.52 - 49.56

 it's really the case and go that when you use it if everybody uses mutexes correctly 

确实如此，如果每个人都能正确使用Go中的mutex



49.56 - 50.01

you are getting atomicity for the sequence of operations inside the mutex

你将获得互斥锁内部操作序列的原子性

那么在面对一系列操作的情况下，你就能去利用mutex所为我们提供的原子性

50.02 - 50.08

that you know if you take out a lock and go and you do 47 different read and write a lot of variables and then release the lock

如果你获取到一把锁，对大量的变量进行了47次不同的读和写操作，然后释放锁



50.08 - 50.10

if everybody follows that locking strategy 

如果所有人都遵循这种锁策略



50.10 - 50.17

nobody's ever going to see some sort of weird intermediate version of the data as of halfway through you're updating it

那在更新的过程中，就没有人会看到更新这些数据时可能出现的某些奇怪的中间版本

50.18 - 50.20

right just makes things atomic no argument

即确保操作的原子性，这点毋庸置疑



50.20 - 50.21

 these locks aren't really like that 

这些锁并不能真的做到这点

50.21-50.25

because if the client that holds the lock fails 

因为如果client持有锁时，但它中途操作失败了

50.25 - 50.26

it just releases the lock 

它就会将锁释放

50.26-50.29

and somebody else can pick up the lock 

其他人就可以去拿到这把锁



50.30 - 50.31

so it does not guarantee atomicity 

So，它并不保证原子性



50.32 - 50.34

because you can get partial failures in distributed systems

因为在分布式系统中，你会遇上部分失败的情况

50.34-50.39

 where you don't really get partial failures of ordinary threaded code 

然而，这种部分失败的情况，你不会在普通的多线程代码中遇到



50.41 - 50.43

so if the current lock holder had the lock 

So，如果当前锁持有者持有着锁



50.43 - 50.47

and needed to update a whole bunch of things that were protected by that lock before releasing

它需要在释放锁之前去更新一堆被该锁所保护的东西

50.47 - 50.51

 and only got halfway through updating this stuff and then crashed

然后，在它将内容更新了一半后，它就崩溃了



50.51 - 50.53

then the lock will get released 

然后，锁就会被释放掉

50.53-50.54

you'll get the lock 

接着，你就会拿到这个锁

50.54-50.57

and yet when you go to look at the data，it's garbage

当你去看这些数据的时候，它就是一团垃圾



50.58 - 51.01

 because it's just whatever random seed it was in the middle of updated

因为这些数据只被更新了一半



51.01 - 51.08

 so there's these locks don't by themselves provide the same atomicity guarantee that threading locks do 

So，这些锁自身和线程锁相比，它们无法提供相同的原子性保证

51.09 - 51.14

and so we're sort of left to imagine for ourselves by the paper or why you would want to use them 

So，看了paper，我们得有点自己的想法，为什么我们会想去使用这些锁



51.14 - 51.17

or why this is the sort of some of the main examples in the paper

为什么它们会作为paper中的重要例子



51.17 - 51.21

 so I think if you use locks like this

So，我觉得如果你以这种方式使用锁

51.21-51.25

 then you sort in a distributed system then you have two general options

在分布式系统中，你基本有两个选择



51.25 - 51.26

one is 

其中一点是

51.26-51.32

everybody who acquires a lock has to be prepared to clean up from some previous disaster

每个获取到锁的人都应该准备好去清理前一个拿着锁的人所遗留下来的灾难



51.30 - 51.33

 right so you acquire this lock 

So，你拿到了这个锁



51.33 - 51.35

you look at the data 

然后查看这些数据



51.35 - 51.39

you try to figure out gosh if the previous owner of the lock crashed

你会试着弄清楚，如果前一个持有该锁的持有者崩溃了





51.39 - 51.41

you know when I'm looking at the data

你知道的，当我查看这段数据时



51.41 - 51.49

you know how can I fix the data to make up how can I decide if the previous owner crashed and what do I do to fix up the data 

如果前一个拥有锁的持有者崩溃了，那么我该如何修复数据呢



51.49 - 51.51

and you can play that game

我们可以假设这样一个场景



51.51 - 51.57

especially if the convention is that you always update in a particular sequence

按照惯例，如果我们始终按照特定顺序进行更新操作



51.57 - 52.03

you may be able to detect where in that sequence the previous holder crashed，assuming they crashed 

假设前一个锁持有者崩溃了，那么你可能能够检测出前一个锁持有者是在该操作序列的哪个位置崩溃的

52.03-52.09

but it's a you know it's a tricky game the requires sort of a kind you don't need for like thread locking 

它就是个花活，使你不需要线程锁类型的操作

这个情况就好像，在你没有线程锁的情况下，你需要去处理这个棘手的问题一样（知秋注：就好像多线程情况下，没有锁所出现的中间状态数据，你需要去想办法处理）



52.09-52.13

um the other reason maybe these locks would make sense  is

这些锁有意义的另一个理由可能就是

52.13-52.18

if there's sort of soft locks protecting something that doesn't really matter

这些soft lock是用来保护那些不是很重要的东西

52.18-52.19 ！！！！！！！

so for example

So，例如

52.19-52.23

 if you're running MapReduce jobs map tasks reduce tasks

如果你在跑MapReduce，执行Map任务和Reduce任务

52.23-52.32

you could use this kind of lock to make sure only one task only one worker executed each task 

你可以使用这种锁来确保，每个任务只有一个worker来执行



52.32- 52.34

so workers gonna run task 37

So，比如，worker要运行task 37

52.34-52.36

it gets the lock for task 37

该worker就会拿到task 37的锁



52.36 - 52.39

execute it marks it as executed and releases it

它就会执行任务，并将task 37标记为已执行，然后释放锁

52.39-52.46

well the way MapReduce works it's actually proof against crashed workers anyway

总之，MapReduce的这种工作方式实际上是针对故障崩溃的worker的



52.46-52.50

so if you grab a lock

So，如果你抢到了一个锁

52.50-52.52

 and you crash halfway through your MapReduce job

接着，你执行MapReduce任务时，中途崩溃了

52.52-52.53

 so what？

So，这会怎么样呢？

52.53-52.56

 the next person who gets the lock，you know lock will be released when you crash

你知道的，当你崩溃了，你的锁就会被释放，那么下一个人就可以拿到锁

52.56-53.00

because your next person who gets it will see you didn't finish the task and just reexecute it 

下一个拿到该锁的人就会看到你并没有完成任务，那么它就会重新执行该任务

53.00-53.01

and it's just not a problem

这并不是什么问题



53.01 - 53.03

because of the way MapReduce is defined

因为MapReduce已经解决了这种问题



53.03 - 53.09

so you could use these locks or some kind of soft lock thing although anyway

So，你可以使用这些锁或者某些soft lock

53.09-53.12

and you know maybe the other thing which we should be thinking about is

我们应该去思考下其他事情


53.12-53.18

that some version of this be used to do things like elect a master

比如，我们可以使用该锁的某个变体来选出一个master

53.18-53.23

but if what we're really doing here is electing a master you know we could use code much like this

但如果我们使用这种锁来选出一个master，我们就会使用黑板上的这些代码

53.23-53.24

and that would probably be a reasonable approach

这可能是种合理的方式

53.24-53.25

yeah

请讲

53.42-53.45

oh yeah yeah yeah so the picking of paper talk that remember the text in the paper were says

还记得paper中说的么

53.45-53.46

it's going to delete the ready file 

Zookeeper会去删掉ready文件

53.46-53.49

and then do a bunch of updates to files 

接着，对文件进行一堆更新操作

53.49-53.50

and then recreate the ready file 

然后，重新创建ready文件

53.50-53.53

that would that is a fantastic way

这是一种很棒的方式

53.53-54.01

 of sort of detecting and coping with the possibility that the previous lock held or the previous master or whoever it is crashed halfway through

它可以检测并处理前一个锁持有者或master之类中途崩溃的情况



54.01 - 54.03

 because gosh the ready file has never be recreated

因为ready文件还没有被重新创建出来

54.18-54.22

Inigo program yeah sadly that is possible

很遗憾这是有可能发生的

54.22-54.27

 and you know either okay so the question is  nothing about zookeeper

So，他的问题和Zookeeper无关

54.27-54.29

but if you're writing threaded code and go

但如果你使用Go来编写多线程代码

54.29-54.38

a thread acquires a lock，could it crash while holding the lock halfway through whatever stuff it's supposed to be doing while holding a lock 

当一条线程拿到一个锁后，当它持有锁并执行一段逻辑的时候，这条线程中途出现问题会崩溃么？

54.38-54.38

and the answer is yes 

答案是Yes



54.38 - 54.42

actually there are ways for an individual thread to crash and go 

实际上，有多种方式可以让一条线程崩溃

54.42-54.47

oh I forget where they are maybe divide by zero  certain panics

可能是除以0，或者恐慌（panic）之类的事情

可能是除以0，这类可笑的事情

54.47-54.48

 anyway you can do it

总之，你可以做到这点



54.48 - 54.56

 and my advice about how to think about that  is

关于如何思考这个，我的意见是

54.56-54.57

 that the program is now broken

现在这个程序已经崩溃了

54.57-54.59

and you've got to kill it

你得将它杀死

54.59-55.02

 because in threaded code

因为在多线程代码中

55.02-55.03

the way the think about locks is that

我们思考锁的方式是这样的

55.03-55.05

 while the lock is held

当持有锁的时候

当锁被某线程持有时

55.05-55.09

the invariants in the data don't hold 

数据的不变性（invariants）可能会被破坏掉

55.09-55.14

so there's no way to proceed，if the lock holder crashes

So，如果该锁的持有者崩溃了，那就没办法去处理了



55.14-55.16

 there's no safe way to proceed

我们没有安全的办法去处理这个



55.15 - 55.20

 because all you know is whatever the invariants were that the lock was protecting no longer hold 

因为如你所知的那样，这个用来保护该数据的锁的不再被持有了

55.20-55.24

so and so and if you do want to proceed 

如果你确实想处理

55.24-55.26

you have to leave the lock marked as held

你得将状态设置为持有锁的状态

你得将锁设置为持有状态（知秋注：即你要握住这把锁）

55.26-55.29

so that no one else will ever be able to acquire it

So，这样就没有人能够去获取这把锁了



55.29 - 55.35

and you know unless you have some clever idea，that's pretty much the way you have to think about it in a threaded program

除非你有一些很聪明的想法，不然这就是你思考多线程程序的方式



55.35 - 55.39

because that's kind of the style with which people write threaded lock programs 

因为这就是人们在多线程程序中使用锁的方式

55.39-55.40

if you're super clever

如果你超级聪明

55.40-55.46

you could play the same kinds of tricks like this ready flag trick

那么你就可以去使用Zookeeper中ready文件这种办法

55.46-55.50

now it's super hard and go 

这做起来超级难

55.50-55.57

because the memory model says there is nothing you can count on except if there's a happens before relationship

因为在内存模型上，没什么你能指望的。除非有happens-before关系，只有它才能保证程序运行顺序

55.57-56.02

so if you play this game of writing changing some variables and then setting a done flag

So，如果你对变量进行修改，然后设置了一个done的标志符

        

56.02-56.10

that doesn't mean anything unless you release a lock and somebody else acquires a lock

除非你将该锁释放，然后其他人获取了这个锁，不然啥也不是

也没什么意义。除非你先将该锁释放，然后其他人获取了这个锁再做后续操作

56.10-56.16

 and only then can anything be said about the order in which or in even whether the updates happen

只有这样，我们才知道发生的顺序或者更新有没有成功

56.16-56.18

 so this is very very hard

So，这真的很难很难



56.18 - 56.24

it very hard and go to recover from a crash of a thread that holds the lock

我们很难将一个拿着锁的线程从崩溃中恢复回来


56.25 - 56.30

here is maybe a little more possible

这种方式实现的可能性还稍微大点



56.31 - 56.36

okay okay okay

Ok



56.36-56.43

that's all I want to talk about zookeeper

这就是我所想说的关于Zookeeper的内容

56.43-56.46

it's just two pieces of high bid 

这里主要强调了两部分

56.46-56.50

one is at these clever ideas for high performance by reading from any replica

从高性能方面来讲，其中一个巧妙的想法就是从任意replica上读取数据



56.50-56.53

but the they sacrifice a bit of consistency 

但这样做就会牺牲掉一些一致性

56.53-56.57

and the other interesting thing interesting take-home is

另一个我们感兴趣的点是

56.57-57.04

 that they worked out this API that really does let them be a general-purpose sort of coordination service

他们所制定的API确确实实让Zookeeper成为了一个通用的协调服务

57.04-57.08

 in a way that simpler schemes like put get interfaces just can't do 

这是那些较为简单的scheme（比如，put和get接口）所不能做到的

57.08-57.14

so they worked out a set of functions here that allows you to do things like write mini transactions and build your own locks 

So，他们制定了一系列功能，以此让你去实现write mini-transactions以及构建你自己的锁

57.14-57.18

and it all works out although requires care

尽管某些方面需要注意下，但一切都是可行的

57.18-57.20

 okay

+++++++++++++++++++++++++++++++++++++++++++++++++++++++++

57.20-57.25

now I want to turn to today's paper  which is CRAQ

现在我想来讲下今天的paper，即CRAQ



57.25-57.36

the reason why we're reading CRAQ paper it's a couple reasons

我们读CRAQ这篇论文的原因有两个

57.36-57.42

one is is that it's it does replication for fault tolerance 

其中一点是，它是为了能够容错（fault tolerance ）而去进行replication

57.42-57.55

and as we'll see the properties you get out of CRAQ or its predecessor chain replication are very different in interesting ways from the properties you get out of a system like raft 

我们可以从CRAQ中看到一些东西，它的链式复制和我们之前看到的系统（Raft）非常的不同，而这也是我们所感兴趣的



57.55-57.59

and so I'm actually going to talk about

So，我会去讨论这些内容

57.59-58.03

CRAQ is sort of an optimization to an older scheme called chain replication

CRAQ是对一种叫做链式复制（chain replication）的较旧方案的一种优化


58.07-58.11

 chain replications actually fairly frequently used in the real world

实际上，在现实生活中，链式复制（chain replication）在很多地方都有用到



58.11 - 58.13

 there's a bunch of systems that use it

很多系统都使用它



58.13 - 58.16

CRAQ is an optimization to it 

CRAQ则是对它的一种优化

58.16-58.19

that actually does a similar trick - zookeeper

实际上，它所玩的技巧和Zookeeper是相似的



58.19- 58.26

where it's trying to increase read throughput by allowing reads to any replicas

它通过将读请求分给任意replica来提高读请求的吞吐量

58.26-58.31

 so that you get you know number of replicas factor of increase in the read performance

读性能会随着replica数量的增加而增加

58.31-58.34

the interesting thing about CRAQ is that

CRAQ中令我们感兴趣的一点是

58.34-58.41

it does that well preserving linearise ability unlike zookeeper 

它与Zookeeper不同，它很好地保留了线性一致性的能力



58.41 - 58.45

which you know it seemed like in order to be able to read from any replica

Zookeeper为了能够从任意replica中读取数据



58.45-58.47

 they had to sacrifice freshness 

它们就得牺牲数据的即时性

58.47-58.48

and therefore snot linearizable

因此，Zookeeper就不具备线性一致性

58.48-58.55

CRAQ actually manages to do these reads from any replica  while preserving strong consistency

实际上，CRAQ也是从任意replica上读取数据，但它保留了强一致性

58.55-58.57

 I'm just pretty interesting

这点我很感兴趣

58.57-58.59

 okay 



58.59-59.02

so first I want to talk about the older system chain replication

So，首先，我想谈论下这种较老的系统，即链式复制（chain replication）

59.07-59.11

chain replication is a it's just a scheme

链式复制（chain replication）其实是一种方案

59.11-59.14

for you have multiple copies you want to make sure they all seen the same sequence of write

假设你有多个副本，你想去确保它们收到的都是相同顺序的写操作

59.14-59.16

so it's like a very familiar basic idea 

So，这是你们非常熟悉的一个基本思想

59.16-59.20

but it's a different topology  than raft

但它是一种与raft所不同的拓扑

59.20-59.23

 so the idea is 

So，它的思想是


59.23-59.27

that there's a chain of servers and chain replication

假设这里有一连串服务器


59.27-59.30

 and the first one is called the head 

我们将这条链上第一个服务器称之为head

59.30-59.36

last one's called the tail

最后一个则叫做tail

59.36-59.36

 when a write comes in

当我们收到一个写请求的时候

59.36-59.38

 when a client wants to write something 

当一个client想进行写操作的时候


59.38-59.39

say some client

假设，这里有某个client



59.39 - 59.42

it sends always all writes get sent to the head

它总是将所有的写请求发送给head

59.42-59.49

 the head updates or replaces its current copy of the data that the clients writing 

head会更新或替换该client所写入数据的当前副本

head会将该当前数据更新或替换为client所写入数据的当前副本

59.49-59.52

so you can imagine be put get key value store 

So，你可以想象成对key/value进行put和get操作


59.54-59.58

so you know if everybody started out with you know version a of the data  and 

So，假设，这条链中的每个服务器一开始的数据版本都是A

59.58-1.00.02

under chain replication，  when the head processes the write

在链式复制（chain replication）下，当head处理写请求时

1.00.02-1.00.03

 and maybe we're writing value B

如果我们正在写入的值是B


1.00.03-1.00.03

 you know the head just replaces its a with a B

head就会将A替换为B

1.00.03-1.00.08

 and passes the write down the chain

然后将该写请求沿着该链进行传播

1.00.08-1.00.11

 as each node sees the write

当每个节点看到这个写请求的时候

1.00.11-1.00.15

 it replaces over writes its copy the data the new data

它就会将新的数据写入该节点的数据副本中

1.00.15-1.00.17

 when the write gets the tail 

当写请求到达tail时

1.00.17-1.00.22

the tail sends the reply back to the client 

tail会对client进行响应

1.00.22-1.00.24

saying we completed your write

并说，我们已经处理完了你的写请求



1.00.24 - 1.00.27

that's how writes work 

这就是CRAQ处理写请求的方式

1.00.27-1.00.29

reads

再来说下处理读请求这块

1.00.29-1.00.31

if a client wants to do a read

如果client想进行一次读请求


1.00.31-1.00.36

 it sends the read to the tail  the read request to the tail

它会向tail发送读请求


1.00.36-1.00.39

and the tail just answers out of its current state

tail则会将它当前版本的数据作为响应发送给client

1.00.39-1.00.42

 so if we ask for this，whatever this object was 

So，如果我们想读取这个，不管这个对象是什么

1.00.42-1.00.43

the tail which is I hope current values B

tail就会将当前值B返回给我们

1.00.43-1.00.46

reads are a good deal simpler

So，读请求处理起来很简单

1.00.46-1.00.53

okay 



1.00.53-

so it should think for a moment like why to 





1.00.59 - 1.01.00

chain replication，so this is not CRAQ

So，这并不是CRAQ，这是链式复制（chain replication）

1.01.00-1.01.04

just to be clear this is chain replication chain replication is linearizable

我只是为了让你们了解链式复制（chain replication）是具备线性一致性的







五十二  阅举报
09-04
1.01.07-1.01.10

 you know in the absence of failures，what's going on is 

在没有故障的情况下，这会发生什么呢？



1.01.10 - 1.01.15

that we can essentially view it as really then the purposes of thinking about consistency 

从基于一致性的目标来思考


1.01.15-1.01.19

it's just this one server， the server sees all the writes

这个服务器看到了所有的写请求

1.01.19-1.01.20

and it sees all the reads 

它也看到了所有的读请求

1.01.20-1.01.22

and process them one at a time 

然后，它一次处理一个请求

1.01.22-1.01.25

and you know a read will just see the latest value that's written

一个读请求会看到被刚写入的最新的值

1.01.25-1.01.32

and that's pretty much all there is to it from the point of view look if there's no crashes what the consistency is like

从我们的角度来看，如果没有发生崩溃的情况下，那这就是所谓的一致性了

1.01.32-1.01.36

pretty simple 

相当简单



1.01.36-1.01.41

the failure recovery

我们来讲下故障恢复

1.01.41-1.01.48

 the a lot of the rationale behind chain replication is that 

链式复制（chain replication）背后的原理是



1.01.48- 1.01.51

the set of states you can see when after there's a failure is relatively constrained

发生故障后你可以看到的一组状态，相对来说会受到限制


1.01.56-1.02.00

because of this very regular pattern with how the writes get propagated 

因为这就是传播写请求的最常规的模式

1.02.00-1.02.03

and at a high level what's going on is that 

从一个高级层面来看，这里所会发生的事情是

1.02.03-1.02.09

any committed write that is any write that could have been acknowledged to a client to the writing client 

当任何被提交的写请求已经被发起该写请求的client所确认



1.02.09-1.02.12

or any write that could have been exposed in a read

或者，任何写请求的结果可能已经暴露，并作为一个读请求的结果返回给client

或者，任何写请求的结果被暴露出来作为响应读请求

1.02.12 - 1.02.17

that'll neither of those will ever happen unless that write reached the tail 

除非写请求传递到了这条链的tail，否则这一切不会发生



1.02.17-1.02.18

in order for it to reach the tail

为了将写请求传递到tail



1.02.18-1.02.21

it had to a pass through them in process by every single node in the chain

该写请求必须被这条链中的每个节点进行处理

1.02.21-1.02.28

so we know that if we ever exposed to write ever acknowledged write ever use it to a read 

so 如果我们想要将该写请求结果暴露出来用于read，即该写请求已被成功确认

1.02.28-1.02.31

that means every single node until the tail must know about that write

这意味着，直到tail处的整个路径上的每个节点都必须知道这个写请求

1.02.31-1.02.34

we don't get these situations

我们不会遇上这种情况，即



1.02.34-1.02.38

 like  if you'll call figure seven figure eight in the raft paper

如果你看下raft论文中的figure 7和8



1.02.38-1.02.40

where you can have just hair-raising complexity 

那么其中的复杂程度让我们头皮发麻



1.02.40-1.02.43

and how the different replicas differ if there's a crash 

而在这里，如果发生崩溃的话，那么不同replica之间的差异会是怎么样的呢？


1.02.44-1.02.51

here you know either that it is committed or it before the crash should reach some point

如果写请求已经被提交了，或者在发生崩溃前，它已经到达了该链中的某个节点

1.02.51-1.02.53

 and nowhere after that point

它无法到达该点之后的节点

1.02.53-1.02.55

because the progress of writes has always linear

因为写入的进度始终是线性的



1.02.55-1.02.57

so committed writes are always known everywhere

So，所有节点都应该知道那些已经被提交的写请求

1.02.57-1.02.58

 if a write isn't committed

如果某个写请求并没有被提交的话



1.02.58-1.03.01

 that means that before whatever crash it was that disturb the system

这意味着，在发生任何会扰乱系统的崩溃前



1.03.01-1.03.05

 the write of got into a certain point everywhere before that point and nowhere after point 

该写请求已经传递到崩溃点之前的所有节点，它并没有传递到崩溃点之后的节点





1.03.05-1.03.07

there's really the only two setups

这里真的就只有这两种情况

1.03.09-1.03.10

and at a high level

从一个高级层面来看

1.03.10-1.03.14

failure recovery is relatively simple

故障恢复相对来说很简单

1.03.14-1.03.16

 also if the head fails

如果head故障了



1.03.16 - 1.03.19

then to a first approximation

那么第一种可能就是

1.03.19-1.03.21

 the next node can simply take over his head

下一个节点可以简单地接手head的工作

1.03.21-1.03.23

 and nothing else needs to get done

除此以外，就不需要做别的事情了

1.03.23-1.03.29

 because any write that made it as far as the second node  while it was the head that failed 

因为当head发生了故障，那么任何client发起的写请求就会发给第二个节点



1.03.29- 1.03.31

so that write will keep on going and we'll commit 

So，写请求会继续往下传递，然后我们会提交该请求





1.03.31-1.03.34

if there's a wrtie that made it to the head before a crash

如果在head崩溃之前，head收到了一个写请求

1.03.34-1.03.35

 but the head didn't forward it 

但这个head并没有将该写请求转发到其他节点

1.03.35-1.03.37

well that's definitely not committed

Well，这肯定就是个未被提交的写请求

1.03.37-1.03.38

 nobody knows about it 

除了head以外，其他节点都不知道有这个写请求



1.03.38 - 1.03.41

and we definitely didn't send it an acknowledgment to the writing client

我们肯定也不会将确认信息发送给发出该写请求的client


1.03.41 - 1.03.43

 because the write didn't get down here

因为该写请求并未到达tail处



1.03.43 - 1.03.45

 so we're not obliged to do anything about a write

So，我们没有义务对该写请求做出响应



1.03.45 - 1.03.49

 it only reached a crashed head before it failed 

因为在故障发生前，它只到达了这个崩溃的head节点处



1.03.49-1.03.51

I may be the client will resend

可能该client会重新发起该写请求



1.03.51-1.03.53

but you know not our problem 

但这并不关我们的事



1.03.53-1.03.55

if the tail fails

如果tail发生了故障



1.03.55-1.03.58

 it's actually very similar 

实际上，它和head发生了故障是很相似的

1.03.58-1.03.58

the tail fails 

如果tail发生了故障


1.03.58-1.04.01

the next node can directly take over 

那么下一个节点（指tail前面的一个节点）可以接手tail的工作

1.04.01-1.04.06

because everything the tail knew then next the node just before it also knows

因为tail所知道的事情，它前面的一个节点也必然知道



1.04.06-1.04.09

 because the tale only hears things from the node just before it 

因为tail所知道的东西还是来源于它前面的一个节点



1.04.09-1.04.16

and it's a little bit complex of an intermediate node fails

如果head和tail之间的节点发生了故障，那么对我们来讲，就会有点复杂





1.04.16 - 1.04.21

but basically what needs to be done is we need to drop it from the chain

但简单来讲，我们要做的就是将它从链中移除


1.04.21-1.04.25

 and now there may be writes that it had received  that the next node hasn't received yet

比如，前一个节点收到了写请求，但下一个节点并没有收到这个写请求



1.04.25-1.04.28

and so if we drop a node out of the chain

So，如果我们将一个节点从链中删除

1.04.28-1.04.35

the predecessor may need to resend recent writes to its new successor right

前一个节点可能得需要重新发送最近的写请求给它的下一个新节点

1.04.35-1.04.39

 that's the recovery in a nutshell

总而言之，这就是故障恢复

1.04.39-1.04.49

 that's for why this construction why this instead of something else like why this verse is wrapped for example 

这就是为什么使用这种构造，而不是其他构造的原因了









1.04.49-1.04.56

the performance reason  is that 

出于性能的原因



1.04.56-1.05.03

in raft，if you recall we you know if we have a leader  and a bunch of you know some number of replicas

你们回想一下raft，如果我们有一个leader以及一堆replicas


1.05.03-1.05.06

right with the leader it's not in a chain

对于leader来说，它并不在这个链中

1.05.06-1.05.09

we got these the replicas are all directly fed by the leader 

我们所拥有的这些replica都是由leader直接提供数据的

1.05.09-1.05.13

so if a client write comes in or a client read for that matter

So，如果此时一个client发起了一个写请求或者读请求


1.05.13-1.05.18

the leader has to send it itself to each of the replicas

leader得将它发送给每个replica



1.05.18 - 1.05.22

whereas in chain replication the leader on the head only has to do one send

然而，在这种链式复制（chain replication）的情况下，leader只需要在head处发送一次请求即可


1.05.22-1.05.25

and these sends on the network are actually reasonably expensive 

实际上通过raft这种方式（即leader向所有client发送信息）的代价会非常昂贵

1.05.25-1.05.32

and so that means the load on a raft leader is going to be higher than the load on a chain replication leader

So，这就意味着raft leader的工作负载要比chain replication leader高得多

1.05.32-1.05.34

 and so that means that

So，这意味着

1.05.34-1.05.40

 you know as the number of client requests per second that you're getting from clients goes up

随着client端每秒请求数量的增加

1.05.40-1.05.47

 a raft leader will hit a limit  and stop being able to get faster sooner than a chain replication head

raft leader就会达到处理能力的极限，并且它的处理速度不再比chain replication head来的更快了

1.05.47-1.05.50

 because it's doing more work than the chain replication had

因为这种方式要比chain replication做更多工作

1.05.50-1.05.54

 another interesting difference between chain replication and raft is

chain replication和raft之间另一个我们感兴趣的不同之处在于 

1.05.54-1.06.00

 that the reads in raft are all also required to be processed by the leaders

在raft中，所有的读请求都需要由leader来处理

1.06.00-1.06.02

 the leader sees every single request from clients

leader会看到每个来自client端的请求


1.06.02-1.06.06

where's here the head sees everybody sees all the writes

然而，在这里，每个节点都会看到所有的写请求

1.06.06-1.06.10

 but only a tail sees the read requests 

但只有tail才会看到读请求

1.06.10-1.06.14

so there may be an extent to which the load is sort of split between the head and the tail

So，我们可以将这些工作量在head和tail之间进行平衡

1.06.14-1.06.17

rather than concentrated in the leader

而不是将工作全都丢给leader来处理



1.06.17 - 1.06.24

and and as I mentioned before

正如我之前提到的那样

1.06.24-1.06.36

the failure different sort of analysis required to think about different failure scenarios is a good deal simpler and chain replication than it is raft  and as a big motivation 

考虑到不同的故障情况所需进行的不同类型的分析，链式复制比起raft来说要容易得多，这就是我们使用链式复制的一个很大的动机

1.06.36-1.06.37

because it's hard to get this stuff correct 

因为它很难去保证这些东西的正确性

1.06.37-1.06.38

yes

请问

1.06.38-1.06.45

学生提问

1.06.45-1.06.48

yeah so if the tail fails

So，如果tail故障了

1.06.48-1.06.50

 but its predecessor had seen a write

但它的前一个节点看到了一个写请求

1.06.50-1.06.51

that the tail hadn't seen

然而，tail并没有看到这个写请求

1.06.51-1.06.55

then the failure of that hell basically commits that write is now committed 

那么基本来讲，这里的故障在于这个写请求已经被提交了



1.06.55-1.06.57

because it's reached the new tail

因为它已经到达了新的tail节点处（即原来tail的前一个节点）

1.06.57-1.06.59

 and so he could respond to the client 

So，它可以对client端进行响应



1.06.59 - 1.07.00

it probably won't

虽然可能它并不会对client进行响应

1.07.00-1.07.05

because it you know it wasn't a tail when it received the write 

因为当它（tail的前一个节点）收到写请求的时候，它还不是tail

1.07.05-1.07.07

and so the client may resend the write 

So，client可能得重新发送该写请求

1.07.07-1.07.08

and that's too bad and

这就太糟糕了



1.07.08 - 1.07.12

so we need duplicate suppression probably at the head

So，我们需要在head处抑制重复发送的请求

1.07.12-1.07.21

basically all the systems were talking about require in addition to everything else suppression of duplicate client requests 

基本上，我们所讨论的所有系统都会去抑制client重复发送请求



1.07.21-1.07.23

yes

请问

1.07.23-1.07.32

学生提问

1.07.32 - 1.07.33

can you say it again

你能再说一遍吗？

1.07.33-1.07.42

you want to know who makes the decisions about how to

你想去知道谁做出了这种决定？

1.07.42-1.07.44

 that's a outstanding question 

这是一个很棒的问题

1.07.44-1.07.45

the question is

他的问题是

1365

1.07.45 - 1.07.48

or rephrase the question a bit 

我来重新组织下问题


1.07.48-1.07.49

if there's a failure

如果这里有一个故障

1.07.49-1.07.55

like or suppose the second node stops being able to talk to the head 

假设第二个节点不能够和head通信的话

1.07.55-1.07.59

can this second node just take over

那么这第二个节点能否去接手head的工作

1.07.59-1.08.02

can it decide for itself gosh the head seems to thought away

它会说，这个head看起来已经挂掉了

1.08.02-1.08.03

 I'm gonna take over his head

我要去接手head的工作



1.08.03-1.08.06

 and tell clients to talk to me instead of the old head 

并告诉client，你们得和我进行通信，而不是和原来的head通信

1.08.06-1.08.07

but what do you think

但你是怎么想的呢？

1.08.07-1.08.11

that's not like a plan

这有点不符我们的计划

1.08.15-1.08.19

with the usual assumptions we make about how the network behaves

根据我们通常对网络行为的假设

1.08.19-1.08.23

 that's a recipe for split brain

也就是脑裂的情况



1.08.23-1.08.25

 right if you do exactly what I said 

如果你遇上了我所说的情况

1.08.25-1.08.28

because of course what really happened was 

这里真正会发生的事情是


1.08.28-1.08.30

that look the network failed here

假设，这里发生了网络故障

1.08.30-1.08.32

 the head is totally alive 

head确实还活着



1.08.32-1.08.34

and the head thinks its successor has died

head就会觉得它后面的节点挂了



1.08.34-1.08.36

 you know the successors actually alive

实际上，它后面的节点还活着的



1.08.36-1.08.37

 it thinks the head has died

后面的节点觉得head死掉了



1.08.37 1.08.38

 and they both say well

它们就会说

1.08.38-1.08.41

 gosh that other server seems to have died 

这个head节点似乎死掉了

1.08.41-1.08.42

I'm gonna take over 

我得去接收它的工作

1.08.42-1.08.44

and the head is gonna say oh I'll just be a sole replica

head就会说，我是唯一一个replica

1.08.44-1.08.48

 and I you know act as the head and the tail

我既扮演了head，也扮演了tail



1.08.47 - 1.08.50

because the rest of the chain seems to have gone away 

因为链中剩下的部分似乎没法联系上了



1.08.50-1.08.51

and second I'll do the same thing 

链中剩下的一部分也会做相同的事情



1.08.51-

and now we have two independent split brain versions of the data 

现在，我们就产生了两个独立的脑裂分区，它们各自都拥有不同的数据版本

1392

1.08.55 - 1.08.58

which will gradually get out of sync 

数据会逐渐变得不同步

1.08.57-1.09.07

so this construction is not proof against network partition 

So，这种构造被证明无法用于应对网络割裂的情况

1.09.07-1.09.11

and has not does not have a defense against split brain

无法抵御脑裂（split brain）问题



1.09.11-1.09.14

 and what that means  in practice is

这在实战中意味着什么呢

1.09.13-1.09.15

  it cannot be used by itself

我们不能单独使用它这种构造

1.09.15-1.09.21

it's like a helpful thing to have in our backpocket but it's not a complete replication story 

这看起来是我们备选方案中非常有用的一个选择，但它并不具备完善的replication能力

1.09.21- 1.09.25

so it's it's very commonly used

So，它非常常用

1.09.25-1.09.27

 but it's used in this stylized way

但它是以这种风格进行使用的

1.09.27-1.09.33

in which there's always an external Authority you know not this chain

即这里我们始终需要一个第三方来辅助我们做些事情



1.09.34-1.9.41

 that decides who's that sort of makes a call on who's alive and who's dead 

它会通过调用某个函数来判断哪些服务器还活着或者挂掉了

即通过调用这个第三方的一些东西来判断哪些服务器还活着或者挂掉了

1.09.41-1.09.47

and make sure everybody agrees on a single story about who constitutes the change 

以此来确保所有人都统一意见

1.09.47-1.09.52

there's never any disagreement some people think the chain is this no and some people think the chain is this other node 

让大家不再对链上的节点有分歧






1.09.52-1.09.55

so what's that's usually called as a configuration manager

So，这通常叫做配置管理器

1.09.55-1.10.03

and its job is just a monitor aliveness

它的职责就是去监控节点是否存活

1.10.03-1.10.09

every time the configuration manager thinks the server's dead

每当配置管理器觉得某个服务器死掉了

1.10.09-1.10.12

 it sends out a new configuration

它就会对外发送一个新的配置

1.10.12-1.10.17

 in which you know that this chain has a new definition head whatever tail 

那么这个链就会有了一个新的定义，比如谁是head，谁是tail



1.10.17- 1.10.22

and that's server that the configuration manager thinks is that may or may not be dead

配置管理器觉得这台服务器可能死了或者没死

1.10.22-1.10.23

 but we don't care

但我们并不在意

1.10.23-1.10.27

because everybody is required to follow then your configuration

因为所有节点都得遵循配置管理器所发出的配置



1.10.27-1.10.29

and so there can't be any disagreement

So，这里不会有任何分歧存在


1.10.29 - 1.10.31

because there's only one party making these decisions

因为此处只有一方可以做出这些决定



1.10.31 - 1.10.34

 not going to disagree with itself 

它不会去否定自己



1.10.34 - 1.10.35

of course how do you make a service that's fault tolerant and doesn't disagree with itself

这就是你构建出一个具备容错能力且不会否定自身的服务的方式了



1.10.36 - 1.10.40

 but doesn't suffer from split brain if there's network partitions

并且，如果遇上网络割裂，它不会受到脑裂的影响（split brain）

1.10.40-1.10.51

 and the answer to that is that the configuration manager usually uses Raft or paxos or in the case of crack zookeeper

对此，答案就是，该配置管理器通常会去使用Raft，PAXOS或这个例子中的Zookeeper

1.10.51-1.10.56

 which itself of course is built on a raft like scheme

Of course，Zookeeper是在Raft之上构建的

1.10.56-1.11.01

so you to the usual complete set up in your data center is it 

So，通常你数据中心的完整配置应该是这样的


1.11.01-1.11.06

you have a configuration manager it's based on oRaft，Paxos or whatever 

你所拥有的配置管理器是基于Raft，Paxos或者类似的东西而构建的



1.11.06-1.11.09

so it's fault tolerant  and does not suffer from split brain

So，它具备容错能力，并且不会受到脑裂影响



1.11.09 - 1.11.15

 and then you split up your data over a bunch of chains if you know room with a thousand servers in it

如果你的机房中有一千台服务器，然后我们将数据拆分到这些链中


1.11.15-1.11.17

and you have you know chain a 

你有chain A


1.11.17-1.11.22

you know it's these servers

它里面有三个服务器



1.11.22-1.11.27

 or the configuration manager decides that the chain should look like 

配置管理器会去决定这条链看起来应该是怎么样的






1.11.26-1.11.29

chain a is made of server one server to server three 

Chain A是由服务器1，服务器2和服务器3所组成


1.11.29-1.11.36

chain B you know server 4，server 5，server 6 whatever and

Chain B则是由服务器4，服务器5和服务器6组成



1.11.36 - 1.11.39

it tells everybody this whole list

配置管理器会将这个列表告诉所有人



1.11.39-1.11.42

it's all the clients know all the servers knows

这样，所有的client知道了，所有的服务器也知道了


1.11.42-1.11.48

and the individual servers opinions about whether other servers are alive or dead are totally neither here nor there

每个服务器就不用去关心其他服务器是生还是死


1.11.48 - 1.11.54

if this server really does die

如果这个服务器真的挂掉了

1.11.54-1.12.01

then the head is required to keep trying indefinitely until I guess a new configuration from the configuration manager 

那么head会不停地试着和这个挂掉的服务器进行通信，除非它从配置管理器处拿到了新的配置



1.12.01 - 1.12.05

not allowed to make decisions about who's alive and who's dead

它没有权利去决定其他服务器的生死

1.12.05-1.12.08

 what's that

你想说啥？

1.12.08-1.12.12

oh boy you've got a serious problem

Oh，老弟，你说的是个严重的问题

1.12.12-1.12.14

 so that's why you replicated using raft

So，这就是你使用raft来进行复制的原因

1.12.14-1.12.18

make sure the different replicas are on different power supplies the whole works

以此来确保不同的replica使用的是不同的电源，并进行整个工作


1.12.18-1.12.24

but this this construction I've set up here it's extremely common

但我这里所设置的这种构造是非常常见的



1.12.22 - 1.12.27

 and it's how chain replication is intended to be used 

这就是我们该如何使用链式复制（chain replication）的方式

1.12.27-1.12.28

how CRAQ intend to be used

这也是我们使用CRAQ的方式

1.12.28-1.12.31

 and the logic of it is that

此处的逻辑是

1.12.31-1.12.37

 like chain replication， if you don't have to worry about partition and split brain

比如在链式复制（chain replication）中，如果我们不用去担心网络割裂和脑裂问题

1.12.37-1.12.43

 you can build very high speed efficient replication systems using chain replication for example

那么，我们就可以使用链式复制（chain replication）来构建出高速高效的replication系统

1.12.43-1.12.48

so these individual you know data replication and

So，对于此处的数据副本来说



1.12.48 - 1.12.50

we're sharding the data over many chains

我们将数据分摊到许多链上



1.12.50 - 1.12.57

individually this these chains can be built to be just the most efficient scheme for the particular kind of thing that you're replicating 

我们可以使用这些链来构建出用来复制特定事物的最有效方案



1.12.57-1.12.59

you may read heavy write heavy whatever 

你可能需要进行大量读取，或者大量写入之类的操作



1.12.59-1.13.01

but we don't have to worry too much about partitions

但我们无需过于关心网络割裂这种情况



1.13.01 - 1.13.09

and then all that worry is concentrated in the reliable non split-brain configuration manager

我们所需要关心的就是可靠的无脑裂的配置管理器



1.13.18-1.13.19

okay



1.13.19-1.13.20

so your question is 

So，你的问题是


1.13.20-1.13.26

why are we using chain replication here instead of raft

我们这里为什么用的是链式复制（chain replication）而不是raft



1.13.30-1.13.31

 okay 



1.13.31-1.13.34

so it's like a totally reasonable question

So，总的来讲，这是个合理的问题



1.13.34-1.13.41

 it doesn't really matter for this construction

对于这种构造来说，并没有关系


1.13.41 - 1.13.43

because even if we're using raft here

因为即使我们这里使用raft



1.13.43- 1.13.59

 we still need one party to make a decision with which there can be no disagreement about how the data is divided over our hundred different replication groups 

在没有分歧的情况下，我们依然得需要一方来做出决定，我们得将数据划分到数百个不同的replication组中（知秋注：通过hash或者其他算法来将数据分片，发送到对应的处理replication group中，以此来对请求分流，降低服务器压力）

1.13.59-1.14.04

right so all you know and I need kind of big system you're splitting your sharding or splitting up the data

so 我需要一个很大的系统，你要将你的数据切分，



1.14.04-1.14.07

somebody needs to decide how the data is assigned to the different replication groups 

有人需要去判断这些数据是如何分配到不同的replication group中的

通过一些方式对这些数据进行判断，并分配到不同的replication group中

1.14.07 - 1.14.10

this has to change over time as you get more or less Hardware more data or whatever 

这个会随着你数据量或者硬件数量的变化而变化



1.14.10 - 1.14.12

so if nothing else

So，如果没有发生其他事情的话



1.14.12 - 1.14.14

 the configuration manager is saying well

配置管理器就会说




1.14.14 - 1.14.18

look you know the keys start with A or B goes here or then C or D goes here

以A或B开头的就放在这里，以C或D开头的就放在下面



1.14.18 - 1.14.21

 even if you use Paxos here 

即使你这里使用的是Paxos

1.14.21-1.14.23

now there's also this smaller question

这里还有个小问题

1.14.23-1.14.30

 if we didn't eat you know what should we use for replication should be chain replication or paxos or raft or whatever

我不会告诉你应该使用什么来作为实现replication 的方式，比如 chain replication或paxos 或raft 或者其他

1.14.30-14.35

 and people do different things

不同的人会选择不同的方式

1.14.35-1.14.39

some people do actually use Paxos based replication

实际上，有些人会选择基于Paxos来做到replication

1.14.39-1.14.39

like spanner

比如，Spanner

1.14.39-1.14.43

 which I think we're gonna look at later in the semester 

这个我们会在这学期之后的课程中讲到

1.14.43-1.14.43

has this structure 

它使用的也是黑板上我所画的这种结构

1.14.43-1.14.48

but it actually uses Paxos to replicate writes for the data 

但它实际上是使用Paxos来对针对数据的写请求进行复制

1.14.48-1.14.53

you know the reason why you might not want to use Paxos or raft is

我们之所以不想去使用Paxos或raft的原因是

1.14.53-1.14.57

 that it's arguably more efficient to use this chain construction 

可以说，使用这种链式结构，我们的系统可以变得更加高效



1.14.57 - 1.15.00

because it reduces the load on the leader 

因为它减少了leader的工作量

1.15.00-1.15.03

and that may or may not be a critical issue 

这可能并不一定是其中的关键问题

1.15.03-1.15.09

the a reason to favor raft Paxos is that 

我们倾向于使用raft和Paxos的一个原因是

1.15.09-1.15.13

they do not have to wait for a lagging replica

它们无需去等待一个落后的replica

1.15.13-1.15.16

 this chain replication has a performance problem

这种链式复制（chain replication）就有这样一个性能问题

1.15.16-1.15.18

that if one of these replicas is slow

如果其中一个replica的速度很慢

1.15.18-1.15.20

because even for a moment

因为即使是片刻



1.15.20 - 1.15.23

you know because every write has to go through every replica 

你们知道的，因为每个写请求都得经过每个replica



1.15.23-1.15.28

even a single slow replica slows down all  write operations

只要有一个replica的速度很慢，那它就会降低所有写操作处理的进度

1.15.28-1.15.29

and I can be very damaging 

这就让我们很受伤了

1.15.29-1.15.29

you know 

你知道的

1.15.29-1.15.31

if you have thousands of servers 

如果我们有数千台服务器

1.15.31-1.15.39

probably did any given time you know seven of them are out to lunch or unreliable or slow 

假设，在某一时刻，其中7台服务器出了问题，变得不可靠，或者速度变慢



1.15.39-15.41

because somebody's installing new software who knows what

因为此时可能有人正在安装新的软件，拖累了机器的速度



1.15.41 - 1.15.49

and that so it's a bit damaging to have every request be sort of limited by the slowest server

So，处理每个请求的时候都会被最慢的服务器所拖累，这就让我们很受伤

1.15.49-1.15.54

 whereas raft and paxos well it's so raft for example

So，例如，对于raft来说

1.15.54-1.15.56

 if one of the followers is slow, it doesn't matter

如果其中一个follower变慢了，这没关系



1.15.56 - 1.15.57

because that leader only has to wait for a majority

因为leader只需要去等待多数派就行了



1.15.57 - 1.16.00

 it doesn't have to wait for all of them 

它无需去等待所有服务器对它进行响应

1.16.00-1.16.02

you know ultimately they all have to catch up 

你知道的，最终这些follower都会赶上leader的进度



1.16.02-1.16.07

but raft is much better resisting transient slowdown

但raft能更好地应对瞬间网络抖动或硬件问题引发的服务降速所带来的影响

1.16.07-1.16.17

 and some Paxos based systems although not really raft are also good at dealing with the possibility that the replicas are in different data centers  and maybe far from each other 

某些基于Paxos的系统也能很好的应对处于不同的数据中心的replica所发生的这种情况（即瞬间服务降速），这些服务器彼此之间的距离可能很远

1.16.17-1.16.18

and because you only need a majority

因为我们需要的只是多数派



1.16.18-1.16.22

 you don't have to necessarily wait for acknowledgments from a distant data center

我们无需去等待那些距离我们很远的数据中心返回给我们确认信息



1.16.22-1.16.29

 and so that can also leads people to use paxos raft like majority schemes rather than chain replication

So，这样会让人们使用Paxos和raft这种多数派策略，而不是链式复制（chain replication）



1.16.29-1.16.35

but this is sort of a it depends very much on your workload and what you're trying to achieve

但这就取决于你的工作量有多少，以及你想达到怎样的效果

1.16.35-1.16.43

 but this overall architecture is in I don't know if it's Universal but it's extremely common

虽然我并不清楚整个架构是否通用，但它是非常常见的一种架构

1.16.43-1.16.45

yes

请讲

1.16.45-1.17.03

学生提问

1.17.02 - 1.17.05

like intentional topologies

这就像是互联网的拓扑结构

1.17.05-1.17.09

 okay the for a network that's not broken 

对于网络来说，它并没有断开

1.17.09-1.17.12

the usual assumption is that

此处我们所通常做的假设是

1.17.12-1.17.15

all the computers can talk to each other through the network

所有的电脑都可以通过网络互相通信

1.17.15-1.17.21

for networks that are broken，because somebody stepped on a cable or some routers misconfigured 

因为有人把网线给踩断了，或者某些路由器配置错误，那么这些网络就会中断



1.17.21-1.17.24

any crazy thing can happen

任何疯狂的事情都可能发生



1.17.24-1.17.27

so absolutely due to miss configuration 

So，由于这种错误的配置


1.17.27-1.17.34

you can get a situation where you know these two nodes can talk to the configuration manager 

你就会遇上这种情况，即这两个节点都可以和配置管理器进行通信



1.17.34-1.17.35

and the configuration managers think they're up

并且，配置管理器觉得它们俩都活着






1.17.34 - 1.17.36

 but they can't talk to each other 

但它们无法互相通信

1.17.36-1.17.40

so yes

So，你说的没错



1.17.40-1.17.42

and that's a killer for this right

这就是一个杀手级的灾难


1.17.42-1.17.44

because other configuration manager thinks that are up

因为其他的配置管理器觉得它们都还活着

1.17.44-1.17.45

 they can't talk to each other 

但它们无法互相通信

1.17.45-1.17.47

boy it's just like it's a disaster 

少年，这就是一场灾难

1.17.47-1.17.52

and if you need your system to be resistant to that

如果你需要让你的系统能对此进行防范

1.17.52-1.17.55

then you need to have a more careful configuration manager 

那么你就需要一个更严谨的配置管理器

1.17.55-1.17.58

you need logic in the configuration manager that says gosh

你需要在配置管理器中放一些逻辑

1.17.58-1.17.59

I'm only gonna form a chain out of these services 

我只会从这些服务中生成一条链



1.17.59 - 1.18.01

not only I can talk to that

不止我可以和它进行通信

1.18.01-1.18.02

 but they can talk to each other 

服务彼此之间也可以互相通信



1.18.02-1.18.04

and sort of explicitly check

并且明确地进行检查



1.18.04-.18.06

 and I don't know if that's common

我不清楚这是否常见



1.18.06-1.18.08

 I mean I'm gonna guess not

我猜这应该不怎么常见



1.18.08-1.18.10

but if you were super careful you'd want to

但如果你超级严谨的话，那你就想去这么做



1.18.10-1.18.12

 because even though we talked about network partition

虽然我们讨论过网络割裂

1.18.12-1.18.14

 that's like a abstraction 

它有点抽象

1.18.14-1.18.16

and in reality 

在现实中

1.18.16-1.18.22

you can get any combination of who can talk to who else  and some are may be very damaging

你可以去得到彼此可以与其他人通信的任何组合，有些可能会引发非常大的危害（知秋注：可能会极大增加算法复杂性）

1.18.22-end

okay I'm gonna wrap up and see you next week

Ok，散会，下周再见





四十四  阅举报
